#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•¸æ“šè™•ç†å·¥å…·
åŠŸèƒ½ï¼š
1. è®€å– fainaldata_no_del.json
2. éæ¿¾ç§ç«‹å¤§å­¸å’Œ107å¹´ä¹‹å‰çš„æ•¸æ“š
3. èª¿ç”¨AI APIæª¢æŸ¥å’Œä¿®æ­£çŸ¥è­˜é»å°æ‡‰é—œä¿‚
4. ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“šåˆ° 20250917_result.json
"""

import json
import os
import sys
import random
import threading
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from dataclasses import dataclass

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°å…¥APIå¯†é‘°ç®¡ç†
from api_keys import MultiGroupAPIKeyManager, get_api_key, get_api_keys_count, get_available_groups

try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("è­¦å‘Š: google-generativeai æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬API")

@dataclass
class ProcessingConfig:
    """æ•¸æ“šè™•ç†é…ç½®"""
    # æ–‡ä»¶è·¯å¾‘
    input_file: str = "../data/fainaldata_no_del.json"
    filter_output_file: str = "../data/20250918_result.json"
    ai_output_file: str = "../data/20250918_ai_judged_final.json"
    
    # éæ¿¾è¨­å®š
    min_year: int = 107  # æ°‘åœ‹107å¹´
    schools_to_remove: List[str] = None
    
    # AIè¨­å®š
    max_workers: int = 6
    progress_interval: int = 50
    
    def __post_init__(self):
        if self.schools_to_remove is None:
            self.schools_to_remove = [
                "ç¾©å®ˆå¤§å­¸", "è¯åˆå¤§å­¸", "é‡‘é–€å¤§å­¸", "æ…ˆæ¿Ÿå¤§å­¸", "ä¸­åŸå¤§å­¸",
                "é«˜é›„å¤§å­¸", "æ±æµ·å¤§å­¸", "é•·åºšå¤§å­¸", "è‡ºä¸­ç§‘å¤§", "é«˜é›„æ‡‰ç”¨ç§‘æŠ€å¤§å­¸"
            ]

class DataProcessor:
    def __init__(self, mode="filter"):
        """
        åˆå§‹åŒ–æ•¸æ“šè™•ç†å™¨
        mode: "filter" - æ•¸æ“šéæ¿¾æ¨¡å¼, "ai_judge" - AIé‡æ–°åˆ¤æ–·æ¨¡å¼
        """
        self.mode = mode
        
        if mode == "filter":
            self.input_file = "../data/fainaldata_no_del.json"
            self.output_file = "../data/20250918_result.json"
        else:  # ai_judge
            self.input_file = "../data/20250918_result.json"  # ä½¿ç”¨æ¨¡å¼1çš„è¼¸å‡º
            self.output_file = "../data/20250918_ai_judged_final.json"
        
        self.processed_count = 0
        self.filtered_count = 0
        self.error_count = 0
        self.ai_judged_count = 0
        
        # å®šç¾©éœ€è¦å®Œå…¨ç§»é™¤çš„å­¸æ ¡
        self.schools_to_remove = [
            "ç¾©å®ˆå¤§å­¸",
            "è¯åˆå¤§å­¸", 
            "é‡‘é–€å¤§å­¸",
            "æ…ˆæ¿Ÿå¤§å­¸",
            "ä¸­åŸå¤§å­¸",
            "é«˜é›„å¤§å­¸",
            "æ±æµ·å¤§å­¸",
            "é•·åºšå¤§å­¸",
            "è‡ºä¸­ç§‘å¤§",
            "é«˜é›„æ‡‰ç”¨ç§‘æŠ€å¤§å­¸"
        ]
        
        # å®šç¾©éœ€è¦ç§»é™¤çš„å¹´ä»½ï¼ˆ106å¹´ä¹‹å‰ï¼‰
        self.min_year = 107  # æ°‘åœ‹107å¹´
        
        # çµ±è¨ˆå„å­¸æ ¡çš„é¡Œç›®æ•¸é‡
        self.school_stats = {}
        
        # æ¨™æº–åŒ–çŸ¥è­˜é»é«”ç³»ï¼ˆåŸºæ–¼ insert_mongodb.py + æ•¸å­¸èˆ‡çµ±è¨ˆé ˜åŸŸï¼‰
        self.knowledge_domains = {
            "æ•¸ä½é‚è¼¯ï¼ˆDigital Logicï¼‰": [
                "æ•¸é‡è¡¨ç¤ºæ³•", "æ•¸ä½ç³»çµ±èˆ‡é¡æ¯”ç³»çµ±", "é‚è¼¯æº–ä½èˆ‡äºŒé€²ä½è¡¨ç¤ºæ³•", "æ•¸ä½ç©é«”é›»è·¯èˆ‡ PLD ç°¡ä»‹",
                "åŸºæœ¬é‚è¼¯é—œä¿‚èˆ‡å¸ƒæ—ä»£æ•¸", "æˆ–é–˜ã€åŠé–˜èˆ‡åé–˜", "åæˆ–é–˜èˆ‡ååŠé–˜", "äº’æ–¥æˆ–é–˜èˆ‡äº’æ–¥åæˆ–é–˜",
                "å¸ƒæ—ä»£æ•¸ç‰¹è³ª", "å–®è®Šæ•¸å®šç†", "å¤šè®Šæ•¸å®šç†èˆ‡ç¬¬æ‘©æ ¹å®šç†", "å¸ƒæ—ä»£æ•¸å¼ç°¡åŒ–æ³•", "å¡è«¾åœ–èˆ‡çµ„åˆé‚è¼¯è¨­è¨ˆæ­¥é©Ÿ"
            ],
            "ä½œæ¥­ç³»çµ±ï¼ˆOperating Systemï¼‰": [
                "æ¦‚èªª", "ä½œæ¥­ç³»çµ±çµæ§‹", "è¡Œç¨‹è§€å¿µ", "åŸ·è¡Œç·’èˆ‡ä¸¦è¡Œæ€§", "CPU æ’ç­",
                "åŒæ­¥å·¥å…·", "åŒæ­¥ç¯„ä¾‹", "æ­»çµ", "ä¸»è¨˜æ†¶é«”", "è™›æ“¬è¨˜æ†¶é«”", "å¤§é‡å„²å­˜çµæ§‹", "è¼¸å…¥/è¼¸å‡ºç³»çµ±"
            ],
            "è³‡æ–™çµæ§‹ï¼ˆData Structureï¼‰": [
                "è³‡æ–™çµæ§‹å®šç¾©", "è³‡æ–™çµæ§‹å°ç¨‹å¼æ•ˆç‡å½±éŸ¿", "æ¼”ç®—æ³•å®šç¾©", "ç¨‹å¼æ•ˆç‡åˆ†æ",
                "ä¸€ç¶­é™£åˆ—", "äºŒç¶­é™£åˆ—", "å–®å‘éˆçµä¸²åˆ—", "é›™å‘èˆ‡ç’°ç‹€éˆçµä¸²åˆ—", "ä½‡åˆ—", "å †ç–Š", "äºŒå…ƒæ¨¹èˆ‡äºŒå…ƒæœå°‹æ¨¹"
            ],
            "é›»è…¦ç¶²è·¯ï¼ˆComputer Networkï¼‰": [
                "ç°¡ä»‹", "è¨Šè™Ÿ", "è¨Šè™Ÿå‚³è¼¸", "èª¿è®Š", "é¡æ¯”å‚³è¼¸èˆ‡æ•¸ä½å‚³è¼¸",
                "å€åŸŸç¶²è·¯æ‹“æ¨¸æ–¹å¼", "å€åŸŸç¶²è·¯é–‹æ”¾æ¶æ§‹", "å€åŸŸç¶²è·¯å…ƒä»¶", "å€åŸŸç¶²è·¯é€£ç·šå¯¦ä½œ", "TCP/IP é€šè¨Šå”å®š"
            ],
            "è³‡æ–™åº«ï¼ˆDatabaseï¼‰": [
                "1-1 è³‡æ–™åº«ç”±ä¾†", "1-2 è³‡æ–™åº«ç®¡ç†ç³»çµ±", "1-3 è³‡æ–™æ¨¡å‹", "1-4 ä¸‰å±¤å¼æ¶æ§‹",
                "2-1 è¨­è¨ˆæµç¨‹", "2-2 å€‹é«”é—œä¿‚æ¨¡å‹", "2-3 ä¸»éµèˆ‡å¤–éƒ¨éµ", "2-4 æ­£è¦åŒ–",
                "3-1 SQL èªè¨€", "3-2 SSMS æ“ä½œ", "4-1 è³‡æ–™å‹åˆ¥", "4-2 ä½¿ç”¨ SQL æ•˜è¿°æ–°å¢è³‡æ–™è¡¨"
            ],
            "AIèˆ‡æ©Ÿå™¨å­¸ç¿’ï¼ˆAI & Machine Learningï¼‰": [
                "1-1 AI å·¥ç¨‹å´›èµ·", "1-2 åŸºç¤æ¨¡å‹ä½¿ç”¨æ¡ˆä¾‹", "1-3 AI æ‡‰ç”¨è¦åŠƒ",
                "2-1 è¨“ç·´æ•¸æ“šèˆ‡å»ºæ¨¡", "2-2 å¾Œè¨“ç·´èˆ‡å–æ¨£", "3-1 èªè¨€å»ºæ¨¡æŒ‡æ¨™èˆ‡ç²¾ç¢ºè©•ä¼°",
                "4-1 æ¨¡å‹é¸æ“‡èˆ‡è¨­è¨ˆè©•ä¼°ç®¡é“", "5-1 æç¤ºå·¥ç¨‹æœ€ä½³å¯¦ä¾‹", "6-1 RAG èˆ‡ä»£ç†", "6-2 è¨˜æ†¶ç®¡ç†",
                "7-1 å¾®èª¿æ¦‚è¿°èˆ‡æŠ€è¡“", "8-1 æ•¸æ“šèª¿ç†èˆ‡å¢å¼·"
            ],
            "è³‡è¨Šå®‰å…¨ï¼ˆInformation Securityï¼‰": [
                "1-1 è³‡è¨Šå®‰å…¨æ¦‚è«–", "1-2 è³‡è¨Šæ³•å¾‹èˆ‡äº‹ä»¶è™•ç†", "1-3 è³‡è¨Šå®‰å…¨å¨è„…",
                "2-1 èªè­‰ã€æˆæ¬Šèˆ‡å­˜å–æ§åˆ¶", "2-2 è³‡è¨Šå®‰å…¨æ¶æ§‹èˆ‡è¨­è¨ˆ", "2-3 åŸºç¤å¯†ç¢¼å­¸", "2-4 è³‡è¨Šç³»çµ±èˆ‡ç¶²è·¯æ¨¡å‹",
                "3-1 é˜²ç«ç‰†èˆ‡ä½¿ç”¨æ”¿ç­–", "3-2 å…¥ä¾µåµæ¸¬èˆ‡é˜²ç¦¦ç³»çµ±", "3-3 æƒ¡æ„ç¨‹å¼èˆ‡é˜²æ¯’", "3-4 å¤šå±¤æ¬¡é˜²ç¦¦",
                "4-1 è³‡è¨Šå®‰å…¨ç‡Ÿé‹èˆ‡ç®¡ç†", "4-2 é–‹ç™¼ç¶­é‹å®‰å…¨"
            ],
            "é›²ç«¯èˆ‡è™›æ“¬åŒ–ï¼ˆCloud & Virtualizationï¼‰": [
                "1-1 CPUã€ä¼ºæœå™¨ã€å­˜å„²ã€ç¶²è·¯è™›æ“¬åŒ–", "1-2 Xenã€KVMã€RHEV ç°¡ä»‹", "1-3 VMware / VirtualBox / Hyper-V",
                "2-1 KVM åŸç†èˆ‡æ¶æ§‹", "2-2 Qemu æ¶æ§‹èˆ‡é‹è¡Œæ¨¡å¼", "2-3 Qemu å·¥å…·ä»‹ç´¹",
                "3-1 Libvirt æ¶æ§‹èˆ‡ API", "3-2 XML é…ç½®æ–‡ä»¶", "4-1 å®‰è£èˆ‡ä½¿ç”¨ä»‹ç´¹", "4-2 WebVirtMgr ç®¡ç†å¹³è‡º",
                "5-1 è»Ÿä»¶ Overlay SDN", "5-2 ç¡¬ä»¶ Underlay SDN", "6-1 RAID æŠ€è¡“èˆ‡ç¡¬ç›¤æ¥å£", "6-2 é‚è¼¯å·ç®¡ç†"
            ],
            "ç®¡ç†è³‡è¨Šç³»çµ±ï¼ˆMISï¼‰": [
                "1-1 ç¾ä»Šå…¨çƒä¼æ¥­çš„è³‡è¨Šç³»çµ±", "1-2 å…¨çƒé›»å­åŒ–ä¼æ¥­èˆ‡å”åŒåˆä½œ", "1-3 è³‡è¨Šç³»çµ±ã€çµ„ç¹”èˆ‡ç­–ç•¥",
                "2-1 è³‡è¨Šç§‘æŠ€åŸºç¤å»ºè¨­èˆ‡æ–°èˆˆç§‘æŠ€", "2-2 è³‡æ–™åº«èˆ‡è³‡è¨Šç®¡ç†", "2-3 é›»å‚³é€šè¨Šã€ç¶²éš›ç¶²è·¯èˆ‡ç„¡ç·šç§‘æŠ€", "2-4 è³‡è¨Šç³»çµ±å®‰å…¨",
                "3-1 ä¼æ¥­ç³»çµ±æ‡‰ç”¨", "3-2 é›»å­å•†å‹™èˆ‡æ•¸ä½å¸‚å ´", "3-3 çŸ¥è­˜ç®¡ç†èˆ‡ AI",
                "4-1 å»ºç«‹è³‡è¨Šç³»çµ±", "4-2 ç®¡ç†å°ˆæ¡ˆèˆ‡å…¨çƒç³»çµ±"
            ],
            "è»Ÿé«”å·¥ç¨‹èˆ‡ç³»çµ±é–‹ç™¼ï¼ˆSoftware Engineeringï¼‰": [
                "1-1 è»Ÿé«”å·¥ç¨‹å®šç¾©èˆ‡æµç¨‹", "1-2 è»Ÿé«”ç³»çµ±èˆ‡é–‹ç™¼ç¨‹åº", "2-1 éœ€æ±‚å·¥ç¨‹èˆ‡ç³»çµ±æ¨¡å‹",
                "3-1 è»Ÿé«”ç³»çµ±æ¶æ§‹è¨­è¨ˆ", "4-1 ç‰©ä»¶å°å‘è¨­è¨ˆèˆ‡å¯¦å‹™", "5-1 ç³»çµ±æ¸¬è©¦æµç¨‹",
                "6-1 è»Ÿé«”ç³»çµ±ç®¡ç†", "6-2 è»Ÿé«”ç¶­è­·", "7-1 å“è³ªç®¡ç†åŸå‰‡",
                "8-1 è¨­è¨ˆæ¨¡å¼æ‡‰ç”¨", "8-2 è»Ÿé«”é‡æ§‹åŸå‰‡", "9-1 è³‡æ–™åº«ç³»çµ±é–‹ç™¼æµç¨‹", "10-1 è·¨å¹³å°é–‹ç™¼æ¦‚å¿µ"
            ],
            "æ•¸å­¸èˆ‡çµ±è¨ˆï¼ˆMathematics & Statisticsï¼‰": [
                "é›†åˆè«–", "æ•¸åˆ—èˆ‡ç´šæ•¸", "æ¥µé™", "å¾®åˆ†", "ç©åˆ†",
                "æ©Ÿç‡", "çµ±è¨ˆæ¨è«–", "å¸¸æ…‹åˆ†é…", "å‡è¨­æª¢å®š",
                "ç·šæ€§ä»£æ•¸ï¼ˆçŸ©é™£ã€å‘é‡ã€ç‰¹å¾µå€¼ï¼‰", "æ•¸ç†é‚è¼¯", "é›¢æ•£æ•¸å­¸ï¼ˆé—œä¿‚ã€å‡½æ•¸ã€åœ–è«–ï¼‰"
            ]
        }
        
        # éœ€è¦ç§»é™¤çš„å¤§çŸ¥è­˜é»
        self.domains_to_remove = ["åŸºæœ¬è¨ˆæ¦‚", "è¨ˆç®—æ©Ÿæ¦‚è«–"]
        
        # åˆå§‹åŒ– Gemini API
        if GEMINI_AVAILABLE:
            self.setup_gemini()
    
    def setup_gemini(self):
        """è¨­ç½® Gemini API"""
        try:
            # ä½¿ç”¨APIå¯†é‘°ç®¡ç†å™¨
            api_key = get_api_key()
            if not api_key:
                print("è­¦å‘Š: æœªæ‰¾åˆ°å¯ç”¨çš„APIå¯†é‘°ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬API")
                return
            
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash')
            self.api_keys_count = get_api_keys_count()
            
            print(f"Gemini API åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨å¯†é‘°æ•¸é‡: {self.api_keys_count}")
        except Exception as e:
            print(f"Gemini API åˆå§‹åŒ–å¤±æ•—: {e}")
            self.model = None
            self.api_keys_count = 0
    
    def load_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥åŸå§‹æ•¸æ“š"""
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"æˆåŠŸè¼‰å…¥ {len(data)} ç­†æ•¸æ“š")
            return data
        except Exception as e:
            print(f"è¼‰å…¥æ•¸æ“šå¤±æ•—: {e}")
            return []
    
    def count_school_questions(self, data: List[Dict[str, Any]]):
        """çµ±è¨ˆå„å­¸æ ¡çš„é¡Œç›®æ•¸é‡"""
        for item in data:
            school = item.get('school', '')
            if school not in self.school_stats:
                self.school_stats[school] = 0
            self.school_stats[school] += 1
    
    def process_single_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†å–®å€‹æ•¸æ“šé …ç›®ï¼ˆç”¨æ–¼å¤šåŸ·è¡Œç·’ï¼‰"""
        try:
            if self.mode == "filter":
                # éæ¿¾æ¨¡å¼ï¼šåªä¿ç•™æ•¸æ“šï¼Œä¸é€²è¡ŒAIè™•ç†
                self.processed_count += 1
                return item
            else:  # ai_judge æ¨¡å¼
                # 1. é è™•ç†é¡Œç›®æ ¼å¼
                processed_item = self.preprocess_question_format(item)
                
                # 2. æª¢æŸ¥æ˜¯å¦éœ€è¦AIåˆ¤æ–·
                if self.needs_ai_judgment(processed_item):
                    # 3. ä½¿ç”¨AIé‡æ–°åˆ¤æ–·
                    ai_result = self.ai_judge_question(processed_item)
                    if ai_result:
                        processed_item.update(ai_result)
                        self.ai_judged_count += 1
                    else:
                        # AIåˆ¤æ–·å¤±æ•—ï¼Œä¿ç•™åŸå§‹æ•¸æ“š
                        print(f"âš ï¸ AIåˆ¤æ–·å¤±æ•—ï¼Œä¿ç•™åŸå§‹æ•¸æ“š")
                        self.error_count += 1
                else:
                    # ä¸éœ€è¦AIåˆ¤æ–·ï¼Œç›´æ¥éæ¿¾ç§»é™¤çš„é ˜åŸŸ
                    processed_item = self.filter_removed_domains(processed_item)
                
                self.processed_count += 1
                return processed_item
                
        except Exception as e:
            print(f"âŒ è™•ç†å–®å€‹é …ç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            self.error_count += 1
            return item  # è¿”å›åŸå§‹æ•¸æ“š
    
    def should_filter_item(self, item: Dict[str, Any]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²éæ¿¾è©²ç­†æ•¸æ“š"""
        school = item.get('school', '')
        year = item.get('year', '')
        
        # 1. æª¢æŸ¥æ˜¯å¦ç‚ºéœ€è¦ç§»é™¤çš„å­¸æ ¡
        if school in self.schools_to_remove:
            return True
        
        # 2. æª¢æŸ¥å¹´ä»½æ˜¯å¦ç‚º106å¹´ä¹‹å‰
        try:
            year_int = int(year)
            if year_int < self.min_year:
                return True
        except (ValueError, TypeError):
            # å¦‚æœå¹´ä»½ç„¡æ³•è½‰æ›ï¼Œä¿ç•™è©²ç­†æ•¸æ“š
            pass
        
        return False
    
    def preprocess_question_format(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """é è™•ç†é¡Œç›®æ ¼å¼ï¼šå°‡ésingle/groupè½‰æ›ç‚ºsingleæ ¼å¼"""
        item_type = item.get('type', '')
        
        # å¦‚æœå·²ç¶“æ˜¯singleæˆ–groupï¼Œç›´æ¥è¿”å›
        if item_type in ['single', 'group']:
            return item
        
        # å‰µå»ºæ–°çš„é¡Œç›®æ ¼å¼
        new_item = item.copy()
        
        # å°‡åŸæœ¬çš„typeä¿å­˜åˆ°answer_type
        original_type = item_type if item_type else 'unknown'
        new_item['answer_type'] = original_type
        
        # è¨­ç½®ç‚ºsingleæ ¼å¼
        new_item['type'] = 'single'
        
        # å¦‚æœåŸæœ¬æ²’æœ‰question_textï¼Œå˜—è©¦å¾å…¶ä»–å­—æ®µç²å–
        if 'question_text' not in new_item or not new_item['question_text']:
            # å˜—è©¦å¾å…¶ä»–å¯èƒ½çš„å­—æ®µç²å–é¡Œç›®å…§å®¹
            possible_fields = ['content', 'text', 'description', 'title']
            for field in possible_fields:
                if field in new_item and new_item[field]:
                    new_item['question_text'] = str(new_item[field])
                    break
        
        # ç¢ºä¿æœ‰answer_typeå­—æ®µ
        if 'answer_type' not in new_item:
            new_item['answer_type'] = 'unknown'
        
        print(f"é è™•ç†é¡Œç›®æ ¼å¼: {original_type} -> single (ä¿å­˜åˆ°answer_type)")
        return new_item
    
    def needs_ai_judgment(self, item: Dict[str, Any]) -> bool:
        """åˆ¤æ–·é¡Œç›®æ˜¯å¦éœ€è¦AIé‡æ–°åˆ¤æ–·ï¼ˆé‡æ–°åˆ†é¡æ‰€æœ‰çŸ¥è­˜é»ï¼‰"""
        # æª¢æŸ¥æ˜¯å¦åŒ…å«éœ€è¦ç§»é™¤çš„å¤§çŸ¥è­˜é»
        key_points = item.get('key-points', [])
        if isinstance(key_points, str):
            key_points = [key_points]
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«éœ€è¦ç§»é™¤çš„å¤§çŸ¥è­˜é»
        for domain in self.domains_to_remove:
            if any(domain in kp for kp in key_points):
                return True
        
        # æª¢æŸ¥ç¾¤çµ„é¡Œçš„å­é¡Œç›®çŸ¥è­˜é»
        if 'sub_questions' in item:
            for sub_q in item['sub_questions']:
                sub_key_points = sub_q.get('key-points', [])
                if isinstance(sub_key_points, str):
                    sub_key_points = [sub_key_points]
                for domain in self.domains_to_remove:
                    if any(domain in kp for kp in sub_key_points):
                        return True
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ micro_concepts å­—æ®µï¼Œå¦‚æœæ²’æœ‰å‰‡éœ€è¦AIé‡æ–°åˆ†é¡
        if 'micro_concepts' not in item:
            return True
        
        # æª¢æŸ¥ç¾¤çµ„é¡Œçš„å­é¡Œç›®æ˜¯å¦æœ‰ micro_concepts å­—æ®µ
        if 'sub_questions' in item:
            for sub_q in item['sub_questions']:
                if 'micro_concepts' not in sub_q:
                    return True
        
        return False
    
    
    def ai_judge_question(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨AIé‡æ–°åˆ¤æ–·é¡Œç›®æ ¼å¼å’ŒçŸ¥è­˜é»"""
        if not GEMINI_AVAILABLE:
            print("AI API ä¸å¯ç”¨")
            return None
        
        try:
            # ç²å–æ–°çš„APIå¯†é‘°
            api_key = get_api_key()
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash')
            
            # æ§‹å»ºAIåˆ¤æ–·æç¤ºè©
            prompt = self.build_judgment_prompt(question_data)
            
            # èª¿ç”¨ Gemini API
            response = model.generate_content(
                prompt,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            )
            
            # è§£æAIå›æ‡‰
            result = self.parse_ai_judgment(response.text, question_data)
            return result
            
        except Exception as e:
            print(f"AIåˆ¤æ–·å¤±æ•—: {e}")
            return None
    
    def build_judgment_prompt(self, question_data: Dict[str, Any]) -> str:
        """æ§‹å»ºAIåˆ¤æ–·æç¤ºè©"""
        question_text = question_data.get('question_text', '')
        if not question_text and 'group_question_text' in question_data:
            question_text = question_data.get('group_question_text', '')
        
        # æå–çŸ¥è­˜é»ä¿¡æ¯
        key_points = question_data.get('key-points', [])
        if isinstance(key_points, str):
            key_points = [key_points]
        
        # æ§‹å»ºå­é¡Œç›®ä¿¡æ¯
        sub_questions_info = ""
        if 'sub_questions' in question_data:
            for i, sub_q in enumerate(question_data['sub_questions']):
                sub_questions_info += f"\nå­é¡Œ {i+1}: {sub_q.get('question_text', '')[:200]}..."
                sub_key_points = sub_q.get('key-points', [])
                if isinstance(sub_key_points, str):
                    sub_key_points = [sub_key_points]
                sub_questions_info += f"\nå­é¡ŒçŸ¥è­˜é»: {sub_key_points}"
        
        # æ§‹å»ºæ¨™æº–åŒ–çŸ¥è­˜é»é«”ç³»èªªæ˜
        domains_info = ""
        for domain, concepts in self.knowledge_domains.items():
            domains_info += f"\n- {domain}: {', '.join(concepts)}"
        
        prompt = f"""
ä½ æ˜¯ä¸€å€‹æ•™è‚²çŸ¥è­˜é»é‡æ–°åˆ†é¡å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡é¡Œç›®é‡æ–°åˆ†é¡åˆ°æ¨™æº–åŒ–çš„çŸ¥è­˜é»é«”ç³»ä¸­ã€‚

## æ¨™æº–åŒ–çŸ¥è­˜é»é«”ç³»ï¼š
{domains_info}

## åˆ†é¡è¦å‰‡ï¼š
1. **å¤§çŸ¥è­˜é»è½‰æ›**ï¼šå¦‚æœåŸæœ¬å¤§çŸ¥è­˜é»æ˜¯ã€ŒåŸºæœ¬è¨ˆæ¦‚ã€æˆ–ã€Œè¨ˆç®—æ©Ÿæ¦‚è«–ã€ï¼Œå¿…é ˆé‡æ–°åˆ†é¡åˆ°ä¸Šè¿°11å€‹é ˜åŸŸä¸­çš„ä¸€å€‹
2. **å”¯ä¸€å¤§çŸ¥è­˜é»**ï¼šæ¯å€‹é¡Œç›®åªèƒ½æœ‰ä¸€å€‹å¤§çŸ¥è­˜é»ï¼ˆkey-pointsï¼‰
3. **å°çŸ¥è­˜é»éš¸å±¬**ï¼šå°çŸ¥è­˜é»ï¼ˆmicro_conceptsï¼‰å¿…é ˆéš¸å±¬æ–¼å°æ‡‰çš„å¤§çŸ¥è­˜é»
4. **ä¸€è‡´æ€§æª¢æŸ¥**ï¼šå¤§çŸ¥è­˜é»å’Œå°çŸ¥è­˜é»å¿…é ˆç¬¦åˆé‚è¼¯é—œä¿‚
   - ä¾‹å¦‚ï¼škey-points =ã€Œè³‡æ–™çµæ§‹ã€ï¼Œmicro_concepts ä¸èƒ½æ˜¯ã€Œå‡è¨­æª¢å®šã€ï¼ˆå±¬æ–¼æ•¸å­¸èˆ‡çµ±è¨ˆï¼‰
   - ä½†å…è¨±è·¨é ˜åŸŸè¼”åŠ©æ¦‚å¿µï¼Œä¾‹å¦‚ï¼škey-points =ã€Œè³‡æ–™çµæ§‹ã€ï¼Œmicro_concepts å¯ä»¥æ˜¯ ["é™£åˆ—", "æ™‚é–“è¤‡é›œåº¦", "æ©Ÿç‡åˆ†æ"]
5. **æ™ºèƒ½åˆ†æ**ï¼šAIéœ€è¦ä»”ç´°åˆ†æé¡Œç›®å…§å®¹ï¼Œåˆ¤æ–·æœ€é©åˆçš„å¤§çŸ¥è­˜é»ï¼Œç„¶å¾Œæ¨æ–·è©²é ˜åŸŸä¸‹çš„ç›¸é—œå°çŸ¥è­˜é»ï¼Œè‹¥é¡Œç›®æ¶‰åŠæ•¸ç†é‚è¼¯ã€æ©Ÿç‡æˆ–çµ±è¨ˆï¼Œæ‡‰å„ªå…ˆæ­¸é¡åˆ°ã€Œæ•¸å­¸èˆ‡çµ±è¨ˆã€

## è¼¸å‡ºè¦æ±‚ï¼š
- ä¿æŒåŸå§‹è³‡æ–™çµæ§‹ä¸è®Š
- é‡æ–°åˆ†é¡ key-points ç‚ºï¼šå”¯ä¸€çš„å¤§çŸ¥è­˜é»åç¨±ï¼ˆå­—ç¬¦ä¸²ï¼‰
- é‡æ–°åˆ†é¡ micro_concepts ç‚ºï¼šè©²å¤§çŸ¥è­˜é»ä¸‹çš„ç›¸é—œå°çŸ¥è­˜é»åˆ—è¡¨ï¼ˆæ•¸çµ„ï¼‰
- æ ¼å¼ç¯„ä¾‹ï¼š
  {{
    "key-points": "è³‡æ–™çµæ§‹ï¼ˆData Structureï¼‰",
    "micro_concepts": ["ä¸€ç¶­é™£åˆ—", "äºŒç¶­é™£åˆ—", "æ™‚é–“è¤‡é›œåº¦", "ç©ºé–“è¤‡é›œåº¦"]
  }}
- å¿…é ˆç¢ºä¿å¤§çŸ¥è­˜é»å’Œå°çŸ¥è­˜é»çš„é‚è¼¯ä¸€è‡´æ€§
- è¼¸å‡ºç‚º JSON æ ¼å¼

## é¡Œç›®ä¿¡æ¯ï¼š
é¡Œç›®: {question_text[:300]}...
ç•¶å‰çŸ¥è­˜é»: {key_points}
{sub_questions_info}

è«‹åˆ†æä¸¦è¼¸å‡ºé‡æ–°åˆ†é¡å¾Œçš„JSONæ ¼å¼æ•¸æ“šã€‚
"""
        return prompt
    
    def parse_ai_judgment(self, response_text: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æAIåˆ¤æ–·å›æ‡‰"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                return result
            else:
                print("AIå›æ‡‰æ ¼å¼éŒ¯èª¤ï¼Œç„¡æ³•è§£æ")
                return None
        except Exception as e:
            print(f"è§£æAIåˆ¤æ–·å›æ‡‰å¤±æ•—: {e}")
            return None
    
    
    def filter_removed_domains(self, key_points) -> List[str]:
        """éæ¿¾æ‰éœ€è¦ç§»é™¤çš„çŸ¥è­˜é»ï¼Œè®“AIé‡æ–°åˆ¤æ–·"""
        if isinstance(key_points, str):
            key_points = [key_points]
        
        # ç§»é™¤éœ€è¦åˆªé™¤çš„çŸ¥è­˜é»
        filtered_key_points = []
        for kp in key_points:
            kp = kp.strip()
            if kp and not any(domain in kp for domain in self.domains_to_remove):
                filtered_key_points.append(kp)
        
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„çŸ¥è­˜é»ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆè®“AIé‡æ–°åˆ¤æ–·ï¼‰
        if not filtered_key_points:
            return []
        
        # ç›´æ¥è¿”å›éæ¿¾å¾Œçš„çŸ¥è­˜é»åˆ—è¡¨ï¼Œè®“AIä¾†åˆ¤æ–·å’Œåˆ†é¡
        return filtered_key_points
    
    def filter_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éæ¿¾æ•¸æ“šï¼šç§»é™¤æŒ‡å®šå­¸æ ¡å’Œ106å¹´ä¹‹å‰çš„æ•¸æ“š"""
        # å…ˆçµ±è¨ˆå„å­¸æ ¡çš„é¡Œç›®æ•¸é‡
        self.count_school_questions(data)
        
        print("å„å­¸æ ¡é¡Œç›®çµ±è¨ˆï¼š")
        for school, count in sorted(self.school_stats.items()):
            print(f"  {school}: {count} é¡Œ")
        print()
        
        filtered_data = []
        filter_reasons = {
            "æŒ‡å®šå­¸æ ¡": 0,
            "106å¹´ä¹‹å‰": 0
        }
        
        for item in data:
            school = item.get('school', '')
            year = item.get('year', '')
            
            # æª¢æŸ¥æ˜¯å¦æ‡‰è©²éæ¿¾
            if self.should_filter_item(item):
                self.filtered_count += 1
                
                # è¨˜éŒ„éæ¿¾åŸå› 
                if school in self.schools_to_remove:
                    filter_reasons["æŒ‡å®šå­¸æ ¡"] += 1
                else:
                    try:
                        year_int = int(year)
                        if year_int < self.min_year:
                            filter_reasons["106å¹´ä¹‹å‰"] += 1
                    except (ValueError, TypeError):
                        pass
                continue
            
            filtered_data.append(item)
        
        print("éæ¿¾è©³æƒ…ï¼š")
        print(f"  ç§»é™¤æŒ‡å®šå­¸æ ¡: {filter_reasons['æŒ‡å®šå­¸æ ¡']} é¡Œ")
        print(f"  ç§»é™¤106å¹´ä¹‹å‰: {filter_reasons['106å¹´ä¹‹å‰']} é¡Œ")
        
        print(f"\néæ¿¾å®Œæˆï¼šç§»é™¤ {self.filtered_count} ç­†æ•¸æ“šï¼Œä¿ç•™ {len(filtered_data)} ç­†æ•¸æ“š")
        return filtered_data
    
    def call_ai_api(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """èª¿ç”¨AI APIæª¢æŸ¥å’Œä¿®æ­£çŸ¥è­˜é»ï¼ˆéæ¿¾æ¨¡å¼ï¼‰"""
        if not GEMINI_AVAILABLE:
            return self.mock_ai_response(question_data)
        
        try:
            # ç²å–æ–°çš„APIå¯†é‘°
            api_key = get_api_key()
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash')
            
            # æ§‹å»ºæç¤ºè©
            prompt = self.build_knowledge_point_prompt(question_data)
            
            # èª¿ç”¨ Gemini API
            response = model.generate_content(
                prompt,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            )
            
            # è§£æå›æ‡‰
            result = self.parse_ai_response(response.text, question_data)
            return result
            
        except Exception as e:
            print(f"AI API èª¿ç”¨å¤±æ•—: {e}")
            return self.mock_ai_response(question_data)
    
    def build_knowledge_point_prompt(self, question_data: Dict[str, Any]) -> str:
        """æ§‹å»ºçŸ¥è­˜é»ä¿®æ­£AIæç¤ºè©"""
        # æå–é¡Œç›®ä¿¡æ¯
        question_text = question_data.get('question_text', '')
        if not question_text and 'group_question_text' in question_data:
            question_text = question_data.get('group_question_text', '')
        
        # æå–çŸ¥è­˜é»
        key_points = question_data.get('key-points', [])
        if isinstance(key_points, str):
            key_points = [key_points]
        
        # æ§‹å»ºå­é¡Œç›®ä¿¡æ¯
        sub_questions_info = ""
        if 'sub_questions' in question_data:
            for i, sub_q in enumerate(question_data['sub_questions']):
                sub_questions_info += f"\nå­é¡Œ {i+1}: {sub_q.get('question_text', '')[:100]}..."
                sub_key_points = sub_q.get('key-points', [])
                if isinstance(sub_key_points, str):
                    sub_key_points = [sub_key_points]
                sub_questions_info += f"\nå­é¡ŒçŸ¥è­˜é»: {sub_key_points}"
        
        prompt = f"""
ä½ æ˜¯ä¸€å€‹æ•™è‚²çŸ¥è­˜é»å°æ‡‰æª¢æŸ¥å™¨ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šé¡Œç›®å°æ‡‰çš„å¤§çŸ¥è­˜é»ï¼ˆkey_pointï¼‰å’Œå°çŸ¥è­˜é»ï¼ˆsub_pointï¼‰ï¼Œç¢ºä¿è³‡æ–™ç¬¦åˆä»¥ä¸‹è¦å‰‡ï¼š

1. ä¸€è‡´æ€§æª¢æŸ¥ï¼š
   - å¦‚æœä¸€ç­†è³‡æ–™çš„ key_point = Xï¼Œå‰‡è‡³å°‘è¦æœ‰ä¸€å€‹ sub_point å±¬æ–¼ X çš„å°çŸ¥è­˜é»é›†åˆã€‚
   - ä¸å…è¨±å‡ºç¾ key_point = X å»åªæœ‰ Y é¡å°çŸ¥è­˜é»ï¼ˆY â‰  Xï¼‰ã€‚
   - å…è¨±è·¨çŸ¥è­˜é»ï¼Œä¾‹å¦‚ key_point = Aï¼Œsub_point å¯ä»¥åŒ…å« A-1 å’Œ C-2ã€‚

2. æ›¿æ›è¦å‰‡ï¼š
   - å¦‚æœ key_point = "è¨ˆç®—æ©Ÿæ¦‚è«–"ï¼Œè«‹å°‡å…¶æ›¿æ›ç‚ºå…¶ä»–å¤§çŸ¥è­˜é»ï¼ˆéš¨æ©Ÿæˆ–ä¾ç…§æœ€ç›¸é—œçš„çŸ¥è­˜é»ï¼‰ã€‚
   - æ›¿æ›å¾Œï¼Œè©²è³‡æ–™ä¸­ä¸å†ä¿ç•™ "è¨ˆç®—æ©Ÿæ¦‚è«–" é€™å€‹å¤§çŸ¥è­˜é»ã€‚

3. è¼¸å‡ºæ ¼å¼ï¼š
   - ä¿æŒåŸå§‹è³‡æ–™çµæ§‹ä¸è®Šï¼Œåªä¿®æ­£æˆ–æ›¿æ›çŸ¥è­˜é»ã€‚
   - è«‹è¼¸å‡ºç‚º JSON æ ¼å¼ã€‚

é¡Œç›®ä¿¡æ¯ï¼š
é¡Œç›®: {question_text[:200]}...
ç•¶å‰å¤§çŸ¥è­˜é»: {key_points}
{sub_questions_info}

è«‹åˆ†æä¸¦è¼¸å‡ºä¿®æ­£å¾Œçš„JSONæ ¼å¼æ•¸æ“šã€‚
"""
        return prompt
    
    def parse_ai_response(self, response_text: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æAIå›æ‡‰"""
        try:
            # å˜—è©¦å¾å›æ‡‰ä¸­æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                return result
            else:
                # å¦‚æœç„¡æ³•è§£æJSONï¼Œè¿”å›åŸå§‹æ•¸æ“š
                return original_data
        except Exception as e:
            print(f"è§£æAIå›æ‡‰å¤±æ•—: {e}")
            return original_data
    
    def mock_ai_response(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ“¬AIå›æ‡‰ï¼ˆç•¶APIä¸å¯ç”¨æ™‚ï¼‰"""
        # ç°¡å–®çš„è¦å‰‡æ›¿æ›
        result = question_data.copy()
        
        # æ›¿æ› "è¨ˆç®—æ©Ÿæ¦‚è«–" ç‚ºå…¶ä»–çŸ¥è­˜é»
        if 'key-points' in result:
            if isinstance(result['key-points'], str) and 'è¨ˆç®—æ©Ÿæ¦‚è«–' in result['key-points']:
                replacements = ['ä½œæ¥­ç³»çµ±', 'è³‡æ–™çµæ§‹', 'ç¨‹å¼èªè¨€', 'è³‡æ–™åº«', 'ç¶²è·¯æ¦‚è«–']
                result['key-points'] = random.choice(replacements)
            elif isinstance(result['key-points'], list):
                result['key-points'] = [
                    random.choice(['ä½œæ¥­ç³»çµ±', 'è³‡æ–™çµæ§‹', 'ç¨‹å¼èªè¨€', 'è³‡æ–™åº«', 'ç¶²è·¯æ¦‚è«–']) 
                    if 'è¨ˆç®—æ©Ÿæ¦‚è«–' in kp else kp 
                    for kp in result['key-points']
                ]
        
        # è™•ç†å­é¡Œç›®
        if 'sub_questions' in result:
            for sub_q in result['sub_questions']:
                if 'key-points' in sub_q:
                    if isinstance(sub_q['key-points'], str) and 'è¨ˆç®—æ©Ÿæ¦‚è«–' in sub_q['key-points']:
                        sub_q['key-points'] = random.choice(['ä½œæ¥­ç³»çµ±', 'è³‡æ–™çµæ§‹', 'ç¨‹å¼èªè¨€'])
                    elif isinstance(sub_q['key-points'], list):
                        sub_q['key-points'] = [
                            random.choice(['ä½œæ¥­ç³»çµ±', 'è³‡æ–™çµæ§‹', 'ç¨‹å¼èªè¨€']) 
                            if 'è¨ˆç®—æ©Ÿæ¦‚è«–' in kp else kp 
                            for kp in sub_q['key-points']
                        ]
        
        return result
    
    def process_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è™•ç†æ•¸æ“š"""
        processed_data = []
        
        for i, item in enumerate(data):
            try:
                if self.mode == "filter":
                    # éæ¿¾æ¨¡å¼ï¼šåªä¿ç•™æ•¸æ“šï¼Œä¸é€²è¡ŒAIè™•ç†
                    processed_data.append(item)
                    self.processed_count += 1
                else:  # ai_judge æ¨¡å¼
                    # AIåˆ¤æ–·æ¨¡å¼ï¼šå…ˆé è™•ç†æ ¼å¼ï¼Œå†é‡æ–°åˆ¤æ–·çŸ¥è­˜é»
                    # 1. é è™•ç†é¡Œç›®æ ¼å¼
                    processed_item = self.preprocess_question_format(item)
                    
                    # 2. æª¢æŸ¥æ˜¯å¦éœ€è¦AIåˆ¤æ–·
                    if self.needs_ai_judgment(processed_item):
                        processed_item = self.ai_judge_question(processed_item)
                        self.ai_judged_count += 1
                    
                    processed_data.append(processed_item)
                    self.processed_count += 1
                
                if (i + 1) % 100 == 0:
                    if self.mode == "ai_judge":
                        print(f"å·²è™•ç† {i + 1}/{len(data)} ç­†æ•¸æ“šï¼ŒAIåˆ¤æ–· {self.ai_judged_count} ç­†")
                    else:
                        print(f"å·²è™•ç† {i + 1}/{len(data)} ç­†æ•¸æ“š")
                    
            except Exception as e:
                print(f"è™•ç†ç¬¬ {i + 1} ç­†æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
                processed_data.append(item)  # ä¿ç•™åŸå§‹æ•¸æ“š
                self.error_count += 1
        
        return processed_data
    
    def save_data(self, data: List[Dict[str, Any]]):
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        try:
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
            
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"æ•¸æ“šå·²ä¿å­˜åˆ° {self.output_file}")
            print(f"ç¸½è¨ˆè™•ç† {self.processed_count} ç­†æ•¸æ“š")
            print(f"éæ¿¾æ‰ {self.filtered_count} ç­†æ•¸æ“š")
            print(f"è™•ç†éŒ¯èª¤ {self.error_count} ç­†æ•¸æ“š")
            
        except Exception as e:
            print(f"ä¿å­˜æ•¸æ“šå¤±æ•—: {e}")
    

    def run(self):
        """åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šè™•ç†æµç¨‹"""
        mode_name = "æ•¸æ“šéæ¿¾" if self.mode == "filter" else "AIé‡æ–°åˆ¤æ–·"
        print(f"ğŸš€ é–‹å§‹{mode_name}...")
        print(f"ğŸ“ è¼¸å…¥æ–‡ä»¶: {self.input_file}")
        print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {self.output_file}")
        print("-" * 60)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.input_file):
            print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ {self.input_file}")
            return
        
        # 1. è¼‰å…¥æ•¸æ“š
        print("ğŸ“Š æ­£åœ¨è¼‰å…¥æ•¸æ“š...")
        data = self.load_data()
        if not data:
            print("âŒ æ²’æœ‰æ•¸æ“šéœ€è¦è™•ç†")
            return
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(data)} ç­†æ•¸æ“š")
        
        if self.mode == "filter":
            # 2. éæ¿¾æ•¸æ“š
            print("ğŸ” æ­£åœ¨éæ¿¾æ•¸æ“š...")
            filtered_data = self.filter_data(data)
            if not filtered_data:
                print("âŒ éæ¿¾å¾Œæ²’æœ‰æ•¸æ“šéœ€è¦è™•ç†")
                return
            
            print(f"âœ… éæ¿¾å®Œæˆï¼Œä¿ç•™ {len(filtered_data)} ç­†æ•¸æ“š")
            
            # 3. ä¿å­˜éæ¿¾å¾Œçš„æ•¸æ“š
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜éæ¿¾å¾Œçš„æ•¸æ“š...")
            self.save_data(filtered_data)
            print("âœ… æ•¸æ“šéæ¿¾å®Œæˆï¼")
            
        else:  # ai_judge æ¨¡å¼
            # 2. ä½¿ç”¨å¤šåŸ·è¡Œç·’è™•ç†AIåˆ¤æ–·
            print("ğŸ¤– æ­£åœ¨é€²è¡ŒAIé‡æ–°åˆ¤æ–·...")
            print("â³ é€™å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
            processed_data = self.process_with_multithreading(data, self.process_single_item)
            
            # 3. ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š...")
            self.save_data(processed_data)
            print("âœ… AIé‡æ–°åˆ¤æ–·å®Œæˆï¼")
        
        # 4. é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        self.print_statistics()
    
    def print_statistics(self):
        """æ‰“å°çµ±è¨ˆä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è™•ç†çµ±è¨ˆ")
        print("=" * 60)
        print(f"ğŸ“ ç¸½è™•ç†æ•¸é‡: {self.processed_count}")
        print(f"ğŸ” éæ¿¾æ•¸é‡: {self.filtered_count}")
        print(f"ğŸ¤– AIåˆ¤æ–·æ•¸é‡: {self.ai_judged_count}")
        print(f"âŒ éŒ¯èª¤æ•¸é‡: {self.error_count}")
        
        if self.school_stats:
            print("\nğŸ« å­¸æ ¡çµ±è¨ˆ:")
            for school, count in sorted(self.school_stats.items(), key=lambda x: x[1], reverse=True):
                status = "âœ…" if school not in self.schools_to_remove else "âŒ"
                print(f"  {status} {school}: {count} é¡Œ")
        
        print("\n" + "=" * 60)
    
    def ai_convert_knowledge_points(self, data: List[Dict[str, Any]]):
        """ä½¿ç”¨AIè½‰æ›çŸ¥è­˜é»ï¼ˆå¤šåŸ·è¡Œç·’ç‰ˆæœ¬ï¼‰"""
        if not GEMINI_AVAILABLE or not self.model:
            print("AI API ä¸å¯ç”¨ï¼Œè·³éçŸ¥è­˜é»è½‰æ›")
            return
        
        print(f"é–‹å§‹AIçŸ¥è­˜é»è½‰æ›ï¼Œå…± {len(data)} ç­†æ•¸æ“š...")
        print(f"ä½¿ç”¨ {self.api_keys_count} å€‹APIå¯†é‘°é€²è¡Œä¸¦è¡Œè™•ç†")
        
        # ä½¿ç”¨å¤šåŸ·è¡Œç·’è™•ç†
        converted_data = self.process_with_multithreading(data, self.call_ai_api)
        
        # ä¿å­˜è½‰æ›å¾Œçš„æ•¸æ“š
        ai_output_file = "../data/20250917_ai_converted.json"
        try:
            os.makedirs(os.path.dirname(ai_output_file), exist_ok=True)
            with open(ai_output_file, 'w', encoding='utf-8') as f:
                json.dump(converted_data, f, ensure_ascii=False, indent=2)
            print(f"AIçŸ¥è­˜é»è½‰æ›å®Œæˆï¼å…±è½‰æ› {len(converted_data)} ç­†æ•¸æ“š")
            print(f"çµæœå·²ä¿å­˜åˆ°: {ai_output_file}")
        except Exception as e:
            print(f"ä¿å­˜AIè½‰æ›çµæœå¤±æ•—: {e}")
    
    def process_with_multithreading(self, data: List[Dict[str, Any]], process_func) -> List[Dict[str, Any]]:
        """å¤šåŸ·è¡Œç·’è™•ç†æ•¸æ“š"""
        if not data:
            return []
        
        # è¨ˆç®—æ¯å€‹åŸ·è¡Œç·’è™•ç†çš„æ•¸æ“šé‡
        max_workers = min(self.api_keys_count, len(data), 8)  # æœ€å¤š8å€‹åŸ·è¡Œç·’
        batch_size = max(1, len(data) // max_workers)
        
        print(f"ğŸ”„ ä½¿ç”¨ {max_workers} å€‹åŸ·è¡Œç·’ï¼Œæ¯æ‰¹è™•ç† {batch_size} ç­†æ•¸æ“š")
        
        # å°‡æ•¸æ“šåˆ†æ‰¹
        batches = []
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batches.append((i, batch))
        
        # ä½¿ç”¨åŸ·è¡Œç·’æ± è™•ç†
        results = [None] * len(data)
        completed_count = 0
        lock = Lock()
        
        def process_batch(batch_info):
            batch_start, batch_data = batch_info
            batch_results = []
            
            for item in batch_data:
                try:
                    result = process_func(item)
                    batch_results.append(result)
                except Exception as e:
                    print(f"âŒ è™•ç†æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
                    batch_results.append(item)  # ä¿ç•™åŸå§‹æ•¸æ“š
            
            # æ›´æ–°çµæœ
            with lock:
                nonlocal completed_count
                for i, result in enumerate(batch_results):
                    results[batch_start + i] = result
                completed_count += len(batch_data)
                if completed_count % 50 == 0 or completed_count == len(data):
                    progress = (completed_count / len(data)) * 100
                    print(f"â³ é€²åº¦: {completed_count}/{len(data)} ({progress:.1f}%)")
            
            return batch_results
        
        # åŸ·è¡Œå¤šåŸ·è¡Œç·’è™•ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»å‹™
            future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
            for future in as_completed(future_to_batch):
                try:
                    future.result()
                except Exception as e:
                    print(f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
        
        return results

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("å­¸ç¿’åˆ†ææ•¸æ“šè™•ç†å·¥å…·")
    print("=" * 60)
    print()
    
    # é¸æ“‡è™•ç†æ¨¡å¼
    print("è«‹é¸æ“‡è™•ç†æ¨¡å¼ï¼š")
    print("1. æ•¸æ“šéæ¿¾æ¨¡å¼ - ç§»é™¤æŒ‡å®šå­¸æ ¡å’Œå¹´ä»½çš„æ•¸æ“š")
    print("2. AIé‡æ–°åˆ¤æ–·æ¨¡å¼ - é‡æ–°åˆ¤æ–·é¡Œç›®æ ¼å¼å’ŒçŸ¥è­˜é»åˆ†é¡")
    print()
    print("å»ºè­°æµç¨‹ï¼šå…ˆåŸ·è¡Œæ¨¡å¼1ï¼Œå†åŸ·è¡Œæ¨¡å¼2")
    print()
    
    while True:
        choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
        if choice == "1":
            mode = "filter"
            break
        elif choice == "2":
            mode = "ai_judge"
            break
        else:
            print("ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 1 æˆ– 2")
    
    # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    input_file = "../data/fainaldata_no_del.json"
    if not os.path.exists(input_file):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ {input_file}")
        print("è«‹ç¢ºä¿æ–‡ä»¶å­˜åœ¨æ–¼æ­£ç¢ºä½ç½®")
        return
    
    # æª¢æŸ¥æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(input_file) / (1024 * 1024)  # MB
    print(f"è¼¸å…¥æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    if file_size > 100:  # å¤§æ–¼100MB
        print("è­¦å‘Š: æ–‡ä»¶è¼ƒå¤§ï¼Œè™•ç†å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“")
        response = input("æ˜¯å¦ç¹¼çºŒï¼Ÿ(y/N): ")
        if response.lower() != 'y':
            print("å·²å–æ¶ˆè™•ç†")
            return
    
    print()
    print("é–‹å§‹è™•ç†æ•¸æ“š...")
    print("-" * 60)
    
    # åŸ·è¡Œæ•¸æ“šè™•ç†
    processor = DataProcessor(mode=mode)
    processor.run()
    
    print()
    print("=" * 60)
    print("è™•ç†å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()

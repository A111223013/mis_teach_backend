import requests
from lxml import html
import json
import time
import os
from flask import Blueprint, jsonify, request

# å»ºç«‹ Blueprint
ithome_bp = Blueprint("ithome", __name__)

# ======== è¨­å®šå€ ========
BASE_URL = "https://www.ithome.com.tw/latest?page={}"
OUTPUT_FILE = "data/ithome_news.json"  # ä½ çš„æŒ‡å®šè·¯å¾‘
TOTAL_PAGES = 15  # è¦çˆ¬çš„é æ•¸
DELAY_SECONDS = 1  # æ¯é å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰
# =========================

def crawl_ithome_news():
    """çˆ¬å– iThome æ–°èçš„å‡½æ•¸"""
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    all_news = []
    
    for page in range(1, TOTAL_PAGES + 1):
        url = BASE_URL.format(page)
        print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é : {url}")
        
        try:
            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            response.encoding = "utf-8"
            tree = html.fromstring(response.text)
            
            items = tree.xpath('/html/body/div[4]/div/section/div/div[1]/div')
            print(f"  â””â”€ å…±æ‰¾åˆ° {len(items)} ç­†æ–°è")
            
            for i, item in enumerate(items, start=1):
                try:
                    # image
                    img = item.xpath(f'./div/span/div/p[1]/a/img/@src')
                    img_url = img[0] if img else ""
                    
                    # image href
                    img_href = item.xpath(f'./div/span/div/p[1]/a/@href')
                    href = img_href[0] if img_href else ""
                    
                    # tags
                    tags = item.xpath(f'./div/span/div/p[2]/a')
                    tags_data = [{"text": t.text_content().strip(), "href": t.get("href")} for t in tags]
                    
                    # title
                    title_elem = item.xpath(f'./div/span/div/p[3]/a')
                    title_data = {"text": "", "href": ""}
                    if title_elem:
                        title_data["text"] = title_elem[0].text_content().strip()
                        title_data["href"] = title_elem[0].get("href")
                    
                    # summary
                    summary_elem = item.xpath(f'./div/span/div/div/p/text()')
                    summary_text = summary_elem[0].strip() if summary_elem else ""
                    
                    # date
                    date_elem = item.xpath(f'./div/span/div/p[4]/text()')
                    date_text = date_elem[0].strip() if date_elem else ""
                    
                    all_news.append({
                        "image": img_url,
                        "href": href,
                        "tags": tags_data,
                        "title": title_data,
                        "summary": summary_text,
                        "date": date_text
                    })
                    
                except Exception as e:
                    print(f"âŒ ç¬¬ {page} é ç¬¬ {i} ç­†ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            time.sleep(DELAY_SECONDS)
            
        except Exception as e:
            print(f"âŒ çˆ¬å–ç¬¬ {page} é æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    # å„²å­˜æˆ JSON æª”æ¡ˆ
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… çˆ¬å–å®Œæˆï¼Œå…± {len(all_news)} ç­†æ–°èå·²å„²å­˜åˆ°ï¼š{OUTPUT_FILE}")
    return all_news

@ithome_bp.route('/crawl', methods=['POST'])
def crawl_news():
    """æ‰‹å‹•è§¸ç™¼çˆ¬èŸ²çš„ API ç«¯é»"""
    try:
        news_data = crawl_ithome_news()
        return jsonify({
            "success": True,
            "message": f"æˆåŠŸçˆ¬å– {len(news_data)} ç­†æ–°è",
            "count": len(news_data),
            "data_file": OUTPUT_FILE
        })
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}"
        }), 500

@ithome_bp.route('/news', methods=['GET'])
def get_news():
    """å–å¾—å·²çˆ¬å–çš„æ–°èè³‡æ–™"""
    try:
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                news_data = json.load(f)
            return jsonify({
                "success": True,
                "count": len(news_data),
                "data": news_data
            })
        else:
            return jsonify({
                "success": False,
                "message": "å°šæœªçˆ¬å–æ–°èè³‡æ–™ï¼Œè«‹å…ˆåŸ·è¡Œçˆ¬èŸ²"
            }), 404
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"è®€å–æ–°èè³‡æ–™å¤±æ•—: {str(e)}"
        }), 500

@ithome_bp.route('/status', methods=['GET'])
def get_status():
    """æª¢æŸ¥çˆ¬èŸ²ç‹€æ…‹"""
    try:
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                news_data = json.load(f)
            file_stats = os.stat(OUTPUT_FILE)
            return jsonify({
                "success": True,
                "has_data": True,
                "count": len(news_data),
                "last_modified": time.ctime(file_stats.st_mtime),
                "file_path": OUTPUT_FILE
            })
        else:
            return jsonify({
                "success": True,
                "has_data": False,
                "message": "å°šæœªçˆ¬å–æ–°èè³‡æ–™"
            })
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"æª¢æŸ¥ç‹€æ…‹å¤±æ•—: {str(e)}"
        }), 500

# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆï¼Œå‰‡åŸ·è¡Œçˆ¬èŸ²
if __name__ == "__main__":
    crawl_ithome_news()

# ç•¶æ¨¡çµ„è¢«åŒ¯å…¥æ™‚è‡ªå‹•åŸ·è¡Œçˆ¬èŸ²
crawl_ithome_news()

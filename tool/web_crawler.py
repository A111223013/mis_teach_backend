import requests
from lxml import html
import json
import time
import os

# ======== è¨­å®šå€ ========
BASE_URL = "https://www.ithome.com.tw/latest?page={}"
OUTPUT_FILE = "data/ithome_news.json"  # ä½ çš„æŒ‡å®šè·¯å¾‘
TOTAL_PAGES = 15  # è¦çˆ¬çš„é æ•¸
DELAY_SECONDS = 1  # æ¯é å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰
# =========================

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

all_news = []

for page in range(1, TOTAL_PAGES + 1):
    url = BASE_URL.format(page)
    print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é : {url}")

    response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    response.encoding = "utf-8"
    tree = html.fromstring(response.text)

    items = tree.xpath('/html/body/div[4]/div/section/div/div[1]/div')
    print(f"  â””â”€ å…±æ‰¾åˆ° {len(items)} ç­†æ–°è")

    for i, item in enumerate(items, start=1):
        try:
            # image
            img = item.xpath(f'./div/span/div/p[1]/a/img/@src')
            img_url = img[0] if img else ""

            # image href
            img_href = item.xpath(f'./div/span/div/p[1]/a/@href')
            href = img_href[0] if img_href else ""

            # tags
            tags = item.xpath(f'./div/span/div/p[2]/a')
            tags_data = [{"text": t.text_content().strip(), "href": t.get("href")} for t in tags]

            # title
            title_elem = item.xpath(f'./div/span/div/p[3]/a')
            title_data = {"text": "", "href": ""}
            if title_elem:
                title_data["text"] = title_elem[0].text_content().strip()
                title_data["href"] = title_elem[0].get("href")

            # summary
            summary_elem = item.xpath(f'./div/span/div/div/p/text()')
            summary_text = summary_elem[0].strip() if summary_elem else ""

            # date
            date_elem = item.xpath(f'./div/span/div/p[4]/text()')
            date_text = date_elem[0].strip() if date_elem else ""

            all_news.append({
                "image": img_url,
                "href": href,
                "tags": tags_data,
                "title": title_data,
                "summary": summary_text,
                "date": date_text
            })

        except Exception as e:
            print(f"âŒ ç¬¬ {page} é ç¬¬ {i} ç­†ç™¼ç”ŸéŒ¯èª¤: {e}")

    time.sleep(DELAY_SECONDS)

# å„²å­˜æˆ JSON æª”æ¡ˆ
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_news, f, ensure_ascii=False, indent=2)

print(f"\nâœ… çˆ¬å–å®Œæˆï¼Œå…± {len(all_news)} ç­†æ–°èå·²å„²å­˜åˆ°ï¼š{OUTPUT_FILE}")

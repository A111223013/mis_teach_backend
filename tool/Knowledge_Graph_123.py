import json
import random
import threading
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from bson import ObjectId
from tqdm import tqdm
import time
from accessories import init_gemini

# ====== API å¯†é’¥ ======
API_KEYS = [
   "AIzaSyC8y6nInv339tG3j2jwFfd2W3lU1A6aoBg", "AIzaSyAgJI1A8MCEIbvMtuyhWoqvVL1ffDPWjBs", "AIzaSyA0qRAxFFrtL7CljNpDG0YV8JIZEdHBI5c", "AIzaSyD1mJZjj7GWLhDYAgXk-BR9DJf_yTJzSMw",
    "AIzaSyCHSr3_LuQSO5ySj35hdWjhQosKydhntL8", "AIzaSyDDE9KH0cgNsKPhAg0PQIubqdWzA6PICrk", "AIzaSyBUmLCxZrfFCbrHCM_ewEE3vMjCh6l0f1U", "AIzaSyA7siZ-4_k-5nv7Qn2nZF8kffH70wOWwXw",
    "AIzaSyBWoJDc_IcSLIjg42H853McDRO3EpxbeUk", "AIzaSyBUh-IkyMOlf0rrHBmI9Q3TxH9fAIjLV9o", "AIzaSyCCVcBTUtrD_-iyCnVXOxHJ3tVLwzx5pQQ", "AIzaSyDwbI0eFslXKhACHt2GvmjkbEtbuNs7mbQ",
    "YAIzaSyApssNiwMT5fQLEje01yP9sx_-fsSFiIvo", "AIzaSyCLJGdv4IpimC5zCff74cD08R0UfJDGUGY", "AIzaSyBXRswyZbU9GRkQZ0J4QAvbgTNr8TGh7mI", "AIzaSyD1fduo6U9ggGCz4K1vp58m7-WegKIug6E"
]

key_lock = threading.Lock()
key_index = 0
api_error_count = 0
max_api_errors = 1000  # å¢åŠ é”™è¯¯é™åˆ¶

def set_api_key():
    global key_index
    with key_lock:
        api_key = API_KEYS[key_index]
        key_index = (key_index + 1) % len(API_KEYS)
    # ä½¿ç”¨ accessories ä¸­çš„ init_gemini å‡½æ•¸
    return init_gemini('gemini-1.5-flash')

def extract_json_from_text(text):
    if not text:
        return {}
    
    try:
        return json.loads(text)
    except:
        pass
    
    json_match = re.search(r'\{[\s\S]*\}', text)
    if json_match:
        try:
            return json.loads(json_match.group())
        except:
            pass
    
    return {}

def safe_json_parse(text):
    try:
        result = extract_json_from_text(text)
        if result:
            return result
        
        text = text.replace("'", '"')
        lines = text.split('\n')
        json_lines = []
        in_json = False
        
        for line in lines:
            if '{' in line or '[' in line:
                in_json = True
            if in_json:
                json_lines.append(line)
            if '}' in line or ']' in line:
                in_json = False
        
        json_text = '\n'.join(json_lines)
        return json.loads(json_text)
        
    except Exception as e:
        return {}

def match_from_list(text, valid_list):
    if not text or not valid_list:
        return random.choice(valid_list) if valid_list else ""
    
    text_lower = str(text).lower()
    for item in valid_list:
        if str(item).lower() in text_lower:
            return item
    return random.choice(valid_list) if valid_list else ""

# ====== å›ºå®šæ¸…å• ======
domains_list = [
    "æ•¸ä½é‚è¼¯","ä½œæ¥­ç³»çµ±","ç¨‹å¼èªè¨€","è³‡æ–™çµæ§‹","ç¶²è·¯","è³‡æ–™åº«",
    "AIèˆ‡æ©Ÿå™¨å­¸ç¿’","è³‡è¨Šå®‰å…¨","é›²ç«¯èˆ‡è™›æ“¬åŒ–","ç®¡ç†è³‡è¨Šç³»çµ±ï¼ˆMISï¼‰","è»Ÿé«”å·¥ç¨‹èˆ‡ç³»çµ±é–‹ç™¼"
]

blocks_list = [
    # è³‡æ–™åº«ç¶œåˆ
    "è³‡æ–™åº«åŸºç¤èˆ‡SQLæŸ¥è©¢ (SQLåŸºç¤, æŸ¥è©¢é€²éš)",
    "è³‡æ–™åº«è¨­è¨ˆèˆ‡æ­£è¦åŒ– (æ­£è¦åŒ–èˆ‡è¨­è¨ˆ)",
    "è³‡æ–™åº«äº¤æ˜“èˆ‡æ•ˆèƒ½ç®¡ç† (äº¤æ˜“ç®¡ç†, ç´¢å¼•èˆ‡æ•ˆèƒ½èª¿æ ¡)",
    "ç¾ä»£è³‡æ–™åº«ç³»çµ± (åˆ†æ•£å¼è³‡æ–™åº«èˆ‡é›²ç«¯, æ–°èˆˆè­°é¡Œ)",
    "è³‡æ–™åº«å®‰å…¨èˆ‡ç¶­è­· (è³‡æ–™åº«å®‰å…¨èˆ‡å‚™æ´)",

    # äººå·¥æ™ºæ…§ç¶œåˆ
    "äººå·¥æ™ºæ…§åŸºç¤ (äººå·¥æ™ºæ…§æ¦‚è«–, æ©Ÿå™¨å­¸ç¿’åŸºç¤)",
    "æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³• (å¸¸è¦‹æ¼”ç®—æ³•)",
    "æ·±åº¦å­¸ç¿’æŠ€è¡“ (æ·±åº¦å­¸ç¿’)",
    "è³‡æ–™è™•ç†èˆ‡æ¨¡å‹å„ªåŒ– (ç‰¹å¾µå·¥ç¨‹, æ¨¡å‹è©•ä¼°èˆ‡é©—è­‰)",
    "AIæ‡‰ç”¨èˆ‡å½±éŸ¿ (AIæ‡‰ç”¨é ˜åŸŸ, AIèˆ‡ç¤¾æœƒ)",

    # è³‡è¨Šå®‰å…¨ç¶œåˆ
    "è³‡è¨Šå®‰å…¨åŸºç¤ (è³‡è¨Šå®‰å…¨æ¦‚è«–)",
    "åŠ å¯†æŠ€è¡“èˆ‡æ‡‰ç”¨ (å¯†ç¢¼å­¸åŸºç¤)",
    "ç¶²è·¯èˆ‡ç³»çµ±é˜²è­· (ç¶²è·¯å®‰å…¨, ç³»çµ±å®‰å…¨)",
    "æ‡‰ç”¨å®‰å…¨èˆ‡ç®¡ç† (æ‡‰ç”¨å®‰å…¨, å®‰å…¨ç®¡ç†)",
    "è³‡å®‰æ–°èˆˆè­°é¡Œ (æ–°èˆˆè­°é¡Œ)",

    # é›²ç«¯è¨ˆç®—ç¶œåˆ
    "é›²ç«¯åŸºç¤èˆ‡è™›æ“¬åŒ– (é›²ç«¯è¨ˆç®—æ¦‚è«–, è™›æ“¬åŒ–æŠ€è¡“)",
    "é›²ç«¯æ¶æ§‹èˆ‡å¹³å° (é›²ç«¯åŸºç¤æ¶æ§‹, é›²ç«¯é‹ç®—å¹³å°)",
    "é›²ç«¯å®‰å…¨èˆ‡æ‡‰ç”¨ (é›²ç«¯å®‰å…¨, é›²ç«¯æ‡‰ç”¨)",

    # è³‡è¨Šç®¡ç†ç¶œåˆ
    "è³‡è¨Šç³»çµ±åŸºç¤ (MISæ¦‚è«–, è³‡è¨Šç³»çµ±é¡å‹)",
    "ç³»çµ±é–‹ç™¼èˆ‡çµ„ç¹”å½±éŸ¿ (è³‡è¨Šç³»çµ±é–‹ç™¼èˆ‡å°å…¥, è³‡è¨Šç§‘æŠ€èˆ‡çµ„ç¹”)",
    "æ±ºç­–æ”¯æŒèˆ‡æ–°èˆˆè¶¨å‹¢ (MISèˆ‡æ±ºç­–æ”¯æŒ, MISçš„æ–°èˆˆè­°é¡Œ)",

    # è»Ÿé«”å·¥ç¨‹ç¶œåˆ
    "è»Ÿé«”å·¥ç¨‹åŸºç¤ (è»Ÿé«”å·¥ç¨‹æ¦‚è«–)",
    "é–‹ç™¼æ–¹æ³•èˆ‡æµç¨‹ (è»Ÿé«”é–‹ç™¼æ–¹æ³•)",
    "ç³»çµ±åˆ†æè¨­è¨ˆèˆ‡æ¸¬è©¦ (ç³»çµ±åˆ†æèˆ‡è¨­è¨ˆ, ç‰©ä»¶å°å‘æ–¹æ³•, è»Ÿé«”æ¸¬è©¦èˆ‡é©—è­‰)",
    "è»Ÿé«”ç¶­è­·èˆ‡å“è³ª (è»Ÿé«”ç¶­è­·èˆ‡æ¼”åŒ–, è»Ÿé«”å“è³ªä¿è­‰)",
    "è»Ÿé«”å°ˆæ¡ˆç®¡ç† (è»Ÿé«”å°ˆæ¡ˆç®¡ç†)",

    # æ•¸ä½é‚è¼¯ç¶œåˆ
    "æ•¸ä½é‚è¼¯åŸºç¤ (æ•¸ä½é‚è¼¯æ¦‚è«–)",
    "é‚è¼¯é›»è·¯è¨­è¨ˆ (çµ„åˆé‚è¼¯é›»è·¯, åºå‘é‚è¼¯é›»è·¯)",
    "è¨˜æ†¶é«”èˆ‡ç³»çµ±è¨­è¨ˆ (è¨˜æ†¶é«”å–®å…ƒèˆ‡å„²å­˜, æ•¸ä½ç³»çµ±è¨­è¨ˆ)",

    # ä½œæ¥­ç³»çµ±ç¶œåˆ
    "ä½œæ¥­ç³»çµ±åŸºç¤ (ä½œæ¥­ç³»çµ±æ¦‚è«–)",
    "è³‡æºç®¡ç†æŠ€è¡“ (è¡Œç¨‹ç®¡ç†, è¨˜æ†¶é«”ç®¡ç†, è£ç½®ç®¡ç†èˆ‡I/O)",
    "æª”æ¡ˆç³»çµ±èˆ‡åŒæ­¥ (æª”æ¡ˆç³»çµ±, åŒæ­¥èˆ‡æ­»çµ)",

    # ç¨‹å¼è¨­è¨ˆç¶œåˆ
    "ç¨‹å¼èªè¨€åŸºç¤ (ç¨‹å¼èªè¨€æ¦‚è«–, èªæ³•èˆ‡èªç¾©)",
    "ç¨‹å¼çµæ§‹èˆ‡è¨­è¨ˆ (æ§åˆ¶çµæ§‹, å‡½æ•¸èˆ‡åƒæ•¸å‚³é, ç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆOOP)",
    "è¨˜æ†¶é«”èˆ‡éŒ¯èª¤è™•ç† (è¨˜æ†¶é«”ç®¡ç†, ä¾‹å¤–è™•ç†èˆ‡éŒ¯èª¤è™•ç†)",
    "ç¨‹å¼è¨­è¨ˆå¯¦å‹™ (ç¨‹å¼èªè¨€çš„å¯¦å‹™æ‡‰ç”¨)",

    # è³‡æ–™çµæ§‹èˆ‡æ¼”ç®—æ³•ç¶œåˆ
    "è³‡æ–™çµæ§‹åŸºç¤ (è³‡æ–™çµæ§‹æ¦‚è«–)",
    "æ ¸å¿ƒè³‡æ–™çµæ§‹ (ç·šæ€§è³‡æ–™çµæ§‹, æ¨¹ç‹€è³‡æ–™çµæ§‹, åœ–å½¢è³‡æ–™çµæ§‹)",
    "æ¼”ç®—æ³•æŠ€è¡“ (é›œæ¹Šè¡¨èˆ‡æ¼”ç®—æ³•, æ’åºèˆ‡æœå°‹)",

    # é›»è…¦ç¶²è·¯ç¶œåˆ
    "ç¶²è·¯åŸºç¤èˆ‡æ¶æ§‹ (ç¶²è·¯åŸºç¤æ¦‚å¿µ, ç¶²è·¯æ¨¡å‹)",
    "ç¶²è·¯å”å®šåˆ†å±¤ (å¯¦é«”å±¤èˆ‡è³‡æ–™é€£çµå±¤, ç¶²è·¯å±¤, å‚³è¼¸å±¤, æ‡‰ç”¨å±¤)",
    "ç¶²è·¯å®‰å…¨èˆ‡ç®¡ç† (ç¶²è·¯å®‰å…¨, ç„¡ç·šèˆ‡è¡Œå‹•ç¶²è·¯, ç¶²è·¯ç®¡ç†èˆ‡æ–°èˆˆæŠ€è¡“)"
]

# é€™è£¡æ”¾åˆæ³•çš„å°çŸ¥è­˜é»æ¸…å–®ï¼ˆæˆ‘ç›´æ¥ç¸®çŸ­ï¼Œå¯¦éš›ç”¨ä½ çš„å®Œæ•´æ¸…å–®ï¼‰
micro_concepts_list = [
    # è³‡æ–™åº«åŸºç¤èˆ‡æ¨¡å‹
    "è³‡æ–™åº«åŸºç¤æ¦‚å¿µ (è³‡æ–™/è³‡è¨Š/çŸ¥è­˜, å®šç¾©èˆ‡ç‰¹æ€§, ç³»çµ±çµ„æˆ: DBMS/Database/Application)",
    "è³‡æ–™åº« vs æª”æ¡ˆç³»çµ±çš„æ¯”è¼ƒ",
    "è³‡æ–™æ¨¡å‹æ¼”é€² (éšå±¤å¼, ç¶²è·¯, é—œè¯å¼, ç‰©ä»¶å°å‘, NoSQLèˆ‡æ–°èˆˆæ¨¡å‹)",

    # é—œè¯å¼è³‡æ–™åº«èˆ‡SQL
    "é—œè¯å¼æ¨¡å‹æ ¸å¿ƒæ¦‚å¿µ (Relation/Attribute/Tuple, Primary Key/Foreign Key, å®Œæ•´æ€§ç´„æŸ)",
    "SQLèªè¨€ç¶œåˆ (DDL, DML-SELECT/INSERT/UPDATE/DELETE, DCL-GRANT/REVOKE, TCL-COMMIT/ROLLBACK)",
    "SQLé€²éšæŸ¥è©¢æŠ€è¡“ (èšåˆå‡½æ•¸-COUNT/AVG/SUM/MAX/MIN, å­æŸ¥è©¢, JOIN-Inner/Outer/Cross)",
    "è¦–åœ– (View) çš„æ‡‰ç”¨èˆ‡ç®¡ç†",

    # è³‡æ–™åº«è¨­è¨ˆèˆ‡æ­£è¦åŒ–
    "æ­£è¦åŒ–ç†è«–èˆ‡å¯¦å‹™ (1NF, 2NF, 3NF, BCNF)",
    "åæ­£è¦åŒ– (Denormalization) ç­–ç•¥èˆ‡å–æ¨",

    # äº¤æ˜“è™•ç†èˆ‡ä½µç™¼æ§åˆ¶
    "äº¤æ˜“ç®¡ç†èˆ‡ACIDç‰¹æ€§",
    "ä½µç™¼æ§åˆ¶æ©Ÿåˆ¶ (é–å®šLocking, æ­»çµDeadlockè™•ç†)",
    
    # è³‡æ–™åº«æ•ˆèƒ½èˆ‡å„²å­˜
    "æ•ˆèƒ½èª¿å„ªæŠ€è¡“ (ç´¢å¼•-B-Tree/Hash, æŸ¥è©¢æœ€ä½³åŒ–, è³‡æ–™åº«å¿«å–Buffer Pool)",
    "å¤§å‹è³‡æ–™åº«æ¶æ§‹ (åˆ†å‰²Partitioning, åˆ†ç‰‡Sharding, åˆ†æ•£å¼è³‡æ–™åº«)",

    # ç¾ä»£è³‡æ–™åº«èˆ‡é›²ç«¯
    "åˆ†æ•£å¼ç³»çµ±ç†è«– (CAPç†è«–-Consistency/Availability/Partition Tolerance)",
    "é›²ç«¯è³‡æ–™åº«æœå‹™ (AWS RDS, Google Cloud Spanner)",
    "NoSQLè³‡æ–™åº«å¯¦ä½œ (MongoDB, Cassandra, Redis)",

    # è³‡æ–™åº«å®‰å…¨èˆ‡ç®¡ç†
    "è³‡æ–™åº«å®‰å…¨ç®¡ç† (ä½¿ç”¨è€…æˆæ¬Šèˆ‡è§’è‰², SQL Injectioné˜²ç¯„)",
    "å‚™æ´èˆ‡é«˜å¯ç”¨æ€§ (å‚™ä»½èˆ‡é‚„åŸBackup & Recovery, é«˜å¯ç”¨æ€§HA, ç½é›£å¾©åŸDR)",

    # è³‡æ–™æ‡‰ç”¨èˆ‡åˆ†æ
    "é€²éšè³‡æ–™è™•ç† (è³‡æ–™æ¢å‹˜Data Mining, å¤§æ•¸æ“š-Big Data/Hadoop/Spark)",
    "å•†æ¥­æ™ºæ…§ç³»çµ± (è³‡æ–™å€‰å„²Data Warehouse, ETLæµç¨‹, å•†æ¥­æ™ºæ…§BI)",

    # äººå·¥æ™ºæ…§åŸºç¤
    "äººå·¥æ™ºæ…§æ¦‚è«– (å®šç¾©èˆ‡æ­·å², å¼±AI vs å¼·AI)",
    "æ©Ÿå™¨å­¸ç¿’ç¯„ç–‡ (æ©Ÿå™¨å­¸ç¿’/æ·±åº¦å­¸ç¿’èˆ‡AIé—œä¿‚, ç›£ç£å¼/éç›£ç£å¼/å¼·åŒ–å­¸ç¿’)",
    
    # æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•
    "åŸºç¤æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³• (ç·šæ€§å›æ­¸/é‚è¼¯å›æ­¸, æ±ºç­–æ¨¹/éš¨æ©Ÿæ£®æ—, SVM, KNN)",
    "æ·±åº¦å­¸ç¿’æ¶æ§‹ (ç¥ç¶“ç¶²è·¯åŸºç¤-Perceptron/MLP, CNN, RNN/LSTM/GRU, Transformer/Attention)",
    "éç›£ç£å­¸ç¿’æŠ€è¡“ (èšé¡æ¼”ç®—æ³•-K-Means/åˆ†å±¤èšé¡)",

    # æ©Ÿå™¨å­¸ç¿’å¯¦å‹™
    "è³‡æ–™é è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹ (è³‡æ–™å‰è™•ç†-Normalization/Standardization, ç‰¹å¾µé¸æ“‡/é™ç¶­-PCA/LDA, ç‰¹å¾µæŠ½å–)",
    "æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼° (è¨“ç·´é›†/é©—è­‰é›†/æ¸¬è©¦é›†åŠƒåˆ†, è©•ä¼°æŒ‡æ¨™-Accuracy/Precision/Recall/F1, äº¤å‰é©—è­‰, éæ“¬åˆ/æ¬ æ“¬åˆ)",

    # AIæ‡‰ç”¨èˆ‡å€«ç†
    "AIæ‡‰ç”¨é ˜åŸŸ (è‡ªç„¶èªè¨€è™•ç†NLP, é›»è…¦è¦–è¦ºCV, èªéŸ³è¾¨è­˜, ç”Ÿæˆå¼AI-GAN/Diffusion)",
    "AIå€«ç†èˆ‡ç¤¾æœƒå½±éŸ¿ (AIå€«ç†èˆ‡åå·®, AIå®‰å…¨èˆ‡é¢¨éšª, AIèˆ‡æœªä¾†å·¥ä½œ)",

    # è³‡è¨Šå®‰å…¨åŸºç¤
    "è³‡å®‰åŸºç¤æ¦‚å¿µ (CIAä¸‰è¦ç´ -Confidentiality/Integrity/Availability, å¸¸è¦‹å¨è„…-æƒ¡æ„ç¨‹å¼/ç¤¾äº¤å·¥ç¨‹/ç¶²è·¯æ”»æ“Š)",
    
    # åŠ å¯†èˆ‡èªè­‰
    "åŠ å¯†æŠ€è¡“ç¶œåˆ (å°ç¨±å¼åŠ å¯†-AES/DES, éå°ç¨±å¼åŠ å¯†-RSA/ECC, é›œæ¹Šå‡½æ•¸-MD5/SHA-2/SHA-3)",
    "æ•¸ä½ç°½ç« èˆ‡æ†‘è­‰é«”ç³»",
    "èº«åˆ†èªè­‰èˆ‡å­˜å–æ§åˆ¶ (ä½¿ç”¨è€…èªè­‰, å­˜å–æ§åˆ¶æ©Ÿåˆ¶)",

    # ç¶²è·¯èˆ‡ç³»çµ±å®‰å…¨
    "ç¶²è·¯å®‰å…¨é˜²è­· (é˜²ç«ç‰†Firewall, IDS/IPS, VPN/å®‰å…¨é€šè¨Šå”å®š-SSL-TLS/IPSec)",
    "ç³»çµ±èˆ‡æ‡‰ç”¨å®‰å…¨ (ä½œæ¥­ç³»çµ±å®‰å…¨, Webå®‰å…¨-SQL Injection/XSS/CSRF, è¡Œå‹•æ‡‰ç”¨å®‰å…¨, é›²ç«¯å®‰å…¨)",
    
    # è³‡å®‰ç®¡ç†èˆ‡æ²»ç†
    "è³‡å®‰æ²»ç†èˆ‡é¢¨éšªç®¡ç† (é¢¨éšªè©•ä¼°, ISO 27001èˆ‡å®‰å…¨æ²»ç†, é›¶ä¿¡ä»»æ¶æ§‹Zero Trust)",
    "è³‡å®‰ç¶­é‹æŠ€è¡“ (æ¼æ´ç®¡ç†èˆ‡ä¿®è£œ, äº‹ä»¶å›æ‡‰Incident Response, å‚™æ´èˆ‡ç½é›£å¾©åŸ)",
    "æ–°èˆˆå®‰å…¨æŠ€è¡“ (å€å¡Šéˆèˆ‡å®‰å…¨, AIåœ¨è³‡å®‰ä¸­çš„æ‡‰ç”¨)",

    # é›²ç«¯è¨ˆç®—åŸºç¤
    "é›²ç«¯è¨ˆç®—æ¦‚è«– (å®šç¾©, æœå‹™æ¨¡å¼-IaaS/PaaS/SaaS, éƒ¨ç½²æ¨¡å¼-å…¬æœ‰é›²/ç§æœ‰é›²/æ··åˆé›²/å¤šé›²)",
    
    # è™›æ“¬åŒ–èˆ‡å®¹å™¨æŠ€è¡“
    "è™›æ“¬åŒ–æŠ€è¡“ (ç³»çµ±è™›æ“¬åŒ–-VMware/KVM/Hyper-V, å®¹å™¨åŒ–-Docker/LXC, å®¹å™¨ç·¨æ’-Kubernetes)",
    
    # é›²ç«¯æ¶æ§‹èˆ‡æœå‹™
    "é›²ç«¯åŸºç¤æ¶æ§‹ (è³‡æ–™ä¸­å¿ƒData Center, åˆ†æ•£å¼å„²å­˜-HDFS/Ceph, è² è¼‰å¹³è¡¡Load Balancer)",
    "ä¸»è¦é›²ç«¯å¹³å°æœå‹™ (AWS-EC2/S3/RDS/Lambda, Google Cloud Platform, Microsoft Azure)",
    "Serverlessæ¶æ§‹èˆ‡é‚Šç·£é‹ç®— (Edge Computing)",
    
    # é›²ç«¯å®‰å…¨èˆ‡ç®¡ç†
    "é›²ç«¯å®‰å…¨ç®¡ç† (èº«åˆ†èˆ‡å­˜å–ç®¡ç†-IAM, é›²ç«¯è³‡å®‰æ¨™æº–-CIS/NIST, é›²ç«¯åŠ å¯†èˆ‡é˜²è­·)",
    "é›²ç«¯æ•´åˆæ‡‰ç”¨ (é›²ç«¯èˆ‡AI/å¤§æ•¸æ“šæ•´åˆ)",

    # è³‡è¨Šç³»çµ±èˆ‡ç®¡ç†
    "è³‡è¨Šç³»çµ±åŸºç¤ (å®šç¾©, MISè§’è‰²èˆ‡åƒ¹å€¼, çµ„ç¹”èˆ‡ç³»çµ±äº’å‹•)",
    "ä¼æ¥­ç³»çµ±é¡å‹ (äº¤æ˜“è™•ç†ç³»çµ±TPS, ç®¡ç†å ±è¡¨ç³»çµ±MIS, æ±ºç­–æ”¯æ´ç³»çµ±DSS, ä¼æ¥­è³‡æºè¦åŠƒERP, ä¾›æ‡‰éˆç®¡ç†SCM, é¡§å®¢é—œä¿‚ç®¡ç†CRM)",
    "é›»å­å•†å‹™èˆ‡çŸ¥è­˜ç®¡ç† (E-Commerce, çŸ¥è­˜ç®¡ç†KM)",
    
    # ç³»çµ±é–‹ç™¼èˆ‡å°ˆæ¡ˆç®¡ç†
    "ç³»çµ±é–‹ç™¼æ–¹æ³•è«– (ç³»çµ±ç™¼å±•ç”Ÿå‘½é€±æœŸSDLC, æ•æ·å¼æ–¹æ³•Agile)",
    "ITå°ˆæ¡ˆç®¡ç†èˆ‡è©•ä¼°",
    
    # ç¾ä»£è³‡è¨Šç³»çµ±è¶¨å‹¢
    "ç¾ä»£ä¼æ¥­æ‡‰ç”¨ (å•†æ¥­æ™ºæ…§BI, è³‡æ–™å€‰å„²èˆ‡è³‡æ–™æ¢å‹˜, å³æ™‚æ±ºç­–æ”¯æŒç³»çµ±)",
    "æ•¸ä½è½‰å‹èˆ‡æ•´åˆ (æ•¸ä½è½‰å‹Digital Transformation, é›²ç«¯MIS, AIèˆ‡MISæ•´åˆ)",

    # è»Ÿé«”å·¥ç¨‹åŸºç¤
    "è»Ÿé«”å·¥ç¨‹æ¦‚è«– (å®šç¾©, åŸå‰‡, è»Ÿé«”ç”Ÿå‘½é€±æœŸ)",
    
    # é–‹ç™¼æµç¨‹èˆ‡æ–¹æ³•
    "é–‹ç™¼æ¨¡å‹ (ç€‘å¸ƒæ¨¡å‹Waterfall, æ¼”åŒ–å¼æ¨¡å‹Evolutionary, æ•æ·é–‹ç™¼-Agile/Scrum/XP, DevOps)",
    
    # éœ€æ±‚èˆ‡è¨­è¨ˆ
    "éœ€æ±‚å·¥ç¨‹ (Requirement Engineering)",
    "ç³»çµ±è¨­è¨ˆèˆ‡å»ºæ¨¡ (UMLåœ–-Use Case/Class/Sequence/Activity, ç³»çµ±è¨­è¨ˆ)",
    
    # ç‰©ä»¶å°å‘æŠ€è¡“
    "ç‰©ä»¶å°å‘æŠ€è¡“ç¶œåˆ (OOPåŸºç¤-Class/Inheritance/Polymorphism, ç‰©ä»¶å°å‘åˆ†æOOA, ç‰©ä»¶å°å‘è¨­è¨ˆOOD)",
    
    # è»Ÿé«”æ¸¬è©¦
    "è»Ÿé«”æ¸¬è©¦ç¶œåˆ (æ¸¬è©¦å±¤æ¬¡-Unit/Integration/System, æ¸¬è©¦æ–¹æ³•-Black Box/White Box, è‡ªå‹•åŒ–æ¸¬è©¦)",
    
    # è»Ÿé«”ç¶­è­·èˆ‡å“è³ª
    "è»Ÿé«”ç¶­è­·é¡å‹ (Corrective/Adaptive/Perfective/Preventive)",
    "è»Ÿé«”å“è³ªèˆ‡ç®¡ç† (è»Ÿé«”å“è³ªæ¨¡å‹-ISO 9126/25010, å“è³ªä¿è­‰æµç¨‹, Code Reviewèˆ‡Refactoring)",
    
    # å°ˆæ¡ˆç®¡ç†èˆ‡ç‰ˆæœ¬æ§åˆ¶
    "é–‹ç™¼ç¶­é‹å·¥å…· (è»Ÿé«”ç‰ˆæœ¬ç®¡ç†-Git/SVN, æŒçºŒæ•´åˆèˆ‡æŒçºŒéƒ¨ç½²CI/CD)",
    "è»Ÿé«”å°ˆæ¡ˆç®¡ç† (å°ˆæ¡ˆè¦åŠƒèˆ‡æ™‚ç¨‹ä¼°ç®—-PERT/Gantt, é¢¨éšªç®¡ç†, æˆæœ¬ä¼°ç®—-COCOMO)",

    # æ•¸ä½é‚è¼¯åŸºç¤
    "æ•¸ä½é‚è¼¯åŸºç¤ (é¡æ¯”vsæ•¸ä½è¨Šè™Ÿ, å¸ƒæ—ä»£æ•¸Boolean Algebra, é‚è¼¯é–˜-AND/OR/NOT/XOR/NAND/NOR)",
    
    # çµ„åˆé‚è¼¯é›»è·¯
    "çµ„åˆé‚è¼¯é›»è·¯è¨­è¨ˆ (æ•¸ä½é›»è·¯èˆ‡é‚è¼¯å‡½æ•¸, åŠåŠ å™¨/å…¨åŠ å™¨, ç·¨ç¢¼å™¨Encoder/è§£ç¢¼å™¨Decoder, å¤šå·¥å™¨Multiplexer/è§£å¤šå·¥å™¨Demultiplexer, æ¯”è¼ƒå™¨Comparator)",
    
    # åºå‘é‚è¼¯é›»è·¯
    "åºå‘é‚è¼¯é›»è·¯è¨­è¨ˆ (é–‚é–å™¨Latch/æ­£åå™¨Flip-Flop, è¨ˆæ•¸å™¨Counter, æš«å­˜å™¨Register)",
    
    # è¨˜æ†¶é«”èˆ‡æ•¸ä½ç³»çµ±
    "è¨˜æ†¶é«”ç³»çµ± (RAM, ROM, è¨˜æ†¶é«”ä½å€èˆ‡è³‡æ–™åŒ¯æµæ’)",
    "æ•¸ä½ç³»çµ±è¨­è¨ˆ (ç‹€æ…‹æ©ŸState Machine, CPLD, FPGA)",

    # ä½œæ¥­ç³»çµ±åŸºç¤
    "ä½œæ¥­ç³»çµ±æ¦‚è«– (å®šç¾©èˆ‡åŠŸèƒ½, æ¼”é€²-æ‰¹æ¬¡/å¤šå·¥/åˆ†æ™‚, Kernelèˆ‡ä½¿ç”¨è€…æ¨¡å¼)",
    
    # è¡Œç¨‹ç®¡ç†
    "è¡Œç¨‹èˆ‡åŸ·è¡Œç·’ç®¡ç† (Process/Thread, PCB, è¡Œç¨‹æ’ç¨‹Schedulingæ¼”ç®—æ³•, è¡Œç¨‹é–“é€šè¨ŠIPC)",
    
    # è¨˜æ†¶é«”ç®¡ç†
    "è¨˜æ†¶é«”ç®¡ç†æŠ€è¡“ (è¨˜æ†¶é«”åˆ†å€/åˆ†é Paging, åˆ†æ®µSegmentation, è™›æ“¬è¨˜æ†¶é«”Virtual Memory, ç½®æ›æ¼”ç®—æ³•Page Replacement)",
    
    # æª”æ¡ˆèˆ‡I/Oç®¡ç†
    "æª”æ¡ˆç³»çµ±ç®¡ç† (æ¶æ§‹èˆ‡åŠŸèƒ½, æª”æ¡ˆå­˜å–æ–¹æ³•, ç›®éŒ„çµæ§‹, ç£ç¢Ÿç©ºé–“ç®¡ç†)",
    "I/Oç³»çµ±ç®¡ç† (I/Oç¡¬é«”èˆ‡è»Ÿé«”, Buffering/Caching, ç£ç¢Ÿæ’ç¨‹æ¼”ç®—æ³•)",
    
    # åŒæ­¥èˆ‡æ­»çµ
    "åŒæ­¥èˆ‡æ­»çµè™•ç† (ç«¶çˆ­æ¢ä»¶Race Condition/äº’æ–¥Mutual Exclusion, è‡¨ç•Œå€Critical Section, è™ŸèªŒSemaphore/ç›£æ§å™¨Monitor, æ­»çµDeadlockè™•ç†)",

    # ç¨‹å¼è¨­è¨ˆåŸºç¤
    "ç¨‹å¼èªè¨€æ¦‚è«– (é«˜éš/ä½éšèªè¨€, Compiler/Interpreter, ç¨‹å¼ç¯„å‹-Procedural/OOP/Functional)",
    
    # ç¨‹å¼åŸºæœ¬çµæ§‹
    "åŸºæœ¬ç¨‹å¼çµæ§‹ (è³‡æ–™å‹æ…‹èˆ‡è®Šæ•¸, æµç¨‹æ§åˆ¶-if/for/while/switch, éè¿´Recursion)",
    
    # ç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆ
    "OOPç¨‹å¼è¨­è¨ˆ (Class/Object, Encapsulation, Inheritance, Polymorphism)",
    
    # è¨˜æ†¶é«”ç®¡ç†èˆ‡éŒ¯èª¤è™•ç†
    "è¨˜æ†¶é«”ç®¡ç† (Stack/Heap, Pointer, è¨˜æ†¶é«”é…ç½®/é‡‹æ”¾, Garbage Collection)",
    "éŒ¯èª¤è™•ç†æ©Ÿåˆ¶ (Exceptionè™•ç†, try-catch-finally)",
    
    # é–‹ç™¼å·¥å…·èˆ‡å¯¦è¸
    "é–‹ç™¼å¯¦è¸ (å‡½å¼åº«èˆ‡æ¡†æ¶Library/Framework, ç¨‹å¼ç¢¼é¢¨æ ¼, å–®å…ƒæ¸¬è©¦Unit Test/é™¤éŒ¯Debugging)",

    # è³‡æ–™çµæ§‹èˆ‡æ¼”ç®—æ³•åŸºç¤
    "è³‡æ–™çµæ§‹èˆ‡æ¼”ç®—æ³•åŸºç¤ (å®šç¾©èˆ‡é‡è¦æ€§, æ•ˆèƒ½åˆ†æ-æ™‚é–“/ç©ºé–“è¤‡é›œåº¦, Big O Notation)",
    
    # åŸºæœ¬è³‡æ–™çµæ§‹
    "ç·šæ€§è³‡æ–™çµæ§‹ (Array, Linked List, Stack, Queue)",
    
    # æ¨¹ç‹€çµæ§‹
    "æ¨¹çµæ§‹èˆ‡æ¼”ç®—æ³• (Treeéæ­·Traversal, Binary Tree/BST, å¹³è¡¡æ¨¹-AVL/Red-Black Tree, Heap/å„ªå…ˆä½‡åˆ—)",
    
    # åœ–å½¢çµæ§‹
    "åœ–çµæ§‹èˆ‡æ¼”ç®—æ³• (Graphè¡¨ç¤ºæ³•-é„°æ¥çŸ©é™£/é„°æ¥ä¸²åˆ—, BFS/DFS, MST-Prim's/Kruskal's, æœ€çŸ­è·¯å¾‘-Dijkstra's/Bellman-Ford)",
    
    # é›œæ¹Šèˆ‡æ’åº
    "é›œæ¹ŠæŠ€è¡“ (Hash Function, ç¢°æ’è§£æ±º, Hash Table)",
    "æ’åºèˆ‡æœå°‹æ¼”ç®—æ³• (æ’åº-å†’æ³¡/é¸æ“‡/æ’å…¥/å¿«é€Ÿ/åˆä½µ/å †ç©, æœå°‹-ç·šæ€§/äºŒå…ƒ)",

    # é›»è…¦ç¶²è·¯åŸºç¤
    "ç¶²è·¯åŸºç¤æ¦‚è«– (å®šç¾©èˆ‡åŠŸèƒ½, ç¶²è·¯åˆ†é¡-LAN/MAN/WAN, æ‹“æ¨¸-Star/Bus/Ring/Mesh, æ•ˆèƒ½æŒ‡æ¨™-é »å¯¬/å»¶é²/ååé‡)",
    
    # ç¶²è·¯æ¨¡å‹èˆ‡å”å®š
    "ç¶²è·¯åˆ†å±¤æ¨¡å‹ (OSIä¸ƒå±¤, TCP/IPæ¨¡å‹)",
    
    # å¯¦é«”èˆ‡è³‡æ–™é€£çµå±¤
    "å¯¦é«”å±¤æŠ€è¡“ (è¨Šè™Ÿç·¨ç¢¼, å‚³è¼¸åª’ä»‹-æœ‰ç·š/ç„¡ç·š)",
    "è³‡æ–™é€£çµå±¤æŠ€è¡“ (å°åŒ…æ¡†æ¶, MACä½å€, éŒ¯èª¤åµæ¸¬-CRC, ä¹™å¤ªç¶²è·¯Ethernet, ARP)",
    
    # ç¶²è·¯å±¤èˆ‡å‚³è¼¸å±¤
    "ç¶²è·¯å±¤å”å®šèˆ‡æŠ€è¡“ (IPä½å€-IPv4/IPv6, å­ç¶²è·¯é®ç½©/CIDR, è·¯ç”±å”å®š-RIP/OSPF/BGP, NAT, ICMP/Ping)",
    "å‚³è¼¸å±¤å”å®š (TCP/UDP, ä¸‰å‘äº¤æ¡/å››æ¬¡æ®æ‰‹, æµé‡æ§åˆ¶/å£…å¡æ§åˆ¶)",
    
    # æ‡‰ç”¨å±¤èˆ‡ç¶²è·¯ç¨‹å¼è¨­è¨ˆ
    "æ‡‰ç”¨å±¤å”å®š (DNS, HTTP/HTTPS, FTP, SMTP, POP3/IMAP)",
    "ç¶²è·¯ç¨‹å¼è¨­è¨ˆ (Socketç¨‹å¼è¨­è¨ˆ)",
    
    # ç¶²è·¯å®‰å…¨èˆ‡æ–°èˆˆæŠ€è¡“
    "ç¶²è·¯å®‰å…¨æŠ€è¡“ (é˜²ç«ç‰†, VPN, IDS/IPS, åŠ å¯†å”å®š-SSL-TLS/IPSec)",
    "ç„¡ç·šèˆ‡è¡Œå‹•ç¶²è·¯ (ç„¡ç·šæ¨™æº–-Wi-Fi/Bluetooth/5G, è¡Œå‹•ç¶²è·¯æ¶æ§‹-Cellular/Handoff)",
    "æ–°èˆˆç¶²è·¯æŠ€è¡“ (IoTç‰©è¯ç¶²ç¶²è·¯, SDNè»Ÿé«”å®šç¾©ç¶²è·¯, é›²ç«¯ç¶²è·¯/é‚Šç·£é‹ç®—)"
]
def classify_question(q, max_retries=2):
    global api_error_count
    
    for attempt in range(max_retries):
        try:
            if api_error_count > max_api_errors:
                return create_random_classification(q)
                
            set_api_key()
            q_text = q.get("question_text", "")
            q_keys = q.get("key-points", [])
            q_options = q.get("options", [])

            prompt = f"""è¯·åˆ†æä»¥ä¸‹é¢˜ç›®å¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š
é¢˜ç›®: {q_text}
é€‰é¡¹: {q_options}
çŸ¥è¯†ç‚¹: {q_keys}

è¯·é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»ï¼ˆå¿…é¡»ä»ç»™å®šåˆ—è¡¨ä¸­é€‰æ‹©ï¼‰ï¼š
1. å¤§çŸ¥è¯†ç‚¹: {domains_list}
2. åŒºå—: {blocks_list}
3. å°çŸ¥è¯†ç‚¹: {micro_concepts_list}

è¿”å›æ ¼å¼: {{"domain": "åç§°", "block": "åç§°", "micro_concepts": ["åç§°1", "åç§°2"]}}
"""

            model = init_gemini("gemini-1.5-flash")
            response = model.generate_content(prompt)
            parsed = safe_json_parse(response.text)
            
            if not parsed:
                api_error_count += 1
                if api_error_count % 100 == 0:
                    print(f"âš ï¸ APIé”™è¯¯è®¡æ•°: {api_error_count}")
                raise ValueError("JSONè§£æå¤±è´¥")
            
            domain = match_from_list(parsed.get("domain",""), domains_list)
            block = match_from_list(parsed.get("block",""), blocks_list)
            mc_list = [match_from_list(mc, micro_concepts_list) for mc in parsed.get("micro_concepts", [])[:2]]
            
            if not mc_list:
                mc_list = random.sample(micro_concepts_list, k=min(1, len(micro_concepts_list)))

            return {
                "domain": {"name": domain},
                "block": {"title": block, "domain_name": domain},
                "micro_concepts": [{"name": mc, "block_title": block} for mc in mc_list],
                "question": {
                    "text": q_text, 
                    "options": q_options, 
                    "micro_concepts": mc_list
                }
            }
            
        except Exception as e:
            if attempt == max_retries - 1:
                return create_random_classification(q)
            time.sleep(1)

def create_random_classification(q):
    q_text = q.get("question_text", "")
    q_options = q.get("options", [])
    
    domain = random.choice(domains_list)
    block = random.choice(blocks_list)
    mc_list = random.sample(micro_concepts_list, k=min(1, len(micro_concepts_list)))
    
    return {
        "domain": {"name": domain},
        "block": {"title": block, "domain_name": domain},
        "micro_concepts": [{"name": mc, "block_title": block} for mc in mc_list],
        "question": {
            "text": q_text, 
            "options": q_options, 
            "micro_concepts": mc_list
        }
    }

def process_in_batches(questions, batch_size=4000):
    """åˆ†æ‰¹å¤„ç†é—®é¢˜ï¼Œæ¯æ‰¹4000ä¸ª"""
    results = []
    total_batches = (len(questions) + batch_size - 1) // batch_size
    
    for batch_num in range(total_batches):
        start_idx = batch_num * batch_size
        end_idx = min((batch_num + 1) * batch_size, len(questions))
        batch_questions = questions[start_idx:end_idx]
        
        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches} ({len(batch_questions)} ä¸ªé—®é¢˜)")
        
        batch_results = []
        max_workers = min(4, len(API_KEYS))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(classify_question, q) for q in batch_questions]
            for future in tqdm(as_completed(futures), total=len(futures), desc=f"æ‰¹æ¬¡ {batch_num + 1}"):
                batch_results.append(future.result())
        
        results.extend(batch_results)
        
        # æ¯å¤„ç†å®Œä¸€ä¸ªæ‰¹æ¬¡å°±ä¿å­˜ä¸€æ¬¡è¿›åº¦
        save_partial_results(results, batch_num + 1)
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
        if batch_num < total_batches - 1:
            print("â³ ç­‰å¾…3ç§’ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡...")
            time.sleep(3)
    
    return results

def save_partial_results(results, batch_num):
    """ä¿å­˜éƒ¨åˆ†ç»“æœ"""
    try:
        # æ„å»ºçŸ¥è¯†å›¾è°±ç»“æ„
        domains_dict, blocks_dict, micros_dict = {}, {}, {}
        questions_out = []

        for r in results:
            # å¤„ç†é¢†åŸŸ
            domain_name = r["domain"]["name"]
            if domain_name not in domains_dict:
                domains_dict[domain_name] = {
                    "name": domain_name,
                    "blocks": set()
                }
            
            # å¤„ç†åŒºå—
            block_title = r["block"]["title"]
            if block_title not in blocks_dict:
                blocks_dict[block_title] = {
                    "title": block_title,
                    "domain_name": domain_name,
                    "subtopics": set()
                }
            domains_dict[domain_name]["blocks"].add(block_title)
            
            # å¤„ç†å°çŸ¥è¯†ç‚¹
            for mc in r["micro_concepts"]:
                mc_name = mc["name"]
                if mc_name not in micros_dict:
                    micros_dict[mc_name] = {
                        "name": mc_name,
                        "block_title": block_title,
                        "dependencies": []
                    }
                blocks_dict[block_title]["subtopics"].add(mc_name)
            
            # å¤„ç†é—®é¢˜
            q = r["question"]
            questions_out.append({
                "text": q["text"],
                "options": q["options"],
                "micro_concepts": q["micro_concepts"]
            })

        # è½¬æ¢ä¸ºæœ€ç»ˆè¾“å‡ºæ ¼å¼
        domains = []
        for domain_name, domain_data in domains_dict.items():
            domains.append({
                "name": domain_name,
                "blocks": list(domain_data["blocks"])
            })
        
        blocks = []
        for block_title, block_data in blocks_dict.items():
            blocks.append({
                "title": block_title,
                "domain_name": block_data["domain_name"],
                "subtopics": list(block_data["subtopics"])
            })
        
        micro_concepts = []
        for mc_name, mc_data in micros_dict.items():
            micro_concepts.append({
                "name": mc_name,
                "block_title": mc_data["block_title"],
                "dependencies": mc_data["dependencies"]
            })

        # ä¿å­˜ä¸ºå››ä¸ªç‹¬ç«‹çš„JSONæ–‡ä»¶
        with open(f"domains_batch_{batch_num}.json", "w", encoding="utf-8") as f:
            json.dump(domains, f, ensure_ascii=False, indent=2)
        
        with open(f"blocks_batch_{batch_num}.json", "w", encoding="utf-8") as f:
            json.dump(blocks, f, ensure_ascii=False, indent=2)
        
        with open(f"micro_concepts_batch_{batch_num}.json", "w", encoding="utf-8") as f:
            json.dump(micro_concepts, f, ensure_ascii=False, indent=2)
        
        with open(f"questions_batch_{batch_num}.json", "w", encoding="utf-8") as f:
            json.dump(questions_out, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²ä¿å­˜æ‰¹æ¬¡ {batch_num} çš„è¿›åº¦")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜è¿›åº¦æ—¶å‡ºé”™: {e}")

def merge_results(total_batches):
    """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ"""
    all_domains, all_blocks, all_micros, all_questions = [], [], [], []
    
    for batch_num in range(1, total_batches + 1):
        try:
            with open(f"domains_batch_{batch_num}.json", "r", encoding="utf-8") as f:
                all_domains.extend(json.load(f))
            with open(f"blocks_batch_{batch_num}.json", "r", encoding="utf-8") as f:
                all_blocks.extend(json.load(f))
            with open(f"micro_concepts_batch_{batch_num}.json", "r", encoding="utf-8") as f:
                all_micros.extend(json.load(f))
            with open(f"questions_batch_{batch_num}.json", "r", encoding="utf-8") as f:
                all_questions.extend(json.load(f))
        except:
            continue
    
    # å»é‡
    domains_dict, blocks_dict, micros_dict = {}, {}, {}
    
    for domain in all_domains:
        if domain["name"] not in domains_dict:
            domains_dict[domain["name"]] = domain
        else:
            # åˆå¹¶blocks
            domains_dict[domain["name"]]["blocks"] = list(set(domains_dict[domain["name"]]["blocks"] + domain["blocks"]))
    
    for block in all_blocks:
        key = f"{block['title']}_{block['domain_name']}"
        if key not in blocks_dict:
            blocks_dict[key] = block
        else:
            blocks_dict[key]["subtopics"] = list(set(blocks_dict[key]["subtopics"] + block["subtopics"]))
    
    for micro in all_micros:
        key = f"{micro['name']}_{micro['block_title']}"
        if key not in micros_dict:
            micros_dict[key] = micro
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open("domains.json", "w", encoding="utf-8") as f:
        json.dump(list(domains_dict.values()), f, ensure_ascii=False, indent=2)
    
    with open("blocks.json", "w", encoding="utf-8") as f:
        json.dump(list(blocks_dict.values()), f, ensure_ascii=False, indent=2)
    
    with open("micro_concepts.json", "w", encoding="utf-8") as f:
        json.dump(list(micros_dict.values()), f, ensure_ascii=False, indent=2)
    
    with open("questions.json", "w", encoding="utf-8") as f:
        json.dump(all_questions, f, ensure_ascii=False, indent=2)
    
    return len(domains_dict), len(blocks_dict), len(micros_dict), len(all_questions)

def main():
    try:
        with open("fainaldata_no_del.json", "r", encoding="utf-8") as f:
            questions = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        return

    print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(questions)} ä¸ªé—®é¢˜")
    
    # è®¡ç®—æ‰¹æ¬¡æ•°é‡
    batch_size = 4000
    total_batches = (len(questions) + batch_size - 1) // batch_size
    print(f"ğŸ“¦ åˆ†æˆ {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} ä¸ªé—®é¢˜")
    
    # åˆ†æ‰¹å¤„ç†æ‰€æœ‰é—®é¢˜
    results = process_in_batches(questions, batch_size=batch_size)
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    domain_count, block_count, micro_count, question_count = merge_results(total_batches)
    
    print("âœ… å·²å®Œæˆå››ä¸ªç‹¬ç«‹çš„JSONæ–‡ä»¶:")
    print("   - domains.json")
    print("   - blocks.json") 
    print("   - micro_concepts.json")
    print("   - questions.json")
    
    print(f"ğŸ“Š ç»Ÿè®¡: {domain_count} é¢†åŸŸ, {block_count} åŒºå—, {micro_count} çŸ¥è¯†ç‚¹, {question_count} é—®é¢˜")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for batch_num in range(1, total_batches + 1):
        for file_type in ["domains", "blocks", "micro_concepts", "questions"]:
            try:
                os.remove(f"{file_type}_batch_{batch_num}.json")
            except:
                pass

if __name__ == "__main__":
    main()
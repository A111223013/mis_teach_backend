# 4-1 模型選擇與設計評估管道

### 1. 核心概念與定義

在機器學習專案中，從眾多可能的模型中挑選出最適合解決特定問題的模型，並確保其性能在真實世界中具有可靠性，是成功的關鍵。這包含了「模型選擇」與「設計評估管道」兩大核心環節。

#### 1.1 模型選擇 (Model Selection)

**定義/核心概念：** 模型選擇是指從一組候選模型中，依據特定評估標準和數據表現，挑選出最佳模型的過程。這個過程旨在找到一個在未見過數據上具有良好泛化能力的模型，避免過度擬合或欠擬合。

**例子：** 想像我們要預測房價。候選模型可能包括：
1.  線性迴歸 (Linear Regression)
2.  決策樹 (Decision Tree)
3.  隨機森林 (Random Forest)
4.  梯度提升機 (Gradient Boosting Machine)
模型選擇的目標就是根據它們在未見過房價數據上的預測表現（例如：均方誤差），選出最準確的模型。

**與相鄰概念的關聯：**
*   **超參數調優 (Hyperparameter Tuning)：** 模型選擇往往與超參數調優緊密相連。超參數是模型訓練前設定的參數（例如：決策樹的深度、隨機森林的樹木數量），它們對模型性能有巨大影響。模型選擇的過程通常會涉及對不同模型及其最佳超參數組合的選擇。
*   **模型評估指標 (Model Evaluation Metrics)：** 選擇模型時，我們需要依賴各種評估指標來量化模型的性能，例如分類問題的準確率 (Accuracy)、精確度 (Precision)、召回率 (Recall)、F1-Score，或迴歸問題的均方誤差 (Mean Squared Error, MSE)、決定係數 ($R^2$) 等。

#### 1.2 設計評估管道 (Design Evaluation Pipeline)

**定義/核心概念：** 設計評估管道是指一個結構化的流程，用於系統性地訓練、驗證、測試和選擇機器學習模型。這個管道確保了模型性能評估的公正性與可靠性，特別是通過將數據劃分為不同的子集來模擬模型在真實世界中的表現。

**核心組成部分：**
*   **訓練集 (Training Set)：** 用於訓練模型，讓模型學習數據中的模式和關係。
*   **驗證集 (Validation Set)：** 用於模型選擇和超參數調優。模型在訓練集上訓練，然後在驗證集上評估性能。根據驗證集的表現來調整超參數或比較不同模型。
*   **測試集 (Test Set)：** 用於對最終選定的模型進行一次性、公正的性能評估。它必須是模型在訓練和驗證階段從未見過的數據，以提供對模型泛化能力最真實的估計。

**推導：** 這個三明治結構的數據劃分是為了解決「數據洩漏 (Data Leakage)」和「過度擬合 (Overfitting)」的問題。如果我們用訓練過的數據評估模型，會得到過於樂觀的結果；如果用測試集進行模型選擇和超參數調優，測試集就會失去其公正性，最終的模型性能估計將不再可靠。

**與相鄰概念的關聯：**
*   **機器學習生命週期 (Machine Learning Lifecycle)：** 設計評估管道是機器學習生命週期中的一個關鍵階段，承接了數據準備和特徵工程，並為模型部署提供了基礎。
*   **泛化能力 (Generalization Ability)：** 評估管道的核心目的就是確保我們選出的模型具有良好的泛化能力，即在未見過的數據上也能保持穩健的性能。

-----

### 2. 典型例子與轉換/推導

#### 2.1 數據集劃分策略

在設計評估管道中，數據集劃分是最基礎且關鍵的一步。

**核心觀念：** 通常，數據集會被劃分為訓練集、驗證集和測試集。劃分比例通常為 60%/20%/20% 或 70%/15%/15%，但具體比例應根據數據量和問題特性進行調整。

**例子：**
假設我們有一個包含 1000 筆客戶數據的數據集，目標是預測客戶是否會購買某產品。
*   **步驟 1: 劃分訓練集與保留集**
    首先，將數據集隨機劃分為訓練集和一個保留集（通常是 80% 訓練集，20% 保留集）。
    *   訓練集 (800 筆)：用於模型訓練。
    *   保留集 (200 筆)：此部分數據會再進一步劃分。
*   **步驟 2: 劃分保留集為驗證集與測試集**
    將保留集（200 筆）再次隨機劃分為驗證集和測試集（各 50%）。
    *   驗證集 (100 筆)：用於超參數調優和模型選擇。
    *   測試集 (100 筆)：用於最終模型性能的公正評估。

**轉換/推導：** 這種劃分方式確保了：
1.  模型在訓練集上學習。
2.  模型在驗證集上"調整"和"選擇"。
3.  模型在測試集上進行"最終審查"，此時測試集完全獨立，其表現能真實反映模型在實際應用中的性能。

#### 2.2 交叉驗證 (Cross-Validation)

當數據量有限，或希望對模型性能有更穩健的估計時，交叉驗證是一種強大的技術，它主要用於替代或補充單一驗證集的角色。

**定義/核心概念：** 交叉驗證是一種統計學方法，用於估計機器學習模型的泛化能力。它通過將數據集多次劃分為訓練集和驗證集，並對每個劃分訓練和評估模型，然後將結果平均以獲得更穩健的性能估計。

**典型例子：K 折交叉驗證 (K-Fold Cross-Validation)**

**步驟：**
1.  將整個訓練數據集（注意：這裡通常指的是訓練集和驗證集合併的部分，或在沒有明確驗證集劃分時，將所有可用數據用於 K 折交叉驗證，最後保留一個獨立測試集）均勻地分成 $K$ 個子集（折，fold）。
2.  重複 $K$ 次：
    *   選取其中一個子集作為**驗證集** (validation fold)。
    *   將其餘 $K-1$ 個子集合併作為**訓練集** (training folds)。
    *   在訓練集上訓練模型。
    *   在驗證集上評估模型性能。
3.  將 $K$ 次迭代的性能評估結果（例如：準確率、MSE）取平均，得到模型性能的最終估計。

**圖示推導 (概念性)：**
假設 $K=5$。
*   **Fold 1:** |驗證|訓練|訓練|訓練|訓練|
*   **Fold 2:** |訓練|驗證|訓練|訓練|訓練|
*   **Fold 3:** |訓練|訓練|驗證|訓練|訓練|
*   **Fold 4:** |訓練|訓練|訓練|驗證|訓練|
*   **Fold 5:** |訓練|訓練|訓練|訓練|驗證|

每次迭代都會得到一個性能分數，最後取這五個分數的平均值。

**與相鄰概念的關聯：**
*   **超參數調優 (Hyperparameter Tuning)：** K 折交叉驗證常與網格搜尋 (Grid Search) 或隨機搜尋 (Random Search) 等方法結合，用於在超參數空間中尋找最佳組合。例如，對於每個候選超參數組合，都執行一次 K 折交叉驗證來評估其平均性能，然後選擇平均性能最好的組合。
*   **偏差-變異數權衡 (Bias-Variance Trade-off)：** 交叉驗證可以提供一個更穩健的性能估計，有助於我們更好地理解模型的偏差和變異數。一個在不同折上性能表現穩定的模型，通常具有較低的變異數。

-----

### 3. 與相鄰概念的關聯

#### 3.1 機器學習生命週期

**核心觀念：** 模型選擇與設計評估管道是機器學習生命週期中的關鍵階段，它銜接了數據準備、特徵工程，並為模型部署提供了可靠的性能依據。

**關聯性：**
*   **數據獲取與清理 (Data Acquisition & Cleaning)：** 這是整個流程的起點。如果數據質量不佳，後續的模型選擇和評估都將失去意義。
*   **特徵工程 (Feature Engineering)：** 創建好的特徵能顯著提升模型潛力。模型選擇是在既有特徵集下尋找最佳模型，而特徵工程則可能直接改變候選模型的表現排序。優良的特徵有助於模型更容易學習模式，減少過擬合或欠擬合的風險，從而簡化模型選擇的難度。
*   **模型部署與監控 (Model Deployment & Monitoring)：** 通過設計評估管道選出的模型，其在測試集上的性能將作為部署後預期表現的基準。部署後的模型也需要持續監控其性能，並與評估管道中獲得的基準進行比較，以發現模型漂移 (model drift) 等問題。

#### 3.2 超參數調優 (Hyperparameter Tuning)

**核心觀念：** 超參數調優是尋找模型最佳超參數組合的過程，而模型選擇則是在調優後的候選模型中做出最終決定。兩者共同構成尋找最佳模型的迭代過程。

**關聯性：**
*   **迭代過程：** 模型選擇和超參數調優是迭代進行的。對於每一個候選模型類型（例如：支持向量機、決策樹），我們都會在其特定的超參數空間中進行調優（通常使用驗證集或交叉驗證），找出該模型類型的最佳版本。然後，我們再在這些最佳版本的模型之間進行最終的模型選擇。
*   **網格搜尋 (Grid Search) 與隨機搜尋 (Random Search)：** 這些是常見的超參數調優策略，它們本質上是在超參數空間中嘗試不同的組合，並通過設計評估管道中的驗證集或交叉驗證來評估每個組合的性能。

#### 3.3 偏差-變異數權衡 (Bias-Variance Trade-off)

**核心觀念：** 模型的總誤差可以分解為偏差（模型對數據的擬合不足）和變異數（模型對訓練數據的敏感性，導致對新數據的表現不穩定）以及不可約誤差。模型選擇的目標是找到一個在這兩者之間取得良好平衡的模型。

**關聯性：**
*   **欠擬合 (Underfitting) 與高偏差：** 欠擬合的模型通常過於簡單，無法捕捉數據中的複雜模式，導致訓練集和驗證集上的表現都很差。這反映了高偏差。
*   **過擬合 (Overfitting) 與高變異數：** 過擬合的模型過於複雜，記住了訓練數據中的噪聲和特有模式，導致在訓練集上表現極佳，但在驗證集或測試集上表現大幅下降。這反映了高變異數。
*   **模型選擇的作用：** 通過比較不同模型的驗證集性能，我們可以判斷模型是否存在欠擬合或過擬合的傾向，並據此調整模型複雜度（例如：從線性模型轉向非線性模型以降低偏差，或為複雜模型增加正規化以降低變異數），以找到在偏差和變異數之間達到最佳平衡點的模型。

-----

### 4. 進階內容

#### 4.1 巢式交叉驗證 (Nested Cross-Validation)

**核心概念：** 當同時進行超參數調優和模型選擇時，巢式交叉驗證提供了一種更為嚴謹的評估方法，以避免驗證集上的「過擬合」(validation set overfitting) 或對模型性能的過度樂觀估計。

**結構：** 巢式交叉驗證包含兩層迴圈：
*   **外層迴圈 (Outer Loop)：** 用於評估模型的泛化性能。它將數據集劃分為 $K_1$ 個折，每次迭代將一個折作為**外部測試集 (Outer Test Set)**，其餘 $K_1-1$ 個折作為**外部訓練集 (Outer Training Set)**。
*   **內層迴圈 (Inner Loop)：** 用於模型選擇和超參數調優。對於外層迴圈的每個外部訓練集，內層迴圈會再次將其劃分為 $K_2$ 個折，進行 K 折交叉驗證，以找到最佳超參數組合。
    *   **內部訓練集 (Inner Training Set)：** 用於訓練模型。
    *   **內部驗證集 (Inner Validation Set)：** 用於評估內部訓練的模型和調整超參數。
*   **最終評估：** 對於外層迴圈的每次迭代，最佳超參數的模型將在相應的外部測試集上進行評估。將所有外部測試集上的性能結果平均，即可得到對模型泛化能力的無偏估計。

**推導：** 巢式交叉驗證的意義在於，外部測試集在整個超參數調優和模型選擇過程中完全沒有被「看見」過。這確保了最終性能估計的公正性，避免了因反覆使用驗證集而可能導致的過度樂觀。

**與相鄰概念的關聯：**
*   **避免數據洩漏：** 它通過嚴格的分離測試數據，防止了數據洩漏。
*   **穩健的性能估計：** 提供了對模型泛化能力更可靠的估計，尤其是在比較多個模型和它們的超參數時。

#### 4.2 模型解釋性與可解釋性 (Model Interpretability & Explainability)

**核心概念：** 隨著模型複雜度的增加，理解模型如何做出預測變得越來越困難。模型解釋性是指模型本身的可理解程度（例如線性模型），而模型可解釋性則是指我們能夠多大程度上解釋複雜模型的決策過程（例如使用 LIME, SHAP 等工具）。

**與模型選擇的關聯：**
*   **不僅限於性能：** 在模型選擇中，我們不應只關注性能指標。在某些應用場景（如醫療、金融），模型的透明度和決策依據至關重要。即使一個簡單模型的性能稍遜，但如果其解釋性高，可能在實踐中更有價值。
*   **權衡取捨：** 模型選擇可能涉及在高性能複雜模型（如深度學習）和低性能高解釋性模型（如決策樹、線性迴歸）之間做出權衡。
*   **後驗解釋工具：** 對於選定的複雜模型，可以使用後驗解釋工具（Post-hoc Explainability Tools）來理解其預測，這也是評估管道的一部分，尤其是在模型部署後。

-----

### 5. 常見錯誤與澄清

#### 5.1 使用測試集進行模型選擇或超參數調優

**常見錯誤：** 許多初學者會將訓練集和測試集劃分後，直接在測試集上評估不同模型的性能，或根據測試集性能調整超參數，然後宣稱測試集上表現最佳的模型就是最終模型。

**澄清：** 測試集是為了對最終選定的模型進行一次性、公正的性能評估而保留的。如果測試集被用於模型選擇或超參數調優，它就失去了其“未見過數據”的獨立性，模型將會對測試集“過度擬合”，導致對真實世界性能的估計過於樂觀。這會造成模型部署後，實際表現遠不如預期。正確的做法是使用**驗證集**或**交叉驗證**來進行模型選擇和超參數調優。

#### 5.2 數據洩漏 (Data Leakage)

**常見錯誤：** 在數據預處理階段，不小心將來自測試集的信息洩漏到訓練集或驗證集中。例如，在劃分數據集之前，對整個數據集進行特徵縮放 (feature scaling) 或填補缺失值。

**澄清：** 數據預處理步驟（如標準化、歸一化、特徵選擇）應該在數據劃分之後，且僅基於**訓練集**的信息來執行。例如，計算標準化的均值和標準差時，只能使用訓練集的數據，然後將這些參數應用於驗證集和測試集。如果使用整個數據集的統計量，測試集的信息就會洩漏到訓練過程中，導致模型評估結果失真。

#### 5.3 只看單一評估指標

**常見錯誤：** 尤其在分類問題中，只關注準確率 (Accuracy) 這一個指標，而忽略了其他可能更重要的指標。

**澄清：** 選擇評估指標必須與實際業務目標和數據特性緊密相關。
*   **不平衡數據集 (Imbalanced Dataset)：** 如果正負樣本數量極度不平衡，高準確率可能沒有意義。例如，在一個 99% 都是負樣本的數據集中，一個總是預測負樣本的模型也能達到 99% 的準確率。此時，精確度 (Precision)、召回率 (Recall)、F1-Score 或 ROC-AUC 會是更合適的指標。
*   **業務需求：** 在某些場景下，召回率（例如：疾病診斷，不希望漏掉任何陽性病例）可能比精確度更重要；而在另一些場景下，精確度（例如：垃圾郵件過濾，不希望錯殺正常郵件）可能更為關鍵。綜合考量多個指標，並理解它們的業務含義，是做出明智模型選擇的關鍵。

-----

### 6. 小練習（附詳解）

#### 小練習 1: 數據集劃分與目的

假設你正在為一家電商公司開發一個機器學習模型，用於預測新註冊用戶在註冊後七天內是否會進行首次購買。你擁有 10,000 筆歷史用戶數據，其中包含用戶特徵和是否購買的標籤。

請說明你會如何劃分這 10,000 筆數據，並詳述每個劃分數據集的目的。

**解答步驟：**

1.  **初始劃分 (訓練集與保留集)：**
    *   **步驟：** 首先，將 10,000 筆數據隨機打亂，然後按照大約 8:2 的比例劃分為訓練集和一個保留集。
        *   **訓練集：** 8,000 筆數據。
        *   **保留集：** 2,000 筆數據。
    *   **目的：** 將大部分數據用於模型的學習，同時保留一部分獨立數據用於後續的驗證和最終測試。

2.  **保留集再劃分 (驗證集與測試集)：**
    *   **步驟：** 接著，將這 2,000 筆保留集數據再次隨機打亂，並按照 1:1 的比例劃分為驗證集和測試集。
        *   **驗證集：** 1,000 筆數據。
        *   **測試集：** 1,000 筆數據。
    *   **目的：**
        *   **驗證集：** 用於模型訓練過程中的超參數調優和模型選擇。我們會在此數據集上評估不同模型的性能，並選擇最佳的超參數組合。這個數據集幫助我們在不碰觸最終測試集的情況下，迭代地改進模型。
        *   **測試集：** 用於對最終選定且超參數已調優的模型進行一次性、公正的性能評估。它必須是模型在訓練和調優過程中從未見過的數據，以提供對模型在實際應用中泛化能力最真實的估計。

3.  **各數據集總結與目的：**
    *   **訓練集 (8,000 筆)：** 用於模型的學習。模型將從中學習用戶特徵與購買行為之間的模式。
    *   **驗證集 (1,000 筆)：** 用於模型選擇和超參數調優。例如，可以比較不同演算法（邏輯迴歸、決策樹、隨機森林）在驗證集上的表現，或調整決策樹的最大深度、隨機森林的樹木數量等超參數，以找出最佳配置。
    *   **測試集 (1,000 筆)：** 用於最終模型性能的公正評估。當所有模型選擇和超參數調優都完成後，只有一次機會在測試集上評估模型性能，以確保對模型在未見過數據上的表現有可靠的估計。

#### 小練習 2: K 折交叉驗證的應用

假設你只有 100 筆數據用於一個迴歸問題，並且已經將其中 20 筆作為獨立的測試集保留。現在剩下 80 筆數據，你需要使用 K 折交叉驗證來評估一個新的線性迴歸模型。

請描述你會如何使用 4 折交叉驗證（K=4）來評估這個線性迴歸模型。

**解答步驟：**

1.  **數據準備：**
    *   你擁有 80 筆用於訓練和驗證的數據。
    *   已經將 20 筆數據作為完全獨立的測試集保留。

2.  **劃分數據折 (Folds)：**
    *   **步驟：** 將這 80 筆數據隨機打亂，然後均勻地劃分成 $K=4$ 個子集（折），每個子集包含 $80 / 4 = 20$ 筆數據。
        *   Fold 1: 20 筆數據
        *   Fold 2: 20 筆數據
        *   Fold 3: 20 筆數據
        *   Fold 4: 20 筆數據

3.  **迭代訓練與評估 (4 輪)：**

    *   **第一輪：**
        *   **驗證集：** Fold 1 (20 筆數據)
        *   **訓練集：** Fold 2 + Fold 3 + Fold 4 (共 60 筆數據)
        *   **操作：** 在訓練集上訓練線性迴歸模型，然後在驗證集 (Fold 1) 上評估模型的均方誤差 (MSE) 或其他迴歸指標，記錄為 $MSE_1$。

    *   **第二輪：**
        *   **驗證集：** Fold 2 (20 筆數據)
        *   **訓練集：** Fold 1 + Fold 3 + Fold 4 (共 60 筆數據)
        *   **操作：** 在新的訓練集上重新訓練線性迴歸模型，然後在驗證集 (Fold 2) 上評估 MSE，記錄為 $MSE_2$。

    *   **第三輪：**
        *   **驗證集：** Fold 3 (20 筆數據)
        *   **訓練集：** Fold 1 + Fold 2 + Fold 4 (共 60 筆數據)
        *   **操作：** 同上，在訓練集上訓練模型，在驗證集 (Fold 3) 上評估 MSE，記錄為 $MSE_3$。

    *   **第四輪：**
        *   **驗證集：** Fold 4 (20 筆數據)
        *   **訓練集：** Fold 1 + Fold 2 + Fold 3 (共 60 筆數據)
        *   **操作：** 同上，在訓練集上訓練模型，在驗證集 (Fold 4) 上評估 MSE，記錄為 $MSE_4$。

4.  **計算平均性能：**
    *   **步驟：** 將四輪得到的 MSE 數值取平均值。
        *   平均 MSE = $\frac{MSE_1 + MSE_2 + MSE_3 + MSE_4}{4}$
    *   **目的：** 這個平均 MSE 提供了一個對該線性迴歸模型在未見過數據上性能的更穩健、偏差更小的估計。這比單一訓練/驗證集劃分更能反映模型的泛化能力，尤其是在數據量較小的情況下。

5.  **最終模型訓練與測試 (額外步驟，但完整流程的一部分)：**
    *   **步驟：** 如果這個平均 MSE 表現令人滿意，並且決定使用線性迴歸模型，那麼最終的模型應該在全部 80 筆訓練+驗證數據上進行訓練。
    *   **目的：** 獲得一個使用所有可用數據訓練出的最佳模型。最後，這個模型將在最初保留的 20 筆**獨立測試集**上進行一次最終評估，以確認其在全新數據上的表現。

-----

### 7. 延伸閱讀/參考

*   **專書：**
    *   Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction* (2nd ed.). Springer. (經典統計學習書籍，深入探討模型選擇、偏差-變異數權衡和交叉驗證)
    *   Géron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* (2nd ed.). O'Reilly Media. (實務操作指南，包含數據劃分、交叉驗證和超參數調優的實例)
*   **線上資源：**
    *   **Scikit-learn 文檔 (Model selection and evaluation)：** [https://scikit-learn.org/stable/model_selection.html](https://scikit-learn.org/stable/model_selection.html) (提供了 Python 機器學習庫中相關功能的詳細說明和範例)
    *   **Kaggle Learn - Intro to Machine Learning (Model Validation)：** [https://www.kaggle.com/learn/intro-to-machine-learning](https://www.kaggle.com/learn/intro-to-machine-learning) (入門級教學，包含數據劃分的基礎概念)
*   **學術論文 (K 折交叉驗證)：**
    *   Kohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. *International Joint Conference on Artificial Intelligence (IJCAI)*, 1137-1143. (關於交叉驗證方法的經典研究)
### 2-2 後訓練與取樣

#### 2-2-1 核心概念：後訓練與取樣概覽

本章節將深入探討機器學習模型部署生命週期中兩個至關重要的階段：**後訓練（Post-training）**與**取樣（Sampling）**。後訓練主要關注在模型完成初始訓練後，如何進一步優化其性能、效率和資源消耗，使其更適合實際部署。而取樣則是指模型在生成任務中，如何從其輸出的機率分佈中選擇最終結果的策略，這直接影響生成內容的品質、多樣性與一致性。

*   **定義：後訓練 (Post-training)**
    指在模型的核心訓練階段（例如預訓練、監督式微調）完成之後，對模型進行的各種優化處理。這些優化通常旨在縮小模型大小、提高推理速度、降低能耗，或將模型適應到特定的硬體或運行環境，而無需重新進行全面的模型訓練。
*   **定義：取樣 (Sampling)**
    特指在生成式模型（如大型語言模型、圖像生成模型）的推斷過程中，如何根據模型輸出層（通常是機率分佈）來選取下一個元素（如單詞、像素值）。有效的取樣策略能夠平衡生成內容的相關性、流暢性、多樣性與創造性。

*   **核心觀念**
    後訓練與取樣是模型從「實驗室」走向「實際應用」的關鍵環節。後訓練確保模型在資源有限的生產環境中高效運行，而取樣則直接決定了用戶體驗到的生成內容品質。兩者共同構成一個完整的模型部署和應用策略。

-----

#### 2-2-2 後訓練技術：優化模型部署

後訓練技術旨在不顯著影響模型性能的前提下，提升其推理效率和降低資源佔用。這對於在邊緣設備、移動裝置或大規模雲端服務上部署大型模型尤為重要。

##### 2-2-2-1 模型量化 (Quantization)

*   **定義/核心觀念**
    模型量化是一種將模型參數（如權重、激活值）從高精度浮點數（例如 FP32, 32位浮點數）轉換為低精度整數（例如 INT8, 8位整數）的技術。這可以顯著減少模型大小、降低記憶體頻寬需求，並利用專門的硬體加速低精度計算，從而提升推理速度。
*   **例子或推導**
    假設一個權重矩陣 $W$ 的原始值為 32 位浮點數。量化過程通常涉及一個縮放因子 $S$ 和一個零點偏移 $Z$：
    $$
    Q = \text{round}(W / S + Z)
    $$
    其中 $Q$ 是量化後的整數值。在推理時，整數值會被反量化回近似的浮點數進行計算，或直接使用整數運算並將結果反量化。
    量化方式主要有：
    1.  **訓練後量化 (Post-Training Quantization, PTQ)**：在模型訓練完成後，直接對模型進行量化。其又可分為：
        *   靜態PTQ：需要少量校準數據來估計激活值的統計分佈，以確定最佳量化參數。
        *   動態PTQ：只量化權重，激活值在運行時根據其範圍動態量化。
    2.  **量化感知訓練 (Quantization-Aware Training, QAT)**：在模型訓練過程中就模擬量化帶來的誤差，讓模型在訓練時學習如何適應量化。通常能達到更好的精度。
*   **與相鄰概念的關聯**
    量化是模型壓縮（Model Compression）的一種主要手段，與剪枝、蒸餾等技術共同構成模型優化的完整體系。它與硬體加速器的發展（如 NVIDIA Tensor Cores, Google TPUs）緊密相關，這些硬體通常針對低精度計算有高度優化。

##### 2-2-2-2 模型剪枝 (Pruning)

*   **定義/核心觀念**
    模型剪枝旨在移除模型中冗餘或不重要的權重、連接、神經元甚至整個層，從而減少模型參數數量，降低計算複雜度。其核心假設是大型模型中存在大量不必要的冗餘。
*   **例子或推導**
    剪枝可以分為：
    1.  **非結構化剪枝 (Unstructured Pruning)**：移除單個權重，使權重矩陣變得稀疏。例如，設定一個閾值 $\epsilon$，將所有 $|w_{ij}| < \epsilon$ 的權重設置為 0。這可以顯著減少參數數量，但需要專門的硬體或軟體來有效處理稀疏矩陣。
    2.  **結構化剪枝 (Structured Pruning)**：移除整個神經元、通道或層。例如，根據通道的 L1 範數（$||W_c||_1$）排序，移除重要性最低的通道。這種方法更容易直接利用現有的密集矩陣運算庫，無需特殊硬體，可以直接減小模型體積和計算量。
*   **與相鄰概念的關聯**
    剪枝也是模型壓縮的關鍵技術。與量化相比，剪枝可以更直接地減少模型的大小和浮點運算次數（FLOPs）。它常與微調結合使用，即剪枝後對模型進行少量微調以恢復精度。

##### 2-2-2-3 知識蒸餾 (Knowledge Distillation)

*   **定義/核心觀念**
    知識蒸餾是一種模型壓縮技術，其核心思想是使用一個性能優秀但通常較大的「教師模型（Teacher Model）」的「知識」來訓練一個更小、更高效的「學生模型（Student Model）」。教師模型的知識不僅包括其最終的硬預測（Hard Labels），還包括其輸出機率分佈的「軟目標（Soft Targets）」，這包含更豐富的類別間關係信息。
*   **例子或推導**
    蒸餾的損失函數通常結合了兩部分：
    1.  學生模型對真實標籤的交叉熵損失（硬目標）。
    2.  學生模型對教師模型軟目標的交叉熵損失。
    $$
    L_{KD} = (1 - \alpha) L_{CE}(\mathbf{y}, \sigma(\mathbf{z}_S)) + \alpha L_{CE}(\sigma(\mathbf{z}_T / \tau), \sigma(\mathbf{z}_S / \tau))
    $$
    其中 $\mathbf{y}$ 是真實標籤，$\mathbf{z}_S$ 和 $\mathbf{z}_T$ 分別是學生和教師模型的 logits，$\sigma$ 是 softmax 函數，$\tau$ 是溫度（Temperature）參數，用於平滑機率分佈，$\alpha$ 是平衡權重。高溫會使機率分佈更平滑，提取更多類別間關係。
*   **與相鄰概念的關聯**
    知識蒸餾是一種有效的模型壓縮和加速方法，特別適合在需要部署輕量級模型而又不想犧牲過多性能的場景。它通常與量化、剪枝等其他壓縮技術結合使用，以達到最佳效果。

##### 2-2-2-4 微調 (Fine-tuning)

*   **定義/核心觀念**
    微調是指在一個已經在大規模數據集上預訓練好的模型基礎上，使用特定任務的數據集繼續訓練模型的過程。儘管它本質上是一個訓練步驟，但在「後訓練」的語境下，它常被視為對通用預訓練模型進行特定應用適應的關鍵「後處理」步驟。它的目標是讓預訓練模型能更好地適應特定下游任務，同時利用預訓練學到的通用知識。
*   **例子或推導**
    假設有一個在海量文本數據上預訓練好的 BERT 模型。如果我們需要將其應用於情感分析任務，我們會收集一個標註好的情感分析數據集（例如電影評論的情緒是正面還是負面）。然後，將 BERT 模型的頂部替換為一個新的分類頭部，並在情感分析數據集上繼續訓練整個模型（或僅訓練新的頭部，凍結部分底層參數）。此時，預訓練的 BERT 參數會被微小調整，以適應情感分析任務的特徵。
*   **與相鄰概念的關聯**
    微調是遷移學習（Transfer Learning）的典型應用。它與預訓練模型（例如 Transformer-based models）的發展息息相關。LoRA (Low-Rank Adaptation) 等參數高效微調 (Parameter-Efficient Fine-Tuning, PEFT) 方法也是一種微調的變體，旨在減少微調所需的計算和存儲資源。

-----

#### 2-2-3 取樣策略：控制生成內容

在生成式模型中，模型通常會輸出一個機率分佈，表示在當前上下文下，每個可能的下一個token的機率。取樣策略就是決定如何從這個分佈中選取實際輸出的token。

##### 2-2-3-1 貪婪取樣 (Greedy Sampling)

*   **定義/核心觀念**
    在每一步生成時，總是選擇機率最高的那個token作為下一個輸出。
*   **例子或推導**
    假設模型在當前步對下一個token的輸出機率為：`{"a": 0.5, "b": 0.3, "c": 0.2}`。貪婪取樣會直接選擇 "a"。
    如果繼續生成，在下一個時間步模型輸出 `{"x": 0.6, "y": 0.4}`，則選擇 "x"。最終序列為 "ax..."。
*   **與相鄰概念的關聯**
    最簡單直接的取樣方法，生成速度快。然而，它缺乏多樣性，容易陷入重複的模式或局部最優解，產生不自然或缺乏創意的內容。在許多開放式生成任務中表現不佳。

##### 2-2-3-2 束搜索 (Beam Search)

*   **定義/核心觀念**
    束搜索不像貪婪取樣那樣只保留一個最佳路徑，而是在每一步保留 `k` 個（束寬，beam width）當前最優的候選序列，並從這 `k` 個序列中擴展，直到達到停止條件。最終從所有完成的序列中選擇累積機率最高的那個。
*   **例子或推導**
    假設 `k=2`。
    1.  第一步，模型輸出 `{"a": 0.5, "b": 0.3, "c": 0.2}`。選擇 "a" (0.5) 和 "b" (0.3) 作為兩個候選序列。
    2.  第二步，從 "a" 擴展，得到 `{"ax": 0.5*0.6=0.3, "ay": 0.5*0.4=0.2}`。從 "b" 擴展，得到 `{"bx": 0.3*0.7=0.21, "by": 0.3*0.3=0.09}`。
    3.  比較四個序列的累積機率：`ax (0.3), ay (0.2), bx (0.21), by (0.09)`。選擇 `ax` (0.3) 和 `bx` (0.21) 作為新的兩個候選序列。
    依此類推。
*   **與相鄰概念的關聯**
    束搜索比貪婪取樣能找到更高機率的序列，常應用於機器翻譯、摘要等需要高精確度和流暢度的任務。但它仍然是確定性的，可能產生重複的短語，且隨著 `k` 的增加，計算成本會顯著上升。

##### 2-2-3-3 隨機取樣與溫度 (Random Sampling with Temperature)

*   **定義/核心觀念**
    **隨機取樣**是根據模型輸出的機率分佈直接進行隨機抽樣，而不是簡單地選擇機率最高的token。
    **溫度 (Temperature, $T$)** 是一個超參數，用於調整機率分佈的「尖銳度」。高溫 ($T > 1$) 會使分佈變平坦，增加低機率token被選中的機會，生成更多樣但可能更隨機的內容。低溫 ($T < 1$) 會使分佈更尖銳，趨向於高機率token，生成更保守的內容。當 $T \to 0$ 時，行為近似於貪婪取樣。
*   **例子或推導**
    給定 logits $\mathbf{z} = [z_1, z_2, \dots, z_V]$，其中 $V$ 是詞彙表大小。
    經過溫度調整後的機率分佈 $P_T(\text{token}_i)$ 為：
    $$
    P_T(\text{token}_i) = \frac{\exp(z_i / T)}{\sum_{j=1}^{V} \exp(z_j / T)}
    $$
    然後從這個調整後的機率分佈中進行隨機抽樣。
*   **與相鄰概念的關聯**
    引入了隨機性，是實現生成內容多樣性的基礎。然而，單純的隨機取樣（尤其是在高溫下）可能導致生成不連貫、不合理的內容。因此，它通常與 Top-K 或 Nucleus Sampling 結合使用。

##### 2-2-3-4 Top-K 取樣 (Top-K Sampling)

*   **定義/核心觀念**
    Top-K 取樣首先對模型輸出的所有token機率進行排序，然後只從機率最高的 `k` 個token中進行隨機抽樣。這在保持一定多樣性的同時，排除了機率極低的、可能產生無意義內容的token。
*   **例子或推導**
    假設模型輸出機率為 `{"a": 0.5, "b": 0.3, "c": 0.15, "d": 0.04, "e": 0.01}`。
    如果 `k=3`，則只考慮 `{"a": 0.5, "b": 0.3, "c": 0.15}` 這三個token進行隨機抽樣。`d` 和 `e` 被排除。
*   **與相鄰概念的關聯**
    Top-K 是在隨機取樣的基礎上進行改進，提供了一個在多樣性和品質之間取得平衡的方法。其 `k` 值為固定值，對於不同上下文的機率分佈可能不總是最佳選擇。

##### 2-2-3-5 核取樣 (Nucleus Sampling / Top-P Sampling)

*   **定義/核心觀念**
    核取樣是一種更動態的Top-K變體。它不是固定選擇 `k` 個機率最高的token，而是選擇一個最小的token集合，使得這個集合中token的累積機率超過一個閾值 `p`。然後從這個集合中進行隨機抽樣。
*   **例子或推導**
    假設模型輸出機率同上：`{"a": 0.5, "b": 0.3, "c": 0.15, "d": 0.04, "e": 0.01}`。
    如果 `p=0.9`：
    1.  排序：a(0.5), b(0.3), c(0.15), d(0.04), e(0.01)
    2.  累積機率：
        *   a: 0.5
        *   a + b: 0.5 + 0.3 = 0.8
        *   a + b + c: 0.8 + 0.15 = 0.95
    3.  因為 0.95 > 0.9，所以選擇集合 `{a, b, c}`。然後從 `{a: 0.5, b: 0.3, c: 0.15}` 中進行隨機抽樣。
    在不同的上下文下，這個集合的大小 `k` 會動態變化。
*   **與相鄰概念的關聯**
    核取樣在多樣性和品質之間提供了更好的平衡，尤其是在模型對某些token給出極高機率時（如常見詞），它會自動縮小候選集；而在機率分佈較平坦時（如創作性表達），它會擴大候選集。這使得它在開放式文本生成（如故事、對話）中表現優異。

-----

#### 2-2-4 後訓練與取樣的協同應用

後訓練技術使模型能夠以更小的體積和更高的效率運行，這為實施更複雜的取樣策略提供了可能。例如：

*   **資源限制下的高品質生成**：在邊緣設備上，經過量化或剪枝的模型可以以較低的計算成本運行。這使得我們即便在資源有限的設備上，也能夠利用核取樣或帶有溫度的Top-K取樣來生成高品質、多樣化的內容，而無需僅依賴性能較差的貪婪取樣。
*   **大規模部署的效率提升**：對於大型語言模型在雲端的大規模服務，後訓練可以大幅降低單次推理的延遲和成本。結合高效的取樣（如優化後的束搜索或並行 Top-P 取樣），可以實現在高並發情境下快速且多樣的內容生成。
*   **模型迭代與適應**：微調後的模型能夠更好地理解特定任務的語義和風格。在此基礎上，選擇恰當的取樣策略，可以確保生成內容更符合任務要求，例如在一個經過幽默風格微調的模型上，使用高溫度和Top-P取樣可能產生更有趣的回答。

-----

#### 2-2-5 常見錯誤與澄清

*   **量化不是萬能藥，精度損失是潛在代價**
    量化確實能帶來巨大的效率提升，但它本質上是一種有損壓縮。激進的量化（如從 FP32 到 INT4）可能會導致模型精度顯著下降，尤其是在對精度敏感的任務上。QAT 通常能減輕這個問題，但會增加訓練複雜度。
*   **後訓練與微調的區別**
    微調（Fine-tuning）是模型訓練的一個階段，通常是利用預訓練模型在特定任務數據上進行繼續訓練，以適應新任務。後訓練（Post-training）則更側重於模型訓練完成後，為部署而進行的優化（如量化、剪枝、蒸餾），這些步驟通常不涉及大規模的數據學習，而是對模型結構或參數表示進行調整。微調可以作為後訓練優化的前置步驟，也可以與後訓練技術結合使用（例如微調一個量化感知模型）。
*   **貪婪取樣不總是「最佳」選擇**
    雖然貪婪取樣在每一步都選擇機率最高的token，但這不保證最終生成的整個序列是全局最優的。它可能導致生成內容重複、缺乏新穎性或陷入局部最優。在需要創意、多樣性或長篇連貫性的任務中，隨機取樣、Top-K 或核取樣通常表現更好。
*   **束搜索不保證找到全局最優序列**
    束搜索雖然比貪婪取樣更優，但由於其固定 `k` 個候選的限制，它仍然是一種啟發式搜索，不保證找到數學上的全局最優序列。束寬 `k` 越大，越接近全局最優，但計算成本也越高。
*   **高溫度或大 `p`/`k` 值並非總是更好**
    雖然增加溫度或擴大 Top-K/Top-P 的範圍可以增加生成內容的多樣性，但過高的溫度或過大的 `k`/`p` 值可能導致模型生成不連貫、不相關或完全胡言亂語的內容。選擇合適的取樣參數需要根據具體任務和模型特性進行調整和實驗。

-----

#### 2-2-6 小練習

##### 小練習 1: 量化應用選擇

**情境：** 你正在開發一個運行在資源受限的物聯網（IoT）設備上的語音助手應用。該設備只有極小的記憶體和有限的計算能力，但需要一個相當精確的語言理解模型。

**問題：** 你會建議採用哪種或哪些後訓練技術來優化這個模型？請說明你的選擇理由，並考慮潛在的權衡。

**詳解：**

1.  **首選技術：模型量化 (Quantization)**
    *   **理由：**
        *   **大幅降低記憶體佔用：** 將 FP32 參數轉換為 INT8 或更低精度可以顯著減少模型文件大小和運行時記憶體消耗，這對於記憶體極小的 IoT 設備至關重要。
        *   **提升推理速度：** 低精度計算在許多嵌入式處理器上可以得到硬體加速，從而降低延遲，提升語音助手的響應速度。
        *   **降低功耗：** 更少的記憶體讀寫和更快的計算通常也意味著更低的能耗，這對電池供電的 IoT 設備至關重要。
    *   **權衡：**
        *   **精度損失：** 量化是一種有損壓縮，可能會導致語音理解模型的精度輕微下降。需要仔細評估在可接受的精度範圍內進行量化。
        *   **實施複雜性：** 考慮採用量化感知訓練 (QAT) 而非訓練後量化 (PTQ)，因為 QAT 通常能保持更好的精度，但在實施上會增加訓練流程的複雜度。

2.  **輔助技術：模型剪枝 (Pruning)**
    *   **理由：**
        *   **進一步壓縮：** 如果量化後模型仍然過大或過慢，剪枝可以作為補充手段，移除冗餘的神經元或連接，進一步減少模型大小和計算量。
        *   **結構化剪枝優於非結構化：** 考慮到 IoT 設備的通用硬體特性，結構化剪枝（如移除通道）可能更優，因為它能產生更小的密集模型，更容易被標準庫和硬體加速器利用，而無需特殊的稀疏矩陣處理。
    *   **權衡：**
        *   **潛在的更大精度損失：** 剪枝不當可能導致比量化更明顯的精度下降。
        *   **需要微調：** 剪枝後通常需要進行少量微調（fine-tuning）來恢復模型的性能。

**總結：** 對於資源受限的 IoT 設備上的語音助手，**模型量化**是核心優化手段，它直接解決了記憶體和計算效率的問題。**模型剪枝**可以作為補充，進一步壓縮模型，但需仔細權衡精度和複雜度。

---

##### 小練習 2: 取樣策略比較

**情境：** 你正在為一個寫作助手開發一個功能，該功能需要根據用戶輸入的開頭，生成多個（例如 3-5 個）不同但都合理且富有創意的故事開頭。

**問題：** 比較貪婪取樣 (Greedy Sampling) 和核取樣 (Nucleus Sampling / Top-P Sampling) 在此情境下的適用性。你會推薦哪種策略，並解釋原因？

**詳解：**

1.  **貪婪取樣 (Greedy Sampling) 在此情境下的適用性：**
    *   **優點：** 快速，每次都選擇機率最高的token，結果確定性高。
    *   **缺點：**
        *   **缺乏多樣性：** 總是選擇最「安全」的詞語，生成的內容往往單一、重複，缺乏創意和新穎性。
        *   **局部最優：** 可能會陷入重複的循環，無法探索出更長遠、更具吸引力的故事情節。
        *   **單一輸出：** 一次只能生成一個序列，要生成多個建議，需要重複運行，且每次結果基本相同。
    *   **結論：** 貪婪取樣不適合需要「多個不同但富有創意的故事開頭」的需求。

2.  **核取樣 (Nucleus Sampling / Top-P Sampling) 在此情境下的適用性：**
    *   **優點：**
        *   **高多樣性：** 允許從一個動態選擇的、累積機率達到 `p` 的token集合中隨機抽樣，這使得模型能夠探索更廣泛的詞彙和表達，產生更多樣化的故事開頭。
        *   **保持合理性：** 通過設定 `p` 值，可以排除機率極低的、可能產生不連貫內容的token，從而在保持多樣性的同時，確保生成的內容仍然具有一定的合理性和連貫性。
        *   **動態適應：** 候選集合的大小會根據上下文的機率分佈動態調整，這比固定 `k` 的 Top-K 取樣更靈活，對於不同的文本生成情況都能有較好的表現。
        *   **產生多個獨特輸出：** 由於引入了隨機性，每次運行可以得到不同的生成結果，正好滿足生成多個不同故事開頭的需求。
    *   **缺點：**
        *   **計算成本略高：** 需要對所有token的機率進行排序和累計，比貪婪取樣略慢。
        *   **參數調優：** 需要調整 `p` 值和可能的溫度參數，以找到多樣性與品質的最佳平衡點。

3.  **推薦策略及原因：**
    我會強烈推薦使用**核取樣 (Top-P Sampling)**，並結合適當的**溫度 (Temperature)** 參數。

    *   **核心原因：** 該功能的核心需求是「多個不同但都合理且富有創意的故事開頭」。核取樣恰好能在「多樣性」和「合理性/品質」之間取得良好平衡。它通過隨機抽樣保證了創意和不同，而通過累積機率閾值 `p` 保證了結果不至於荒謬。結合適當的溫度參數可以進一步調整生成內容的「保守」程度與「發散」程度，以達到最佳的創意效果。
    *   **操作建議：** 可以嘗試設置 `p` 值在 0.8 到 0.95 之間，並調整溫度 `T` 在 0.7 到 1.0 之間，然後生成多個序列，讓用戶選擇最喜歡的。

-----

#### 2-2-7 延伸閱讀/參考

*   **模型量化：**
    *   「Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference」 (TensorFlow Lite 量化白皮書)
    *   PyTorch Quantization documentation: [https://pytorch.org/docs/stable/quantization.html](https://pytorch.org/docs/stable/quantization.html)
    *   ONNX Runtime quantization: [https://onnxruntime.ai/docs/performance/quantization.html](https://onnxruntime.ai/docs/performance/quantization.html)
*   **模型剪枝：**
    *   「Learning both Weights and Connections for Efficient Neural Networks」 (Han et al., 2015)
    *   「Pruning neural networks: is it time to nip it in the bud?」 (Blalock et al., 2020)
*   **知識蒸餾：**
    *   「Distilling the Knowledge in a Neural Network」 (Hinton et al., 2015)
    *   「BERT-of-Theseus: Compressing BERT by Progressive Module Replacing」 (Xu et al., 2020)
*   **微調與 PEFT：**
    *   「BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding」 (Devlin et al., 2018)
    *   「LoRA: Low-Rank Adaptation of Large Language Models」 (Hu et al., 2021)
*   **取樣策略：**
    *   Hugging Face Transformers庫的生成配置文檔，詳細介紹了各種取樣參數：[https://huggingface.co/docs/transformers/main_classes/text_generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
    *   「The Curious Case of Neural Text Degeneration」 (Holtzman et al., 2019) – 提出了核取樣 (Nucleus Sampling) 並分析了傳統取樣策略的問題。
### Chapter 3 評估方法

#### 3.1 評估方法的核心概念與重要性

##### 3.1.1 定義：什麼是評估方法？
評估方法是指一套用來衡量機器學習模型效能與泛化能力的技術與指標。它能幫助我們理解模型在未見過數據上的表現，判斷模型是否可靠，並在多個模型中做出選擇。

##### 3.1.2 為什麼需要評估？泛化能力 (Generalization Ability)
模型的最終目標是在真實世界中應用，也就是在它未曾見過的數據（新數據）上也能做出準確的預測。這種在未見過數據上保持良好性能的能力稱為**泛化能力**。
如果一個模型僅僅在訓練數據上表現良好，但在新數據上表現差勁，那麼這個模型是沒有實際價值的。評估方法就是用來衡量這種泛化能力的關鍵工具。

##### 3.1.3 過擬合 (Overfitting) 與欠擬合 (Underfitting)
*   **過擬合 (Overfitting)**：當模型過於複雜，在訓練數據上表現得「太好」，以至於學習到了訓練數據中的噪音和特有模式，而非數據的普遍規律。這會導致模型在訓練集上性能極佳，但在測試集（未見過數據）上性能急劇下降。過擬合的模型缺乏泛化能力。
*   **欠擬合 (Underfitting)**：當模型過於簡單，無法捕捉數據中的基本模式。這會導致模型在訓練集和測試集上表現都差勁。欠擬合的模型通常訓練不足或特徵選擇不當。

評估方法能夠幫助我們識別模型是否存在過擬合或欠擬合，從而指導模型的調整與優化。

##### 3.1.4 評估在機器學習生命週期中的位置
評估是機器學習項目中不可或缺的一環，它通常發生在模型訓練之後、部署之前，並且是模型選擇、參數調優和迭代改進的基礎。

-----

#### 3.2 分類模型評估指標：混淆矩陣 (Confusion Matrix)

##### 3.2.1 定義：什麼是混淆矩陣？
混淆矩陣 (Confusion Matrix) 是一個表格，用於可視化分類演算法的性能。它顯示了模型對真實類別和預測類別之間的對應關係，特別適用於二元分類問題。

##### 3.2.2 組成元素
在二元分類問題中，我們通常將兩個類別標記為「正例 (Positive)」和「負例 (Negative)」。
混淆矩陣由四個基本元素構成：

|             | **預測為正例 (Predicted Positive)** | **預測為負例 (Predicted Negative)** |
| :---------- | :--------------------------------: | :--------------------------------: |
| **實際為正例 (Actual Positive)** | 真正例 (True Positive, TP)         | 假負例 (False Negative, FN)        |
| **實際為負例 (Actual Negative)** | 假正例 (False Positive, FP)        | 真負例 (True Negative, TN)         |

*   **真正例 (True Positive, TP)**：模型正確地將正例預測為正例。
*   **假正例 (False Positive, FP)**：模型錯誤地將負例預測為正例（Type I error）。
*   **假負例 (False Negative, FN)**：模型錯誤地將正例預測為負例（Type II error）。
*   **真負例 (True Negative, TN)**：模型正確地將負例預測為負例。

##### 3.2.3 典型例子：二元分類問題
想像一個醫療診斷模型，旨在檢測患者是否患有某種疾病（正例）。

*   **TP**：患者實際患病，模型預測患病。
*   **FP**：患者實際未患病，模型預測患病（誤診）。
*   **FN**：患者實際患病，模型預測未患病（漏診）。
*   **TN**：患者實際未患病，模型預測未患病。

混淆矩陣的例子：
假設一個模型對 100 名患者進行預測：
實際患病人數：30 人
實際未患病人數：70 人

模型預測結果：
|             | 預測患病 | 預測未患病 |
| :---------- | :------: | :--------: |
| **實際患病** |    25    |     5      |
| **實際未患病** |    10    |     60     |

從這個矩陣中，我們可以看到：
*   TP = 25 (25 個實際患病者被正確診斷為患病)
*   FN = 5 (5 個實際患病者被誤診為未患病)
*   FP = 10 (10 個實際未患病者被誤診為患病)
*   TN = 60 (60 個實際未患病者被正確診斷為未患病)

總人數為 $25 + 5 + 10 + 60 = 100$ 人。

-----

#### 3.3 基於混淆矩陣的常見評估指標

從混淆矩陣中，我們可以衍生出多種評估分類模型性能的指標。不同的指標強調模型在不同方面的表現。

##### 3.3.1 準確度 (Accuracy)
*   **定義**：模型正確預測的樣本數佔總樣本數的比例。
    $$ \text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN} $$
*   **意義**：衡量模型整體分類正確的能力。
*   **優缺點**：
    *   **優點**：直觀易懂，計算簡單。
    *   **缺點**：當類別分佈不平衡時（即某一類別的樣本數量遠多於另一類別），準確度會產生誤導。例如，在一個 99% 樣本是負例的數據集中，一個總是預測為負例的模型也能達到 99% 的準確度，但它對正例的識別能力是零。

##### 3.3.2 精確率 (Precision)
*   **定義**：在模型預測為正例的所有樣本中，實際為正例的比例。
    $$ \text{Precision} = \frac{TP}{TP + FP} $$
*   **意義**：衡量模型在預測正例時的「精確性」。高精確率意味著模型預測為正例的結果中，很少有是錯誤的（即誤報少）。
*   **適用場景**：當假正例 (FP) 的成本很高時，例如垃圾郵件過濾（不希望把正常郵件誤判為垃圾郵件）。

##### 3.3.3 召回率 (Recall / Sensitivity)
*   **定義**：在所有實際為正例的樣本中，模型正確預測為正例的比例。
    $$ \text{Recall} = \frac{TP}{TP + FN} $$
*   **意義**：衡量模型找出所有實際正例的能力。高召回率意味著模型很少漏掉真正的正例（即漏報少）。
*   **適用場景**：當假負例 (FN) 的成本很高時，例如疾病診斷（不希望把實際患病者誤判為健康）、詐騙交易檢測（不希望漏掉真正的詐騙交易）。

##### 3.3.4 F1 分數 (F1-score)
*   **定義**：精確率 (Precision) 和召回率 (Recall) 的調和平均值。
    $$ F1\text{-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} $$
*   **意義**：綜合考慮了精確率和召回率，是對兩者的一種平衡。當精確率和召回率都較高時，F1 分數才會高。它對於類別不平衡問題比單純的準確度更具參考價值。
*   **與相鄰概念的關聯**：F1 分數是 $F_{\beta}$ 分數的一個特例，其中 $\beta=1$。$F_{\beta}$ 分數允許我們調整精確率和召回率的相對重要性（$\beta > 1$ 更看重召回，$\beta < 1$ 更看重精確）。

##### 3.3.5 與相鄰概念的關聯：精確率與召回率的權衡 (Precision-Recall Trade-off)
在許多分類問題中，提高精確率往往會降低召回率，反之亦然。例如，要提高疾病診斷的召回率（減少漏診），模型可能會變得更敏感，將更多人判斷為患病，這也可能導致精確率下降（增加誤診）。因此，根據具體的應用場景，我們需要權衡這兩個指標，選擇最合適的模型。

-----

#### 3.4 ROC 曲線 (Receiver Operating Characteristic Curve) 與 AUC (Area Under the Curve)

##### 3.4.1 定義：ROC 曲線的原理
ROC 曲線是一種圖形化工具，用於評估二元分類模型的性能，尤其是在不同分類閾值下的表現。它繪製的是**真陽性率 (True Positive Rate, TPR)** 與**假陽性率 (False Positive Rate, FPR)** 之間的關係曲線。

*   **真陽性率 (TPR)**：等同於召回率 (Recall)。
    $$ \text{TPR} = \frac{TP}{TP + FN} $$
    衡量模型識別出所有正例的能力。
*   **假陽性率 (FPR)**：
    $$ \text{FPR} = \frac{FP}{FP + TN} $$
    衡量模型將負例誤判為正例的比例。

##### 3.4.2 推導：如何繪製 ROC 曲線
許多分類模型（如邏輯迴歸、SVM、決策樹）並非直接輸出類別標籤，而是輸出一個介於 0 到 1 之間的分數（或機率），表示樣本屬於正例的可能性。我們需要設定一個**分類閾值 (Threshold)**：
*   如果分數 ≥ 閾值，則預測為正例。
*   如果分數 < 閾值，則預測為負例。

ROC 曲線的繪製步驟：
1.  獲得模型對所有測試樣本的預測分數。
2.  對所有可能的閾值（通常是模型輸出的不同分數值）進行迭代。
3.  對於每一個閾值，計算出該閾值下的 TPR 和 FPR。
4.  將 (FPR, TPR) 作為一個點繪製在二維座標系上。
5.  將所有點連接起來，就形成了 ROC 曲線。

理想的分類器會將所有正例排在所有負例之前，其 ROC 曲線會經過 (0,1) 點（FPR=0, TPR=1），表示在沒有任何假陽性的情況下，所有正例都被識別出來。隨機猜測的分類器會產生一條從 (0,0) 到 (1,1) 的對角線（稱為「隨機線」）。好的分類器曲線會盡可能地向左上方彎曲。

##### 3.4.3 AUC (Area Under the Curve) 的意義
*   **定義**：AUC 是 ROC 曲線下的面積。
*   **意義**：AUC 值介於 0.5 和 1 之間。
    *   AUC 越高，表示模型在隨機選擇的正例和負例中，區分它們的能力越強。
    *   AUC 等於 1 表示一個完美分類器。
    *   AUC 等於 0.5 表示模型性能與隨機猜測相當。
    *   AUC 小於 0.5 表示模型性能差於隨機猜測，可能學習到了錯誤的模式（反向預測）。
*   **優點**：
    *   **閾值不敏感**：AUC 衡量的是模型在所有可能分類閾值下的整體性能，因此它不受特定閾值選擇的影響。這使得它成為評估模型優劣的一個強大指標。
    *   **類別不平衡魯棒性**：AUC 對於類別不平衡問題相對不敏感，因為 TPR 和 FPR 都只關注各自類別內部的情況。

##### 3.4.4 與相鄰概念的關聯：解決單一閾值指標的局限性
傳統的混淆矩陣指標（如準確度、精確率、召回率、F1 分數）都需要設定一個固定的分類閾值。這使得在不同應用場景下，選擇一個「最佳」閾值成為一個挑戰。ROC 曲線和 AUC 則提供了一個「無閾值」的評估方式，能夠更全面地反映模型在不同敏感度下的區分能力，幫助我們理解模型的潛在表現。

-----

#### 3.5 評估策略：交叉驗證 (Cross-Validation)

##### 3.5.1 核心觀念：為什麼需要交叉驗證？避免數據劃分偏差
在模型訓練中，我們通常會將數據集劃分為訓練集 (Training Set) 和測試集 (Test Set)。訓練集用於訓練模型，測試集則用於評估模型的泛化能力。
然而，如果我們只進行一次隨機劃分，測試集的評估結果可能由於這次特定的數據劃分而存在偶然性或偏差。例如，如果測試集中的樣本特別「容易」或特別「困難」，就會導致評估結果不準確。

**交叉驗證**是一種更可靠的評估策略，它通過多次訓練和測試模型，利用數據的多次劃分來獲得更穩健、更可靠的性能評估。它有助於：
*   減少評估結果對單一數據劃分的依賴性。
*   更全面地評估模型的泛化能力。
*   充分利用有限的數據。

##### 3.5.2 典型例子：K-Fold 交叉驗證 (K-Fold Cross-Validation)
K-Fold 交叉驗證是最常用的一種交叉驗證方法。
其步驟如下：
1.  將整個數據集隨機劃分為 K 個大小近似相等、互不重疊的子集（稱為「折疊」或「folds」）。
2.  進行 K 次迭代：
    *   在每次迭代中，選擇其中一個子集作為**驗證集 (Validation Set)**，其餘 K-1 個子集聯合起來作為**訓練集 (Training Set)**。
    *   使用訓練集訓練模型。
    *   使用驗證集評估模型的性能，記錄下評估指標。
3.  K 次迭代完成後，將 K 次評估指標的平均值作為模型最終的性能評估結果。

**例子**：5-Fold 交叉驗證
*   數據集被分成 5 個子集 (Fold 1, Fold 2, Fold 3, Fold 4, Fold 5)。
*   **第 1 次**：用 Fold 2-5 訓練模型，用 Fold 1 驗證，得到評估結果 1。
*   **第 2 次**：用 Fold 1, 3-5 訓練模型，用 Fold 2 驗證，得到評估結果 2。
*   ...
*   **第 5 次**：用 Fold 1-4 訓練模型，用 Fold 5 驗證，得到評估結果 5。
*   最終的模型性能是這 5 個評估結果的平均值。

**其他常見的交叉驗證策略**：
*   **留一交叉驗證 (Leave-One-Out Cross-Validation, LOOCV)**：K 等於樣本總數 N。每次只用一個樣本做驗證集，其餘 N-1 個樣本做訓練集。計算成本高，但在樣本量小時可能有用。
*   **分層 K-Fold 交叉驗證 (Stratified K-Fold Cross-Validation)**：在劃分 K 個子集時，確保每個子集中各類別的比例與原始數據集中的比例大致相同。這對於類別不平衡的數據集尤為重要。

##### 3.5.3 與訓練/測試集劃分的關聯
交叉驗證彌補了單次訓練/測試集劃分的不足。在實際應用中，我們通常會先將原始數據集大致分為訓練集和一個獨立的、從不參與訓練或模型選擇的最終**測試集 (Hold-out Test Set)**。然後，在訓練集內部使用交叉驗證進行模型選擇和參數調優。最後，在模型訓練完成並選定最佳參數後，才用最初獨立劃分出來的最終測試集進行一次最終的性能評估，以確保對模型泛化能力的無偏估計。

-----

#### 3.6 常見錯誤與澄清

##### 3.6.1 只看準確度 (Accuracy) 的陷阱
*   **常見錯誤**：在所有分類問題中都使用準確度作為唯一的評估指標。
*   **澄清**：如前所述，當類別分佈不平衡時，準確度會產生誤導。例如，在電商網站中，購買商品的使用者數量遠少於瀏覽但未購買的使用者。如果一個模型總是預測「未購買」，它也能達到非常高的準確度，但這顯然是無用的。在這種情況下，應更多地關注精確率、召回率、F1 分數或 ROC/AUC。

##### 3.6.2 過擬合與欠擬合在評估中的體現
*   **常見錯誤**：混淆過擬合和欠擬合的評估結果。
*   **澄清**：
    *   **過擬合**：模型在訓練集上表現極好，但在獨立的測試集或交叉驗證中表現顯著下降。這通常意味著模型學到了訓練數據的噪音。
    *   **欠擬合**：模型在訓練集和測試集上都表現不佳。這可能表明模型不夠複雜，或者特徵不足以捕捉數據模式。
    正確地識別這兩種情況有助於指導模型選擇（例如，簡化模型以對抗過擬合，或增加模型複雜度、增加特徵以解決欠擬合）。

##### 3.6.3 測試集洩漏 (Data Leakage)
*   **常見錯誤**：在模型訓練或特徵工程階段不小心使用了測試集的信息。
*   **澄清**：數據洩漏是指在訓練模型時，使用了那些在實際應用中無法獲取的信息，或者不當地使用了測試集的信息。例如：
    *   在劃分訓練集和測試集之前進行了特徵縮放 (feature scaling) 或數據歸一化，導致測試集的統計信息（如均值、標準差）混入訓練過程。
    *   特徵工程時，計算了基於整個數據集的統計量（例如，某個特徵的全局平均值），然後應用於訓練集和測試集。
    *   模型選擇或參數調優過程中，反覆在最終測試集上進行評估，導致測試集實際上變成了另一個驗證集，失去了其「未見過」的獨立性。
*   **後果**：導致模型在評估階段表現極佳，但在部署到真實環境時性能急劇下降。
*   **避免方法**：嚴格區分訓練集、驗證集和測試集。所有特徵工程和數據預處理步驟都應該只在訓練集上學習其參數，然後應用於驗證集和測試集。最終測試集只用於一次最終、客觀的評估。

-----

#### 3.7 小練習 (附詳解)

##### 練習 1：計算混淆矩陣與各指標
某個二元分類模型用於預測客戶是否會流失。在對 2000 名客戶的測試數據上，模型獲得了以下結果：
*   實際流失的客戶中有 80 人被模型正確預測為流失。
*   實際流失的客戶中有 20 人被模型錯誤預測為未流失。
*   實際未流失的客戶中有 100 人被模型錯誤預測為流失。
*   實際未流失的客戶中有 1800 人被模型正確預測為未流失。

請計算：
1.  混淆矩陣。
2.  準確度 (Accuracy)。
3.  精確率 (Precision)。
4.  召回率 (Recall)。
5.  F1 分數 (F1-score)。

**詳解**

1.  **混淆矩陣**：
    根據題目描述：
    *   TP (True Positive, 實際流失, 預測流失) = 80
    *   FN (False Negative, 實際流失, 預測未流失) = 20
    *   FP (False Positive, 實際未流失, 預測流失) = 100
    *   TN (True Negative, 實際未流失, 預測未流失) = 1800

    混淆矩陣為：

    |             | **預測流失** | **預測未流失** |
    | :---------- | :----------: | :------------: |
    | **實際流失** |      80      |       20       |
    | **實際未流失** |     100      |      1800      |

2.  **準確度 (Accuracy)**：
    $$ \text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN} = \frac{80 + 1800}{80 + 100 + 20 + 1800} = \frac{1880}{2000} = 0.94 $$
    模型整體預測正確率為 94%。

3.  **精確率 (Precision)**：
    $$ \text{Precision} = \frac{TP}{TP + FP} = \frac{80}{80 + 100} = \frac{80}{180} \approx 0.444 $$
    在模型預測會流失的客戶中，約有 44.4% 的客戶確實流失了。

4.  **召回率 (Recall)**：
    $$ \text{Recall} = \frac{TP}{TP + FN} = \frac{80}{80 + 20} = \frac{80}{100} = 0.8 $$
    在所有實際流失的客戶中，模型成功識別出了 80% 的客戶。

5.  **F1 分數 (F1-score)**：
    $$ F1\text{-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = 2 \times \frac{0.444 \times 0.8}{0.444 + 0.8} = 2 \times \frac{0.3552}{1.244} \approx 2 \times 0.2855 \approx 0.571 $$
    F1 分數約為 0.571，綜合考慮了精確率和召回率。

##### 練習 2：理解 ROC 曲線與 AUC
假設你有兩個二元分類模型 M1 和 M2。它們在同一測試集上的 ROC 曲線如下圖所示（想像的圖形）：
*   模型 M1 的 ROC 曲線非常接近左上角，AUC 值為 0.92。
*   模型 M2 的 ROC 曲線在 M1 曲線的下方，AUC 值為 0.75。
*   還有一條對角線，代表隨機分類器，AUC 值為 0.5。

請根據上述信息判斷：
1.  哪個模型的整體分類性能更好？為什麼？
2.  如果一個應用場景要求盡量減少假陽性（FP），我們應該在哪個模型的 ROC 曲線的哪個區域選擇操作點？

**詳解**

1.  **哪個模型的整體分類性能更好？為什麼？**
    *   **判斷**：模型 M1 的整體分類性能更好。
    *   **原因**：AUC (Area Under the Curve) 值越大，表示分類器在所有可能閾值下的整體性能越好。模型 M1 的 AUC 值為 0.92，顯著高於模型 M2 的 0.75，且更接近理想分類器的 1.0。這意味著 M1 在區分正例和負例方面的能力更強。

2.  **如果一個應用場景要求盡量減少假陽性（FP），我們應該在哪個模型的 ROC 曲線的哪個區域選擇操作點？**
    *   **判斷**：我們應該選擇**模型 M1** 的 ROC 曲線，並在曲線的**左下角區域**選擇操作點。
    *   **原因**：
        *   **減少假陽性 (FP)** 意味著需要降低假陽性率 (FPR)。ROC 曲線的 X 軸就是 FPR。因此，我們需要選擇 FPR 較小的點。
        *   **左下角區域**的點，FPR 值趨近於 0，表示假陽性數量最少。
        *   同時，我們希望在減少假陽性的前提下，盡可能保持較高的真陽性率 (TPR)。由於模型 M1 的整體性能優於 M2，它能在相同的低 FPR 水平下，提供更高的 TPR，即在減少誤報的同時，也能更好地識別出真正的正例。

-----

#### 3.8 延伸閱讀/參考

*   **Matthews Correlation Coefficient (MCC)**：對於類別不平衡問題比 F1 分數更具魯棒性的單一指標。它考慮了混淆矩陣的所有四個元素 (TP, TN, FP, FN)，其值範圍從 -1 (完全錯誤預測) 到 +1 (完美預測)。
*   **Log Loss (對數損失)**：常用於評估輸出機率的分類模型，懲罰那些對實際類別給出低機率預測的模型。
*   **回歸模型評估指標**：
    *   **均方誤差 (Mean Squared Error, MSE)**：預測值與真實值之差的平方的平均值。
    *   **均方根誤差 (Root Mean Squared Error, RMSE)**：MSE 的平方根，與原始數據單位一致，更具解釋性。
    *   **平均絕對誤差 (Mean Absolute Error, MAE)**：預測值與真實值之差的絕對值的平均值。
    *   **決定係數 (Coefficient of Determination, $R^2$)**：表示模型解釋因變量變異性的比例，值介於 0 到 1 之間，$R^2$ 越高表示模型擬合效果越好。

*   **時間序列模型評估**：針對時間序列數據的特殊性，例如考慮時間順序的交叉驗證策略（如時間序列拆分）。

*   **文獻參考**：
    *   Fawcett, T. (2006). An introduction to ROC analysis. *Pattern Recognition Letters*, 27(8), 861-874. (關於 ROC 曲線的經典介紹)
    *   Géron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems* (2nd ed.). O'Reilly Media. (包含豐富的評估指標和實踐建議)
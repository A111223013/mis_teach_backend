# 線性代數基礎：矩陣、向量與特徵值

線性代數是數學的一個重要分支，它研究向量空間、線性變換以及表示這些變換的矩陣。理解矩陣、向量和特徵值是掌握線性代數的基石，它們在物理、工程、電腦科學、統計學等眾多領域都有廣泛應用。

-----

### 核心概念與定義

#### 向量 (Vectors)

##### 定義/核心觀念
在線性代數中，向量可以被視為具有大小（長度）和方向的幾何實體，或者是一個有序的數字列表。在 $n$ 維空間中，一個向量 $\mathbf{v}$ 通常表示為一個列向量（column vector）或行向量（row vector）。
例如，在二維空間中，向量 $\mathbf{v} = \begin{pmatrix} x \\ y \end{pmatrix}$ 表示從原點 $(0,0)$ 到點 $(x,y)$ 的有向線段。

##### 例子與運算
*   **列向量：** $\mathbf{v} = \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix}$
*   **行向量：** $\mathbf{w} = \begin{pmatrix} 1 & 0 & 4 \end{pmatrix}$

**基本運算：**
*   **向量加法：** 兩個相同維度的向量可以逐元素相加。
    $\begin{pmatrix} x_1 \\ y_1 \end{pmatrix} + \begin{pmatrix} x_2 \\ y_2 \end{pmatrix} = \begin{pmatrix} x_1+x_2 \\ y_1+y_2 \end{pmatrix}$
*   **純量乘法：** 一個純量（常數）與向量的每個元素相乘。
    $c \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} cx \\ cy \end{pmatrix}$
*   **點積 (Dot Product) / 內積 (Inner Product)：** 兩個相同維度向量的點積是一個純量。
    $\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T\mathbf{v} = \begin{pmatrix} u_1 & u_2 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = u_1v_1 + u_2v_2$

##### 與相鄰概念的關聯
向量是建構向量空間的基本元素。它們是矩陣乘法中的核心組成部分，尤其是在表示線性變換的輸入和輸出時。

#### 矩陣 (Matrices)

##### 定義/核心觀念
矩陣是一個由數字排列成的矩形陣列。一個 $m \times n$ 矩陣有 $m$ 列和 $n$ 行。
矩陣可以表示線性變換、數據集或線性方程組的係數。

##### 例子與運算
*   **矩陣表示：**
    $A = \begin{pmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{pmatrix}$
*   **特殊矩陣：**
    *   **方陣 (Square Matrix)：** 列數等於行數 ($m=n$)。
    *   **單位矩陣 (Identity Matrix)：** 主對角線元素為 1，其餘為 0 的方陣，記作 $I$。$I_2 = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$。
    *   **零矩陣 (Zero Matrix)：** 所有元素都是 0 的矩陣。

**基本運算：**
*   **矩陣加法/減法：** 只有相同維度的矩陣才能逐元素相加減。
*   **純量乘法：** 純量與矩陣的每個元素相乘。
*   **矩陣乘法：** 如果矩陣 $A$ 是 $m \times p$ 且矩陣 $B$ 是 $p \times n$，則它們的乘積 $AB$ 是一個 $m \times n$ 矩陣。結果矩陣 $C=AB$ 中的元素 $c_{ij}$ 是 $A$ 的第 $i$ 行與 $B$ 的第 $j$ 列的點積。
    $C_{ij} = \sum_{k=1}^p A_{ik} B_{kj}$

##### 與相鄰概念的關聯
矩陣是線性變換的數學表示。將一個向量乘以一個矩陣，其實就是對這個向量進行一次線性變換。例如，$A\mathbf{x}$ 表示將向量 $\mathbf{x}$ 透過矩陣 $A$ 進行變換。

#### 特徵值與特徵向量 (Eigenvalues and Eigenvectors)

##### 定義/核心觀念
對於一個 $n \times n$ 方陣 $A$，如果存在一個非零向量 $\mathbf{v}$ 和一個純量 $\lambda$，使得 $A\mathbf{v} = \lambda\mathbf{v}$，則稱 $\mathbf{v}$ 為矩陣 $A$ 的**特徵向量** (eigenvector)，而 $\lambda$ 為與 $\mathbf{v}$ 相關聯的**特徵值** (eigenvalue)。

**核心意義：**
特徵向量是在線性變換（由矩陣 $A$ 表示）下，其方向不變（或僅反向）的向量。儘管方向可能不變，但它們的長度會被特徵值 $\lambda$ 縮放。
*   如果 $\lambda > 1$，特徵向量被拉伸。
*   如果 $0 < \lambda < 1$，特徵向量被壓縮。
*   如果 $\lambda < 0$，特徵向量方向反轉並被縮放。
*   如果 $\lambda = 0$，特徵向量被映射到零向量。
*   如果 $\lambda = 1$，特徵向量不變。

##### 例子或推導
考慮矩陣 $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$。
假設 $\mathbf{v} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$。
$A\mathbf{v} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \cdot 1 + 1 \cdot 1 \\ 1 \cdot 1 + 2 \cdot 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 3 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 1 \end{pmatrix}$
在這裡，我們看到 $A\mathbf{v} = 3\mathbf{v}$。所以 $\mathbf{v} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ 是矩陣 $A$ 的一個特徵向量，而 $\lambda = 3$ 是與之對應的特徵值。

##### 與相鄰概念的關聯
特徵值和特徵向量揭示了線性變換的本質。它們是理解矩陣對角化、主成分分析 (PCA)、奇異值分解 (SVD) 等高級概念的關鍵。特徵值為零的矩陣不可逆，這也將其與行列式和線性方程組的解聯繫起來。

-----

### 典型例子與轉換/推導

#### 求解特徵值與特徵向量

我們從定義 $A\mathbf{v} = \lambda\mathbf{v}$ 開始推導。
將等式右側移到左側：
$A\mathbf{v} - \lambda\mathbf{v} = \mathbf{0}$

為了將純量 $\lambda$ 與矩陣 $A$ 相減，我們需要引入單位矩陣 $I$：
$A\mathbf{v} - \lambda I\mathbf{v} = \mathbf{0}$
$(A - \lambda I)\mathbf{v} = \mathbf{0}$

這個方程是一個齊次線性方程組。由於我們要求非零的特徵向量 $\mathbf{v}$，這意味著矩陣 $(A - \lambda I)$ 必須是奇異的（singular），也就是說它不可逆。對於一個方陣，不可逆的條件是其行列式為零。
所以，我們得到求解特徵值的**特徵方程式 (Characteristic Equation)**：
$\det(A - \lambda I) = 0$

##### 步驟：
1.  **計算 $A - \lambda I$：** 將矩陣 $A$ 的主對角線元素減去 $\lambda$。
2.  **計算行列式：** 求解 $\det(A - \lambda I) = 0$。這會得到一個關於 $\lambda$ 的多項式（特徵多項式）。
3.  **解特徵多項式：** 找出特徵多項式的所有根，這些根就是矩陣 $A$ 的特徵值 $\lambda_1, \lambda_2, \dots, \lambda_n$。
4.  **對每個特徵值求解特徵向量：** 對於每個特徵值 $\lambda_i$，將其代回 $(A - \lambda_i I)\mathbf{v} = \mathbf{0}$。解這個齊次線性方程組，得到的非零解就是與 $\lambda_i$ 對應的特徵向量。注意，特徵向量通常不唯一，任何特徵向量的非零純量倍數也是特徵向量。

##### 例子：計算 $2 \times 2$ 矩陣的特徵值與特徵向量

設矩陣 $A = \begin{pmatrix} 4 & 2 \\ 1 & 3 \end{pmatrix}$。

**1. 找出特徵值 $\lambda$：**
*   計算 $A - \lambda I$:
    $A - \lambda I = \begin{pmatrix} 4-\lambda & 2 \\ 1 & 3-\lambda \end{pmatrix}$
*   計算行列式並設為零：
    $\det(A - \lambda I) = (4-\lambda)(3-\lambda) - (2)(1) = 0$
    $12 - 4\lambda - 3\lambda + \lambda^2 - 2 = 0$
    $\lambda^2 - 7\lambda + 10 = 0$
*   解二次方程式：
    $(\lambda - 2)(\lambda - 5) = 0$
    所以特徵值為 $\lambda_1 = 2$ 和 $\lambda_2 = 5$。

**2. 找出對應的特徵向量 $\mathbf{v}$：**

*   **對於 $\lambda_1 = 2$：**
    代入 $(A - 2I)\mathbf{v} = \mathbf{0}$:
    $\begin{pmatrix} 4-2 & 2 \\ 1 & 3-2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
    $\begin{pmatrix} 2 & 2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
    這對應於線性方程組：
    $2x + 2y = 0 \implies x + y = 0 \implies x = -y$
    $x + y = 0 \implies x = -y$
    令 $y = t$（其中 $t \neq 0$），則 $x = -t$。
    特徵向量為 $\mathbf{v}_1 = \begin{pmatrix} -t \\ t \end{pmatrix} = t \begin{pmatrix} -1 \\ 1 \end{pmatrix}$。
    我們可以選擇一個簡單的非零向量，例如當 $t=1$ 時，$\mathbf{v}_1 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}$。

*   **對於 $\lambda_2 = 5$：**
    代入 $(A - 5I)\mathbf{v} = \mathbf{0}$:
    $\begin{pmatrix} 4-5 & 2 \\ 1 & 3-5 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
    $\begin{pmatrix} -1 & 2 \\ 1 & -2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
    這對應於線性方程組：
    $-x + 2y = 0 \implies x = 2y$
    $x - 2y = 0 \implies x = 2y$
    令 $y = s$（其中 $s \neq 0$），則 $x = 2s$。
    特徵向量為 $\mathbf{v}_2 = \begin{pmatrix} 2s \\ s \end{pmatrix} = s \begin{pmatrix} 2 \\ 1 \end{pmatrix}$。
    我們可以選擇一個簡單的非零向量，例如當 $s=1$ 時，$\mathbf{v}_2 = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$。

所以，矩陣 $A$ 的特徵值是 $\lambda_1=2$ 和 $\lambda_2=5$，對應的特徵向量分別是 $\mathbf{v}_1=\begin{pmatrix} -1 \\ 1 \end{pmatrix}$ 和 $\mathbf{v}_2=\begin{pmatrix} 2 \\ 1 \end{pmatrix}$。

-----

### 與相鄰概念的關聯

#### 矩陣對角化 (Matrix Diagonalization)

##### 核心觀念
如果一個 $n \times n$ 矩陣 $A$ 有 $n$ 個線性獨立的特徵向量，那麼它可以被對角化。這意味著存在一個可逆矩陣 $P$ 和一個對角矩陣 $D$，使得 $A = PDP^{-1}$。
其中：
*   $P$ 是由 $A$ 的特徵向量作為列向量組成的矩陣（稱為**特徵向量矩陣**）。
*   $D$ 是一個對角矩陣，其對角線元素是與 $P$ 中對應列的特徵向量順序相同的特徵值（稱為**特徵值矩陣**）。

##### 推導與應用
如果 $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ 是 $A$ 的線性獨立特徵向量，對應特徵值 $\lambda_1, \lambda_2, \dots, \lambda_n$。
構造矩陣 $P = \begin{pmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \dots & \mathbf{v}_n \end{pmatrix}$ 和 $D = \begin{pmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_n \end{pmatrix}$。
則 $AP = A\begin{pmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \dots & \mathbf{v}_n \end{pmatrix} = \begin{pmatrix} A\mathbf{v}_1 & A\mathbf{v}_2 & \dots & A\mathbf{v}_n \end{pmatrix} = \begin{pmatrix} \lambda_1\mathbf{v}_1 & \lambda_2\mathbf{v}_2 & \dots & \lambda_n\mathbf{v}_n \end{pmatrix}$。
同時，$PD = \begin{pmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \dots & \mathbf{v}_n \end{pmatrix} \begin{pmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_n \end{pmatrix} = \begin{pmatrix} \lambda_1\mathbf{v}_1 & \lambda_2\mathbf{v}_2 & \dots & \lambda_n\mathbf{v}_n \end{pmatrix}$。
所以 $AP = PD$。由於 $P$ 的列是線性獨立的，因此 $P$ 是可逆的，我們可以得到 $A = PDP^{-1}$。

**應用：計算矩陣的冪 $A^k$**
$A^k = (PDP^{-1})^k = (PDP^{-1})(PDP^{-1})\dots(PDP^{-1}) = PD(P^{-1}P)D(P^{-1}P)\dots D P^{-1}$
$A^k = PD^kP^{-1}$
由於 $D$ 是對角矩陣，$D^k$ 的計算非常簡單：
$D^k = \begin{pmatrix} \lambda_1^k & 0 & \dots & 0 \\ 0 & \lambda_2^k & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_n^k \end{pmatrix}$
這在處理迭代過程或馬爾可夫鏈等問題時非常有用。

#### 相似矩陣 (Similar Matrices)

##### 核心觀念
如果存在一個可逆矩陣 $P$，使得 $B = P^{-1}AP$，則稱矩陣 $A$ 和 $B$ 是相似的。
相似矩陣表示同一個線性變換在不同基底下的表示。

##### 與特徵值/特徵向量的關聯
相似矩陣具有相同的特徵值。
推導：
假設 $\lambda$ 是 $A$ 的特徵值， $\mathbf{v}$ 是其特徵向量，即 $A\mathbf{v} = \lambda\mathbf{v}$。
對於相似矩陣 $B = P^{-1}AP$，我們想找到 $B$ 的特徵值。
考慮 $B(P^{-1}\mathbf{v}) = (P^{-1}AP)(P^{-1}\mathbf{v}) = P^{-1}A(PP^{-1})\mathbf{v} = P^{-1}A\mathbf{v} = P^{-1}(\lambda\mathbf{v}) = \lambda(P^{-1}\mathbf{v})$。
這表示 $\lambda$ 也是 $B$ 的特徵值，且對應的特徵向量是 $P^{-1}\mathbf{v}$。
這說明在相似變換下，特徵值保持不變，而特徵向量則按照變換矩陣 $P^{-1}$ 進行變換。

#### 線性獨立與基底 (Linear Independence and Basis)

##### 核心觀念
如果一組向量中的任何一個向量都不能由其餘向量的線性組合表示，那麼這組向量就是**線性獨立**的。
如果一個向量空間中的一組線性獨立向量可以生成該空間中的所有向量，那麼這組向量就是該向量空間的一個**基底**。

##### 與特徵向量的關聯
當一個 $n \times n$ 矩陣 $A$ 有 $n$ 個不同的特徵值時，其對應的特徵向量一定是線性獨立的。這確保了矩陣可以被對角化。
即使特徵值有重複，如果每個特徵值的**幾何重數**（對應特徵空間的維度）等於其**代數重數**（該特徵值在特徵多項式中作為根的重數），矩陣仍然可以對角化。特徵向量形成的一個基底稱為**特徵基底**。

-----

### 常見錯誤與澄清

1.  **特徵向量的唯一性誤解：**
    *   **錯誤觀念：** 特徵向量是唯一的。
    *   **澄清：** 如果 $\mathbf{v}$ 是一個特徵向量，那麼任何非零純量 $c$ 乘以 $\mathbf{v}$ (即 $c\mathbf{v}$) 也是與相同特徵值 $\lambda$ 相關聯的特徵向量。這是因為 $A(c\mathbf{v}) = c(A\mathbf{v}) = c(\lambda\mathbf{v}) = \lambda(c\mathbf{v})$。通常，我們習慣選擇一個正規化（長度為 1）或包含整數的特徵向量作為代表。

2.  **混淆特徵值為零的意義：**
    *   **錯誤觀念：** 特徵值為零意味著沒有特徵向量。
    *   **澄清：** 特徵值 $\lambda=0$ 是一個完全合法的特徵值。如果 $\lambda=0$ 是一個特徵值，則 $A\mathbf{v} = 0\mathbf{v} = \mathbf{0}$。這意味著特徵向量 $\mathbf{v}$ 位於矩陣 $A$ 的零空間 (Null Space) 中。如果一個矩陣有特徵值 $0$，那麼這個矩陣是不可逆的（因為 $\det(A - 0I) = \det(A) = 0$）。

3.  **非方陣的特徵值：**
    *   **錯誤觀念：** 任何矩陣都有特徵值和特徵向量。
    *   **澄清：** 特徵值和特徵向量的定義僅適用於**方陣**。因為只有方陣才能進行 $(A - \lambda I)$ 這樣的操作並計算其行列式。

4.  **每個矩陣都能對角化：**
    *   **錯誤觀念：** 每個方陣都能被對角化。
    *   **澄清：** 只有當一個 $n \times n$ 矩陣有 $n$ 個線性獨立的特徵向量時，它才能被對角化。如果一個矩陣的幾何重數小於代數重數，它就不能被對角化。這種矩陣稱為**虧損矩陣** (defective matrix)，例如 $A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$。

-----

### 小練習（附詳解）

#### 練習一：計算特徵值與特徵向量

**題目：**
計算矩陣 $B = \begin{pmatrix} 1 & 3 \\ 3 & 1 \end{pmatrix}$ 的特徵值和特徵向量。

**詳解：**

1.  **計算特徵值：**
    *   設定特徵方程式 $\det(B - \lambda I) = 0$。
        $B - \lambda I = \begin{pmatrix} 1-\lambda & 3 \\ 3 & 1-\lambda \end{pmatrix}$
    *   計算行列式：
        $(1-\lambda)(1-\lambda) - (3)(3) = 0$
        $(1-\lambda)^2 - 9 = 0$
        $1 - 2\lambda + \lambda^2 - 9 = 0$
        $\lambda^2 - 2\lambda - 8 = 0$
    *   解二次方程式：
        $(\lambda - 4)(\lambda + 2) = 0$
        所以特徵值為 $\lambda_1 = 4$ 和 $\lambda_2 = -2$。

2.  **計算對應的特徵向量：**

    *   **對於 $\lambda_1 = 4$：**
        代入 $(B - 4I)\mathbf{v} = \mathbf{0}$:
        $\begin{pmatrix} 1-4 & 3 \\ 3 & 1-4 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
        $\begin{pmatrix} -3 & 3 \\ 3 & -3 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
        這對應於線性方程組：
        $-3x + 3y = 0 \implies x = y$
        $3x - 3y = 0 \implies x = y$
        令 $y=t$（$t \neq 0$），則 $x=t$。
        特徵向量為 $\mathbf{v}_1 = t \begin{pmatrix} 1 \\ 1 \end{pmatrix}$。可選取 $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$。

    *   **對於 $\lambda_2 = -2$：**
        代入 $(B - (-2)I)\mathbf{v} = \mathbf{0}$:
        $\begin{pmatrix} 1-(-2) & 3 \\ 3 & 1-(-2) \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
        $\begin{pmatrix} 3 & 3 \\ 3 & 3 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
        這對應於線性方程組：
        $3x + 3y = 0 \implies x = -y$
        $3x + 3y = 0 \implies x = -y$
        令 $y=s$（$s \neq 0$），則 $x=-s$。
        特徵向量為 $\mathbf{v}_2 = s \begin{pmatrix} -1 \\ 1 \end{pmatrix}$。可選取 $\begin{pmatrix} -1 \\ 1 \end{pmatrix}$。

**結論：**
矩陣 $B$ 的特徵值是 $\lambda_1=4, \lambda_2=-2$，對應的特徵向量分別是 $\mathbf{v}_1=\begin{pmatrix} 1 \\ 1 \end{pmatrix}, \mathbf{v}_2=\begin{pmatrix} -1 \\ 1 \end{pmatrix}$。

#### 練習二：判斷是否為特徵向量

**題目：**
判斷向量 $\mathbf{v} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$ 是否為矩陣 $C = \begin{pmatrix} 5 & 2 \\ -1 & 2 \end{pmatrix}$ 的特徵向量。如果是，請找出對應的特徵值。

**詳解：**

1.  **計算 $C\mathbf{v}$：**
    $C\mathbf{v} = \begin{pmatrix} 5 & 2 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 5 \cdot 1 + 2 \cdot (-1) \\ -1 \cdot 1 + 2 \cdot (-1) \end{pmatrix} = \begin{pmatrix} 5 - 2 \\ -1 - 2 \end{pmatrix} = \begin{pmatrix} 3 \\ -3 \end{pmatrix}$

2.  **檢查 $C\mathbf{v}$ 是否為 $\lambda\mathbf{v}$ 的形式：**
    我們有 $C\mathbf{v} = \begin{pmatrix} 3 \\ -3 \end{pmatrix}$。
    將其與 $\mathbf{v} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$ 比較，可以發現 $\begin{pmatrix} 3 \\ -3 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ -1 \end{pmatrix}$。

3.  **得出結論：**
    由於 $C\mathbf{v} = 3\mathbf{v}$，所以向量 $\mathbf{v} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$ 是矩陣 $C$ 的特徵向量，並且它對應的特徵值是 $\lambda = 3$。

-----

### 延伸閱讀與參考

*   **經典教科書：**
    *   Gilbert Strang, *Introduction to Linear Algebra*
    *   David C. Lay, *Linear Algebra and Its Applications*
    *   Serge Lang, *Linear Algebra*
*   **線上課程：**
    *   MIT OpenCourseWare - 18.06 Linear Algebra (Gilbert Strang)
    *   可汗學院 (Khan Academy) - 線性代數系列課程
*   **實際應用：**
    *   **主成分分析 (Principal Component Analysis, PCA)：** 在數據科學和機器學習中，用於降維和特徵提取。它使用數據的共變異數矩陣的特徵向量來找到數據集中最重要的方向（主成分）。
    *   **PageRank 演算法：** Google 搜尋引擎的核心技術之一，利用網頁連結矩陣的特徵向量來評估網頁的重要性。
    *   **振動分析：** 在工程領域，特徵值和特徵向量用於分析結構的固有頻率和模態形狀。
    *   **量子力學：** 薛丁格方程式的解就是特徵值問題，特徵值代表能量，特徵向量代表量子態。

理解特徵值和特徵向量不僅是線性代數的理論核心，更是通往許多現代科學與工程領域應用的大門。
# 3-1 語言建模指標與精確評估

#### 課程目標

本章節旨在深入探討語言模型 (Language Model, LM) 的評估指標。我們將理解這些指標的定義、計算方式，以及它們如何幫助我們量化模型對語言現象的預測能力。透過本章的學習，您將能夠：

*   理解困惑度 (Perplexity, PPL)、每字元位元數 (Bits Per Character, BPC) 與交叉熵 (Cross-Entropy, CE) 的核心概念。
*   掌握這些指標的數學推導與計算方法。
*   辨識不同評估指標之間的關聯性。
*   了解語言模型評估的常見問題與限制。

-----

### 1. 核心概念與定義

語言建模的目標是學習語言序列的機率分佈，即預測下一個詞或字元的機率。為了量化模型預測的準確性，我們需要標準化的評估指標。本節將介紹最常用的三個內部評估指標：困惑度、每字元位元數與交叉熵。

#### 1.1 語言建模回顧

一個語言模型 $P$ 會賦予一個詞序列 $W = (w_1, w_2, \dots, w_N)$ 一個機率值 $P(W)$。根據鏈式法則，這個機率可以分解為：

$$ P(W) = P(w_1, w_2, \dots, w_N) = \prod_{i=1}^{N} P(w_i | w_1, \dots, w_{i-1}) $$

其中 $P(w_i | w_1, \dots, w_{i-1})$ 是在給定前 $i-1$ 個詞的條件下，第 $i$ 個詞出現的機率。評估指標的任務就是衡量模型預測這些條件機率的能力。

#### 1.2 困惑度 (Perplexity, PPL)

##### 定義/核心觀念
困惑度是語言模型最常用的評估指標之一。它量化了模型對測試語料的「不確定性」程度。直觀地說，**困惑度越低，表示模型對測試語料的預測能力越強，模型的表現越好。**

困惑度可以理解為模型在預測下一個詞時，平均有多少個「等可能」的詞彙可供選擇。例如，如果一個模型的困惑度是 100，則表示它在每個時間步上，平均有 100 個詞彙是同樣可能的。

##### 數學表達
對於一個包含 $N$ 個詞的測試語料 $W = (w_1, w_2, \dots, w_N)$，其困惑度定義為：

$$ PPL(W) = P(w_1, w_2, \dots, w_N)^{-\frac{1}{N}} $$

將鏈式法則代入，得到：

$$ PPL(W) = \left( \prod_{i=1}^{N} P(w_i | w_1, \dots, w_{i-1}) \right)^{-\frac{1}{N}} $$

在實際計算中，由於機率值通常很小，連乘會導致浮點數下溢。因此，我們通常會轉化為對數形式進行計算，然後再取指數：

$$ PPL(W) = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, \dots, w_{i-1}) \right) $$

其中 $\log$ 通常是自然對數 (ln)。

#### 1.3 交叉熵 (Cross-Entropy, CE)

##### 定義/核心觀念
交叉熵源自資訊理論，用於衡量兩個機率分佈之間的差異。在語言建模中，我們希望語言模型 $P_M$ 能夠盡可能地逼近真實的語言分佈 $P_T$ (即測試語料所呈現的分佈)。交叉熵正是衡量這種「近似程度」的指標。

對於一個事件，交叉熵可以解釋為「使用由模型 $P_M$ 最佳化的編碼，來編碼真實分佈 $P_T$ 所需的平均位元數」。**交叉熵值越低，表示模型分佈 $P_M$ 與真實分佈 $P_T$ 越接近，模型表現越好。**

##### 數學表達
對於一個離散隨機變量 $X$，如果真實分佈是 $P_T(X)$，模型分佈是 $P_M(X)$，則交叉熵 $H(P_T, P_M)$ 定義為：

$$ H(P_T, P_M) = -\sum_{x} P_T(x) \log P_M(x) $$

在語言建模的語境下，我們通常無法直接獲得真實分佈 $P_T(x)$。但在測試語料中，我們可以將每個詞的出現視為從真實分佈中採樣，因此交叉熵可以近似為測試語料上平均負對數似然 (Average Negative Log-Likelihood)：

$$ H(W) = -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, \dots, w_{i-1}) $$

這正好是困惑度公式中指數函數內部的值。這清楚地顯示了困惑度與交叉熵之間的緊密關係。

#### 1.4 每字元/詞位元數 (Bits Per Character/Word, BPC/BPW)

##### 定義/核心觀念
每字元/詞位元數是語言模型評估的另一種形式，它衡量了模型平均需要多少位元來編碼或「預測」測試語料中的每一個字元或詞。這個指標直接與壓縮效率掛鉤：**BPC/BPW 值越低，表示模型對文本的壓縮能力越強，預測越精確，模型表現越好。**

##### 數學表達
BPC (或 BPW) 的計算方式與交叉熵非常相似，但要求對數的底數為 2：

$$ BPC(W) = -\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i | w_1, \dots, w_{i-1}) $$

如果以詞為單位，則稱為 BPW。如果以字元為單位，則需要將詞序列轉換為字元序列，並計算每個字元的條件機率。

-----

### 2. 典型例子與轉換/推導

本節將透過一個簡單的例子來演示這些指標的計算，並闡明它們之間的關係。

#### 2.1 困惑度與交叉熵的關係推導

從交叉熵的語言模型公式開始：
$$ H(W) = -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, \dots, w_{i-1}) $$

我們將這個值稱為平均負對數似然 (Average Negative Log-Likelihood, ANLL)。
從困惑度的對數形式公式來看：
$$ PPL(W) = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, \dots, w_{i-1}) \right) $$
很明顯，困惑度就是將交叉熵（使用自然對數）取指數：
$$ PPL(W) = \exp(H(W)) $$
或者說：
$$ \log PPL(W) = H(W) $$
這表示困惑度是交叉熵的指數變換。

#### 2.2 困惑度與每字元/詞位元數的關係推導

當交叉熵的對數底數為 2 時，它就是每字元/詞位元數 (BPC/BPW)：
$$ BPC(W) = -\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i | w_1, \dots, w_{i-1}) $$
我們知道對數的換底公式：$\log_b x = \frac{\log_k x}{\log_k b}$。
所以，如果我們的交叉熵 $H(W)$ 使用自然對數 $\ln$，我們可以將其轉換為 $\log_2$：
$$ H(W) = -\frac{1}{N} \sum_{i=1}^{N} \ln P(w_i | w_1, \dots, w_{i-1}) $$
$$ BPC(W) = H(W) \cdot \frac{1}{\ln 2} $$
因此，
$$ BPC(W) = \frac{1}{N} \sum_{i=1}^{N} -\log_2 P(w_i | w_1, \dots, w_{i-1}) $$
由於 $PPL(W) = \exp(H(W))$，且 $\exp(x) = e^x$：
$$ PPL(W) = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \ln P(w_i | w_1, \dots, w_{i-1}) \right) $$
我們可以將 $H(W)$ 替換為 $BPC(W)$ 的表示：
$$ H(W) = BPC(W) \cdot \ln 2 $$
代回 PPL 公式：
$$ PPL(W) = \exp(BPC(W) \cdot \ln 2) $$
$$ PPL(W) = (e^{\ln 2})^{BPC(W)} $$
$$ PPL(W) = 2^{BPC(W)} $$
這個關係式表明，**困惑度是 2 的 BPC 次方**。這再次確認了它們之間緊密的數學聯繫，它們本質上衡量的是同樣的模型預測能力，只是呈現的形式不同。

#### 2.3 範例：計算 PPL, CE, BPC

假設我們有一個簡單的二元語言模型 (bigram model)，並在測試語料 $W = (\text{我, 愛, 台灣})$ 上進行評估。假設模型給出的條件機率如下：

*   $P(\text{我} | \text{<s>}) = 0.5$ (<s> 表示句子開頭)
*   $P(\text{愛} | \text{我}) = 0.8$
*   $P(\text{台灣} | \text{愛}) = 0.9$

**步驟 1: 計算句子的聯合機率**
$P(W) = P(\text{我} | \text{<s>}) \times P(\text{愛} | \text{我}) \times P(\text{台灣} | \text{愛})$
$P(W) = 0.5 \times 0.8 \times 0.9 = 0.36$

**步驟 2: 計算交叉熵 (使用自然對數)**
首先計算每個詞的負對數機率：
*   $-\ln P(\text{我} | \text{<s>}) = -\ln(0.5) \approx 0.693$
*   $-\ln P(\text{愛} | \text{我}) = -\ln(0.8) \approx 0.223$
*   $-\ln P(\text{台灣} | \text{愛}) = -\ln(0.9) \approx 0.105$

句子長度 $N=3$。
$H(W) = -\frac{1}{N} \sum_{i=1}^{N} \ln P(w_i | w_{i-1})$
$H(W) = \frac{1}{3} (0.693 + 0.223 + 0.105) = \frac{1}{3} (1.021) \approx 0.340$

所以，**交叉熵 CE $\approx 0.340$**。

**步驟 3: 計算困惑度 (PPL)**
$PPL(W) = \exp(H(W))$
$PPL(W) = \exp(0.340) \approx 1.405$

所以，**困惑度 PPL $\approx 1.405$**。

**步驟 4: 計算每詞位元數 (BPW)**
首先計算每個詞的負 $\log_2$ 機率：
*   $-\log_2 P(\text{我} | \text{<s>}) = -\log_2(0.5) = -(-1) = 1$
*   $-\log_2 P(\text{愛} | \text{我}) = -\log_2(0.8) \approx -(-0.322) = 0.322$
*   $-\log_2 P(\text{台灣} | \text{愛}) = -\log_2(0.9) \approx -(-0.152) = 0.152$

$BPW(W) = -\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i | w_{i-1})$
$BPW(W) = \frac{1}{3} (1 + 0.322 + 0.152) = \frac{1}{3} (1.474) \approx 0.491$

所以，**每詞位元數 BPW $\approx 0.491$**。

**驗證關係：**
*   $PPL \approx 1.405$
*   $2^{BPW} = 2^{0.491} \approx 1.401$ (略有誤差是因四捨五入)
關係 $PPL = 2^{BPW}$ 成立。

-----

### 3. 與相鄰概念的關聯

#### 3.1 內部評估 (Intrinsic Evaluation) 與外部評估 (Extrinsic Evaluation)

本章介紹的 PPL、CE 和 BPC 都屬於**內部評估 (Intrinsic Evaluation)** 指標。它們直接衡量模型在預測文本序列方面的內部能力，不依賴於模型在特定下游任務中的表現。

**外部評估 (Extrinsic Evaluation)** 則是在一個真實的應用場景中評估語言模型，例如將其整合到機器翻譯、語音辨識或問答系統中，然後評估最終任務的效能（如 BLEU 分數、詞錯誤率等）。

*   **關聯性：** 通常情況下，內部評估指標較好的模型在外部評估中也會表現較好。然而，這種關係並非絕對。有時，一個在 PPL 上略遜一籌的模型可能在特定外部任務中表現更優異，因為 PPL 不直接衡量語義或任務相關的特定能力。

#### 3.2 資訊理論：熵 (Entropy) 與相對熵 (KL Divergence)

*   **熵 (Entropy) $H(P)$：** 衡量一個機率分佈 $P$ 的不確定性或資訊量。對於真實語言分佈 $P_T$，其熵 $H(P_T)$ 表示生成該語言所需的最小平均位元數。
    $$ H(P_T) = -\sum_{x} P_T(x) \log P_T(x) $$
*   **相對熵 (Kullback-Leibler Divergence, KL Divergence) $D_{KL}(P_T || P_M)$：** 衡量當我們使用模型 $P_M$ 來近似真實分佈 $P_T$ 時所損失的資訊量。
    $$ D_{KL}(P_T || P_M) = \sum_{x} P_T(x) \log \frac{P_T(x)}{P_M(x)} $$
    KL 散度總是 $\ge 0$，當且僅當 $P_T = P_M$ 時為 0。

*   **與交叉熵的關聯：** 交叉熵可以分解為熵和相對熵之和：
    $$ H(P_T, P_M) = H(P_T) + D_{KL}(P_T || P_M) $$
    在語言建模中，由於真實分佈 $P_T$ 是固定的（測試語料），其熵 $H(P_T)$ 也是一個常數。因此，最小化交叉熵 $H(P_T, P_M)$ 實際上等價於最小化模型分佈 $P_M$ 與真實分佈 $P_T$ 之間的相對熵 $D_{KL}(P_T || P_M)$。這也解釋了為什麼交叉熵是評估模型擬合真實分佈能力的好指標。

-----

### 4. 進階內容

#### 4.1 處理未知詞 (Out-Of-Vocabulary, OOV)

在計算困惑度時，如果測試語料中出現了訓練時未見過的詞 (OOV)，且模型無法為其分配機率，會導致機率為 0，進而使 $\log 0$ 出現無限大，導致困惑度為無限大。為了解決這個問題，常見的方法有：

1.  **未知詞 (UNK) 標記：** 在訓練前，將訓練語料中出現頻率低的詞替換為特殊的 `<UNK>` 標記。訓練後，模型會學習到 `<UNK>` 標記的機率分佈。在評估時，將測試語料中的 OOV 詞也替換為 `<UNK>` 標記。
2.  **平滑化 (Smoothing)：** 對機率分佈進行平滑處理，確保所有可能的詞，即使是未見過的詞，也能分配到一個微小的非零機率。常見的平滑技術有加一平滑 (Add-1 Smoothing) 或 Kneser-Ney Smoothing。

#### 4.2 字元級與詞級困惑度

*   **詞級困惑度 (Word-level Perplexity)：** 大多數時候我們討論的困惑度是指詞級困惑度，即 $N$ 是測試語料中的詞數。
*   **字元級困惑度 (Character-level Perplexity)：** 在某些情況下（例如處理字元級語言模型或處理 OOV 問題），我們會計算字元級困惑度。此時，$N$ 是測試語料中的字元數，模型預測的是下一個字元。字元級困惑度通常會遠低於詞級困惑度，因為字元集大小通常遠小於詞彙集大小，且字元序列的預測難度相對較低。

*   **比較：** 直接比較字元級和詞級困惑度是沒有意義的。即使是同一語言，字元級模型的 PPL 也不能與詞級模型的 PPL 進行比較。

-----

### 5. 常見錯誤與澄清

1.  **困惑度值越低越好：** 這是最重要的原則。新手可能會混淆，以為像準確度一樣，值越高越好。
2.  **困惑度不能跨語料庫或跨分詞方式直接比較：**
    *   **不同語料庫：** 即使是同一模型，在不同風格、主題或大小的測試語料庫上，其困惑度值會差異很大。例如，在新聞語料上訓練的模型，若在古詩詞語料上測試，其困惑度會非常高。
    *   **不同分詞方式：** 如果兩個模型使用了不同的分詞 (tokenization) 策略（例如，一個使用字元級，一個使用詞級；或詞級分詞詞彙表不同），它們的困惑度值就不能直接比較。因為 $N$（序列長度）會不同，而且 $P(w_i | \dots)$ 的定義也不同。例如，英文的 "don't" 可以被視為一個詞，也可以被分詞為 "do" 和 "n't" 兩個詞。這會直接影響 $N$ 和每個詞的機率。
3.  **對數底數的重要性：** 困惑度的定義通常使用自然對數 (ln 或 $\log_e$)，而 BPC 的定義使用以 2 為底的對數 ($\log_2$)。在論文中或實作時務必明確對數底數，以免造成計算錯誤。如果不確定，最好的做法是直接報告交叉熵 (使用自然對數)，然後根據需要再轉換為 PPL 或 BPC。
4.  **困惑度高不等於模型無用：** 雖然低困惑度通常代表更好的模型，但它只是一個內部指標。一個 PPL 較高的模型可能在某些特定任務（如生成風格化文本）中仍有其獨特的價值。PPL 不能完全捕捉語言的流暢性、連貫性或語義正確性。

-----

### 6. 小練習（附詳解）

#### 小練習 1: 困惑度計算

假設我們有一個語言模型，用於評估句子 "天空 是 藍色 的" (假設每個詞之間的分隔符不算一個詞)。
模型給出的條件機率如下（假設句子開頭為 `<s>`）：

*   $P(\text{天空} | \text{<s>}) = 0.05$
*   $P(\text{是} | \text{天空}) = 0.90$
*   $P(\text{藍色} | \text{是}) = 0.80$
*   $P(\text{的} | \text{藍色}) = 0.95$

請計算此句子在此模型下的困惑度 (PPL)。(請使用自然對數 ln，並保留小數點後三位)

**詳解:**

1.  **確定句子長度 $N$：** 句子 "天空 是 藍色 的" 包含 4 個詞，所以 $N=4$。

2.  **計算每個詞的負自然對數機率：**
    *   $-\ln P(\text{天空} | \text{<s>}) = -\ln(0.05) \approx -(-2.996) = 2.996$
    *   $-\ln P(\text{是} | \text{天空}) = -\ln(0.90) \approx -(-0.105) = 0.105$
    *   $-\ln P(\text{藍色} | \text{是}) = -\ln(0.80) \approx -(-0.223) = 0.223$
    *   $-\ln P(\text{的} | \text{藍色}) = -\ln(0.95) \approx -(-0.051) = 0.051$

3.  **計算平均負對數似然 (即交叉熵 $H(W)$)：**
    $H(W) = \frac{1}{N} \sum_{i=1}^{N} -\ln P(w_i | w_1, \dots, w_{i-1})$
    $H(W) = \frac{1}{4} (2.996 + 0.105 + 0.223 + 0.051)$
    $H(W) = \frac{1}{4} (3.375)$
    $H(W) \approx 0.844$

4.  **計算困惑度 (PPL)：**
    $PPL(W) = \exp(H(W))$
    $PPL(W) = \exp(0.844)$
    $PPL(W) \approx 2.326$

因此，該句子在此模型下的困惑度約為 **2.326**。

#### 小練習 2: PPL 與 BPC 轉換

如果一個語言模型在某測試語料上的困惑度 (PPL) 為 64。
請計算該模型在相同測試語料上的每詞位元數 (BPW)。

**詳解:**

1.  **回顧 PPL 與 BPC 的關係：**
    我們知道 PPL 和 BPC 之間有以下關係：$PPL = 2^{BPC}$。

2.  **代入已知值：**
    題目給定 $PPL = 64$。
    所以，$64 = 2^{BPW}$。

3.  **求解 BPW：**
    要解出 $BPW$，我們可以將等式兩邊取以 2 為底的對數：
    $\log_2(64) = \log_2(2^{BPW})$
    $\log_2(64) = BPW$

    我們知道 $2^6 = 64$，所以 $\log_2(64) = 6$。

    $BPW = 6$

因此，該模型在相同測試語料上的每詞位元數為 **6**。這表示模型平均需要 6 個位元來編碼或預測測試語料中的每個詞。

-----

### 7. 延伸閱讀/參考

*   **Jurafsky, D., & Martin, J. H. (2009). *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition* (2nd ed.). Pearson Prentice Hall.** (尤其是第 4 章：N-gram Language Models，或更新版相關章節)
    *   這是 NLP 領域的經典教材，對語言模型和評估指標有詳盡的介紹。
*   **Shannon, C. E. (1948). A Mathematical Theory of Communication. *The Bell System Technical Journal, 27*(3), 379–423.**
    *   資訊理論的奠基性論文，交叉熵和熵的理論基礎來源。
*   **Understanding Perplexity in Language Models:** 許多線上部落格和教學文章對 PPL 有更直觀的解釋和圖解，例如 Towards Data Science 上的相關文章。
    *   [例如這篇](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94) (請注意，這僅為範例，實際使用時需確認內容品質與時效性)
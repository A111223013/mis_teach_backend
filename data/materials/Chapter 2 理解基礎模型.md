# Chapter 2：理解基礎模型

-----

### 2.1 核心概念：什麼是基礎模型？

#### 2.1.1 定義與核心觀念

**定義：** 基礎模型 (Foundation Models) 是指在**大量、廣泛、多樣化的數據**上進行**預訓練 (Pre-training)** 的巨型人工智慧模型。這些模型具備**通用性**，能夠透過適應性調整 (adaptation) 或微調 (fine-tuning) 來執行多種不同的下游任務，而無需從零開始訓練。

**核心觀念：**

*   **規模化 (Scale)：** 基礎模型最顯著的特徵是其巨大的規模，體現在：
    *   **數據量：** 訓練數據通常達到數百 GB 甚至 PB 級，涵蓋文本、圖像、音頻等多種模態。
    *   **參數量：** 模型擁有數十億、數百億乃至數兆的參數，遠超傳統機器學習模型。
    *   **計算資源：** 訓練和部署需要龐大的計算能力（GPU 時數）。
*   **預訓練 (Pre-training)：** 模型在無標籤或自監督的通用數據上進行訓練，學習廣泛的知識、模式和表示。這個階段是基礎模型具備通用能力的關鍵。
*   **通用性 (Generality)：** 與傳統的「任務專用」模型不同，基礎模型旨在學習一個通用的表示空間，使其能夠適應並執行多種任務，而不僅僅是單一任務。
*   **湧現能力 (Emergent Abilities)：** 隨著模型規模（數據量、參數量）達到一定閾值，會突然出現一些在小規模模型中不存在的、意料之外的新能力。這些能力往往是非線性的提升，例如複雜的推理、程式碼生成、多語言理解等。
*   **適應性 (Adaptation)：** 預訓練後的基礎模型並非直接用於特定任務，而是透過多種方式進行適應，包括：
    *   **微調 (Fine-tuning)：** 在少量特定任務的標籤數據上繼續訓練模型。
    *   **提示工程 (Prompt Engineering)：** 設計精巧的輸入提示 (prompts) 來引導模型完成任務，而無需修改模型權重。
    *   **上下文學習 (In-context Learning)：** 在提示中提供少量範例，讓模型學習任務模式。

#### 2.1.2 起源與里程碑

「基礎模型」這個術語由史丹佛大學人工智慧研究院 (Stanford University's Center for Research on Foundation Models, CRFM) 於 2021 年在其里程碑式論文《On the Opportunities and Risks of Foundation Models》中提出，系統性地總結並定義了這類模型。然而，其底層技術（如 Transformer 架構、預訓練-微調範式、遷移學習）已發展多年，GPT-3、BERT 等模型的成功應用則為其奠定了實踐基礎。

-----

### 2.2 典型基礎模型範例與其運作核心

基礎模型的類型不斷擴展，但目前最為人所知的當屬大型語言模型和擴散模型。

#### 2.2.1 大型語言模型 (Large Language Models, LLMs)

*   **核心觀念：** LLMs 是一類專門處理和生成人類語言的基礎模型。它們通常基於 Transformer 架構，透過預測下一個詞元 (token) 進行自我監督學習，從而學習到語言的語法、語義、事實知識以及一定程度的推理能力。
*   **典型例子：** OpenAI 的 GPT 系列 (GPT-3, GPT-4)、Google 的 PaLM、Meta 的 LLaMA、Anthropic 的 Claude 等。
*   **運作核心 (簡化推導)：**
    1.  **詞元化 (Tokenization)：** 輸入的文本會被分割成一系列的詞元（例如單詞、詞根、標點符號）。
    2.  **嵌入 (Embedding)：** 每個詞元被轉換成一個高維向量（詞元嵌入），捕捉其語義信息。
    3.  **Transformer 架構：** 這些嵌入向量被送入堆疊的 Transformer 層。Transformer 的核心是**自注意力機制 (Self-Attention)**，它允許模型在處理每個詞元時，考慮輸入序列中所有其他詞元的上下文信息，從而捕捉長距離依賴關係。
        *   **純解碼器 (Decoder-only)：** 最常見的 LLM 架構（如 GPT），它在生成時只能「看」到前面的詞元，以自迴歸 (auto-regressive) 的方式預測下一個詞元。
        *   **純編碼器 (Encoder-only)：** 如 BERT，用於理解雙向上下文，適合分類、問答等任務。
        *   **編碼器-解碼器 (Encoder-Decoder)：** 如 T5，編碼器理解輸入，解碼器生成輸出。
    4.  **預訓練目標 (自監督學習)：**
        *   **因果語言模型 (Causal Language Modeling, CLM)：** 預測序列中的下一個詞元。
        *   **掩碼語言模型 (Masked Language Modeling, MLM)：** 隨機遮蓋輸入序列中的某些詞元，讓模型預測被遮蓋的詞元。
    5.  **微調/提示工程：** 預訓練後，模型透過少量特定數據的微調，或者透過精心設計的文本提示 (prompts) 來引導其完成摘要、翻譯、程式碼生成等特定任務。

#### 2.2.2 擴散模型 (Diffusion Models)

*   **核心觀念：** 擴散模型是一種生成模型，擅長生成高質量、高真實感的圖像、音頻或其他數據。其基本思想是透過學習逐步**去噪 (denoising)** 的過程，從隨機雜訊中恢復出有意義的數據。
*   **典型例子：** DALL-E 2/3, Stable Diffusion, Midjourney, Imagen 等。
*   **運作核心 (簡化推導)：**
    1.  **前向過程 (Forward Process - 加噪)：** 從一個真實數據點（例如一張圖像 $x_0$）開始，逐步向其添加高斯雜訊，經過 $T$ 個時間步，最終將其完全轉化為隨機雜訊 $x_T$。這個過程是固定的，無需學習。
    2.  **反向過程 (Reverse Process - 去噪)：** 這是模型的訓練目標。訓練一個神經網路（通常是 U-Net 結構）來學習如何從帶有雜訊的數據 $x_t$ 預測並去除雜訊，從而一步步逆轉前向過程，將雜訊 $x_T$ 逐漸還原成清晰的數據 $x_0$。
    3.  **條件生成：** 許多擴散模型可以接受條件輸入，例如文本提示 (text prompts) 或其他圖像，來引導生成過程。這使得模型能夠根據特定的描述生成相關的內容。這種條件通常透過交叉注意力機制 (cross-attention) 將條件信息融入去噪網路。

#### 2.2.3 多模態基礎模型 (Multimodal Foundation Models)

*   **核心觀念：** 這類模型旨在處理和理解來自多種模態（如文本、圖像、音頻、影片）的資訊。它們通常透過將不同模態的輸入映射到一個共同的嵌入空間 (embedding space) 來實現跨模態的理解和生成。
*   **典型例子：** CLIP (連接文本與圖像), DALL-E (文本到圖像生成), Google 的 Gemini, OpenAI 的 GPT-4V (理解圖像和文本)。
*   **與相鄰概念的關聯：** 多模態基礎模型結合了 LLM 和視覺模型的優勢，是未來 AGI 發展的重要方向之一。

-----

### 2.3 與相鄰概念的關聯

#### 2.3.1 與傳統機器學習模型的區別

| 特徵             | 傳統機器學習模型                        | 基礎模型                                       |
| :--------------- | :-------------------------------------- | :--------------------------------------------- |
| **訓練數據**     | 小而特定，通常有標籤                    | 巨大、廣泛、多樣，多為無標籤                  |
| **模型規模**     | 數千至數百萬參數                        | 數十億至數兆參數                               |
| **訓練方式**     | 通常從頭開始訓練，針對特定任務        | 大規模預訓練，再進行適應性調整或微調        |
| **任務範圍**     | 單一任務，模型專門為其設計和優化      | 多任務適應性，透過不同方式可解決多種下游任務 |
| **通用性/泛化**  | 泛化能力受限於訓練數據與任務領域      | 具備強大的泛化能力，能處理未見過的情境       |
| **新能力**       | 性能隨規模線性提升                      | 展現出湧現能力 (Emergent Abilities)           |
| **開發成本**     | 相對較低                                | 訓練成本極高，主要由大型機構進行             |

#### 2.3.2 與遷移學習 (Transfer Learning) 的關聯

基礎模型是遷移學習最成功的實踐之一。遷移學習的核心思想是將在一個任務上學到的知識應用到另一個相關任務上，以減少新任務所需的數據量和訓練時間。

*   **預訓練階段：** 基礎模型在海量的通用數據上進行預訓練，學習到豐富的通用知識和底層表示（例如語言結構、世界知識、視覺特徵）。這個階段可以看作是知識的「擷取」。
*   **適應性階段：** 預訓練後的基礎模型被視為一個強大的「特徵提取器」或「知識庫」。開發者可以透過：
    *   **微調 (Fine-tuning)：** 在少量特定任務數據上調整模型參數，將通用知識遷移到特定任務。
    *   **零樣本學習 (Zero-shot Learning)：** 直接使用預訓練模型的通用能力，無需任何額外訓練數據，即可執行新任務。
    *   **少樣本學習 (Few-shot Learning)：** 僅提供少量範例，模型就能迅速理解並執行新任務。
    這證明了基礎模型能夠有效地將其預訓練所學的通用知識「遷移」到多個下游任務中。

#### 2.3.3 與人工通用智慧 (AGI) 的展望

基礎模型的發展被許多人視為通往人工通用智慧 (Artificial General Intelligence, AGI) 的重要一步。其展現出的多任務處理能力、湧現能力以及在複雜任務上的表現，使得研究者開始探索其能否具備類人智慧的潛力。

*   **潛力：** 基礎模型在理解和生成多模態數據方面的能力，以及在未見過任務上展現出的泛化能力，暗示了某種形式的通用學習和推理。
*   **挑戰：** 儘管有這些潛力，目前的基礎模型仍面臨諸多挑戰，如缺乏真正的因果推理能力、可解釋性差、容易產生「幻覺」、對偏見敏感以及巨大的計算成本等。它們尚不具備意識、自我意識或真正的理解能力。

-----

### 2.4 進階內容：規模化、湧現能力與倫理考量

#### 2.4.1 規模化 (Scale) 的重要性

規模化是基礎模型成功的基石。模型規模的持續擴大（包括數據量和參數量）被觀察到與其性能的顯著提升、泛化能力的增強以及湧現能力的出現密切相關。

*   **數據規模：** 模型需要處理兆字節級的文本數據、數十億張圖片或其他模態數據。這使得模型能夠學習到更全面、更細緻的世界知識和數據模式。
*   **模型參數：** 從數百萬到數兆的參數，提供了巨大的容量來儲存和處理這些學習到的知識。更多的參數通常意味著模型能捕捉更複雜的關係和模式。
*   **計算資源：** 訓練一個大型基礎模型需要數千個 GPU 並行運算數週到數月，耗費數百萬美元的電力和計算資源。這是目前少數大型科技公司才能承擔的投資。

#### 2.4.2 湧現能力 (Emergent Abilities)

*   **定義：** 湧現能力是指在小規模模型中不存在，但隨著模型規模（數據量、參數量）達到一定閾值後，突然出現的、意料之外的能力。這些能力不是簡單的線性提升，而是模型行為的質變。
*   **重要性：** 湧現能力是基礎模型最令人興奮的特性之一。它表明，僅僅透過擴大模型規模和訓練數據，模型能夠自發地學習到比其預訓練任務本身更複雜、更高級的能力。
*   **典型例子：**
    *   **複雜推理：** 如多步驟邏輯推理、數學問題解答。
    *   **程式碼生成與理解：** 從自然語言描述生成程式碼，或解釋程式碼功能。
    *   **多語言處理：** 在未經明確訓練的語言之間進行翻譯或理解。
    *   **指令遵循：** 更好地理解和執行複雜、多樣化的指令。

#### 2.4.3 倫理與社會衝擊

基礎模型的巨大潛力伴隨著深刻的倫理和社會挑戰，必須加以重視和管理。

*   **偏見與歧視：** 訓練數據往往反映了人類社會的偏見。基礎模型可能會繼承、放大這些偏見，導致歧視性或不公平的輸出。
*   **錯誤資訊 (Misinformation) 與虛假資訊 (Disinformation)：** 模型能夠生成高度真實但完全捏造的內容（文本、圖像、音頻），可能被用於散播錯誤信息、操縱輿論或進行詐騙。
*   **知識產權與版權：** 模型在訓練過程中使用了大量的現有內容。生成內容的版權歸屬、對原作者權利的侵犯等問題尚未有明確法律規範。
*   **勞動市場影響：** 基礎模型在自動化許多認知任務方面表現出色，可能對特定行業的就業結構造成衝擊，同時也可能創造新的工作機會。
*   **能源消耗與環境影響：** 訓練和運行巨型模型需要消耗大量電力，產生碳排放，對環境造成壓力。
*   **安全與濫用：** 模型的強大能力可能被惡意使用者利用，例如生成釣魚郵件、網路攻擊程式碼或虛假新聞。

-----

### 2.5 常見錯誤與澄清

#### 2.5.1 錯誤觀念一：基礎模型擁有「理解」能力或意識。

*   **澄清：** 目前的基礎模型是極為強大的模式識別器、統計關聯器和語言生成器。它們通過分析海量數據中的統計規律來學習詞元之間的關係，並基於這些規律生成看似連貫和智能的輸出。然而，它們並沒有內在的意識、自我意識、情感或人類意義上的「理解」。它們不具備主觀經驗或認知狀態，其行為是基於訓練數據中的模式和預測目標。

#### 2.5.2 錯誤觀念二：基礎模型總能提供準確無誤的資訊。

*   **澄清：** 基礎模型（特別是 LLMs）可能會出現「幻覺 (hallucinations)」現象，即生成看似合理、流暢但事實上完全錯誤、捏造或無根據的資訊。這是因為模型在生成過程中，有時會傾向於「填補」知識空白，或者基於其學到的模式錯誤地組合資訊。此外，模型的輸出品質高度依賴其訓練數據的品質和廣度，以及輸入提示的設計。不應盲目相信其所有輸出。

#### 2.5.3 錯誤觀念三：微調後的模型就完全適應了特定任務，且沒有偏見。

*   **澄清：** 微調確實能讓模型更好地適應特定任務，但它並非萬能藥。
    *   **限制：** 微調後的模型仍可能受限於其預訓練階段學到的通用知識。如果預訓練知識與特定任務需求不匹配，或者微調數據量不足/質量不佳，模型表現仍可能不理想。
    *   **偏見：** 微調數據本身也可能引入新的偏見，或者無法完全消除預訓練階段繼承的偏見。因此，即使經過微調，模型仍可能存在偏見，並需要在部署前進行嚴格的評估和審查。

-----

### 2.6 小練習

#### 練習一：基礎模型特性辨析

請判斷以下描述是否符合「基礎模型」的核心特性。若符合，請簡述原因；若不符合，請說明基礎模型的相關特性。

1.  一個針對手寫數字識別（MNIST）數據集從零開始訓練，並僅用於識別數字的卷積神經網絡。
2.  一個在數百 GB 文本數據上預訓練，之後能透過微調或提示工程執行文本摘要、問答、翻譯等多種任務的模型。
3.  一個只能理解圖像，但無法處理文本資訊的模型。
4.  一個隨著其參數規模達到數十億後，突然展現出複雜程式碼生成能力的模型。

**詳解：**

1.  **不符合。** 基礎模型強調在**廣泛且大量的數據**上進行預訓練，並具備**通用性**，能適應多種下游任務。此描述的模型是任務導向、數據集小且僅限於單一特定任務的傳統機器學習模型。
2.  **符合。** 這完全符合基礎模型的核心定義：**大規模預訓練**（數百 GB 文本數據）、**通用性**（能執行文本摘要、問答、翻譯等多種任務）、以及透過**適應性調整**（微調或提示工程）來運用。
3.  **不符合。** 雖然單模態的基礎模型（如早期的視覺基礎模型）只處理一種模態，但其「基礎」特性在於其**通用學習能力**和**大規模性**，使其在該模態內能適應多種任務。此描述過於籠統，未體現出「基礎」或「通用」的潛力。例如，一個在數百萬張圖像上預訓練，能識別多種物體、生成圖像描述，並能透過微調適應新視覺任務的模型，就符合基礎模型在視覺領域的定義。僅說明「只能理解圖像」並不足以說明其是否具備基礎模型的通用性和大規模特性。
4.  **符合。** 這描述了基礎模型的關鍵特性之一——**湧現能力 (Emergent Abilities)**。當模型規模（參數量）達到一定程度時，會出現一些在小規模模型中不存在的、質變的能力。複雜程式碼生成就是一個典型的湧現能力。

-----

#### 練習二：理解 LLM 的預訓練任務

如果你要訓練一個大型語言模型 (LLM)，最常見的兩種預訓練任務是「因果語言模型 (Causal Language Modeling, CLM)」和「掩碼語言模型 (Masked Language Modeling, MLM)」。

請簡述這兩種任務的核心目標，並說明它們各自適合什麼樣的 LLM 架構（例如：純解碼器、純編碼器、編碼器-解碼器）。

**詳解：**

1.  **因果語言模型 (Causal Language Modeling, CLM)**
    *   **核心目標：** 模型被訓練來預測序列中的下一個詞元 (token)，基於其前面所有的詞元。在預測時，模型只能「看到」或依賴其左側（過去）的上下文資訊。這種單向預測使其非常適合生成連貫的文本。
    *   **適合架構：** 主要用於**純解碼器 (Decoder-only)** 架構，例如 GPT 系列。這種架構天生適合生成任務，因為它以自迴歸 (auto-regressive) 的方式一次生成一個詞元，模仿人類寫作或說話的過程。

2.  **掩碼語言模型 (Masked Language Modeling, MLM)**
    *   **核心目標：** 模型被訓練來預測序列中被「掩碼」或遮蓋住的詞元，可以基於該詞元之前和之後的雙向上下文資訊。模型需要理解整個句子的語義才能準確填補被遮蓋的部分。
    *   **適合架構：** 主要用於**純編碼器 (Encoder-only)** 架構（例如 BERT 系列）或**編碼器-解碼器 (Encoder-Decoder)** 架構（例如 T5）。純編碼器架構擅長理解型任務（如情感分析、文本分類、問答），因為它能捕捉完整的雙向上下文。編碼器-解碼器架構則能同時用於複雜的理解和生成任務。

-----

### 2.7 延伸閱讀

*   **論文：** *On the Opportunities and Risks of Foundation Models* by Stanford HAI (2021). 這是基礎模型的奠基性論文，深入探討了其定義、特性、挑戰與潛力。
    *   [連結到論文](https://crfm.stanford.edu/2021/08/FoundationModels.html)
*   **書籍：** *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 雖然不是專門針對基礎模型，但它提供了理解神經網絡、表示學習和深度學習所有核心技術的堅實基礎。
*   **開放教材：** Hugging Face Transformers 課程。這是一個非常實用的線上課程，深入淺出地介紹了 Transformer 架構、LLMs、擴散模型等概念，並提供實作範例。
    *   [連結到課程](https://huggingface.co/learn/nlp-course/chapter1/1)
*   **機構部落格：** 關注領先的 AI 實驗室（如 OpenAI、Google AI、Meta AI、Anthropic）的官方部落格，它們會定期發布最新的基礎模型研究進展、技術解析和應用案例。
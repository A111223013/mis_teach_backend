# 演算法定義與程式效率分析

演算法是電腦科學的基石，它定義了如何解決問題的步驟。而程式效率分析則是評估這些解決方案優劣的關鍵，確保程式不僅能正確運行，還能以最優化的方式利用資源。本章節將深入探討演算法的定義、其與程式的關係，並詳細介紹程式效率分析的原理與常用工具。

-----

### 1. 核心概念：演算法是什麼？

#### 1.1 演算法的定義

*   **核心觀念：** 演算法 (Algorithm) 是一組明確定義、有限且有序的指令集合，用於解決特定問題或執行特定任務。它是一個抽象的邏輯序列，描述了從輸入到輸出的轉換過程。

*   **演算法的五大基本特性：**
    1.  **輸入 (Input)：** 演算法接受零個或多個外部輸入。這些輸入是演算法處理的原始資料。
    2.  **輸出 (Output)：** 演算法產生一個或多個輸出。這些輸出是問題解決後的結果。
    3.  **明確性 (Definiteness)：** 演算法的每一步驟都必須精確、清晰且無歧義。無論誰來執行，都應得到相同的理解和結果。
    4.  **有限性 (Finiteness)：** 演算法必須在有限的步驟內終止。無限循環的過程不能稱為演算法。
    5.  **有效性 (Effectiveness)：** 演算法的每一步驟都必須是基本且可執行的，理論上可以用筆和紙在有限時間內完成。

*   **例子：**
    *   **煮泡麵的演算法：**
        1.  準備泡麵、碗、叉子和水。
        2.  打開泡麵包裝，取出麵塊和調料。
        3.  將麵塊和調料放入碗中。
        4.  燒開水。
        5.  將沸水倒入碗中，淹沒麵塊。
        6.  蓋上碗蓋，等待三分鐘。
        7.  打開碗蓋，攪拌。
        8.  享用。
    這個例子具備了輸入（泡麵、水）、輸出（煮好的泡麵）、明確的步驟、有限的步驟數以及每個步驟都可執行。

#### 1.2 演算法與程式的關係

*   **核心觀念：** 演算法是解決問題的「思想」或「藍圖」，而程式 (Program) 則是演算法在特定程式語言（如 Python, Java, C++）中的「具體實現」。

*   **與相鄰概念的關聯：**
    *   一個好的演算法是高效程式的基礎。即使使用最快的硬體，如果底層演算法效率低下，程式性能也將受限。
    *   同一個演算法可以用不同的程式語言實現，它們雖然語法不同，但核心邏輯（演算法）是相同的。
    *   一個複雜的程式可能由多個不同的演算法協同工作組成。
    *   程式設計師的工作，通常是將抽象的演算法概念轉化為可執行、可測試、可維護的程式碼。

-----

### 2. 程式效率分析：為什麼與如何衡量？

#### 2.1 為什麼要分析程式效率？

*   **核心觀念：** 在計算機科學中，資源（時間和記憶體）是有限的。分析程式效率的目的是為了理解和預測演算法在不同輸入規模下的性能表現，從而選擇或設計出最佳的解決方案。

*   **分析效率的重要性：**
    *   **處理大規模資料：** 隨著數據量的爆炸式增長，高效能的演算法成為處理巨量資料的關鍵。
    *   **提升用戶體驗：** 響應迅速的應用程式能提供更好的用戶體驗，減少等待時間。
    *   **節省成本：** 伺服器運算時間和記憶體消耗直接關係到運營成本，優化效率可顯著降低開支。
    *   **適用於資源受限環境：** 在嵌入式系統、行動裝置等資源有限的設備上，高效演算法更是不可或缺。
    *   **選擇最佳演算法：** 當面對多種解決方案時，效率分析提供了客觀的依據來選擇最優的演算法。

#### 2.2 如何衡量程式效率？時間複雜度與空間複雜度

*   **核心觀念：** 為了客觀地衡量演算法的效率，我們通常使用兩種獨立於特定硬體、程式語言或編譯器的指標：時間複雜度 (Time Complexity) 和空間複雜度 (Space Complexity)。

    1.  **時間複雜度 (Time Complexity)：**
        *   衡量演算法執行所需時間與輸入大小 (通常用 $n$ 表示) 之間的關係。
        *   它計算的是演算法執行的「基本操作」次數，例如比較、賦值、算術運算等，而不是實際的秒數。
        *   目標是描述當輸入大小 $n$ 趨於無窮大時，操作次數的增長趨勢。

    2.  **空間複雜度 (Space Complexity)：**
        *   衡量演算法執行所需記憶體空間與輸入大小 $n$ 之間的關係。
        *   它計算的是演算法在執行過程中額外佔用的儲存空間（除了輸入本身），例如變數、資料結構等。
        *   同樣關注的是當 $n$ 趨於無窮大時，記憶體需求的增長趨勢。

#### 2.3 漸近符號 (Asymptotic Notations) - 大O符號 (Big-O Notation)

*   **核心觀念：** 漸近符號是一種數學工具，用於描述演算法在輸入大小趨近於無窮大時，其時間或空間需求增長的「上界」。其中，大O符號是最常用來表示演算法最差情況性能的。

*   **大O符號的定義：**
    對於一個表示演算法操作次數的函數 $f(n)$ 和一個簡化的比較函數 $g(n)$，如果存在正的常數 $c$ 和 $n_0$，使得對於所有足夠大的 $n \ge n_0$（即當輸入大小 $n$ 大於某個閾值 $n_0$ 時），都有 $0 \le f(n) \le c \cdot g(n)$，則稱 $f(n) = O(g(n))$。
    *   $f(n)$ 代表演算法實際執行步驟的數量（或記憶體單元數）。
    *   $g(n)$ 是我們尋找的函數，通常是最簡形式的最高階項（例如 $n^2$ 而不是 $2n^2 + 3n - 5$）。
    *   大O符號關注的是增長「趨勢」，忽略常數因子和低階項。

*   **常見的時間複雜度類別 (從最快到最慢)：**
    | 大O符號 | 名稱       | 描述                                                                                              | 範例                                              |
    | :------ | :--------- | :------------------------------------------------------------------------------------------------ | :------------------------------------------------ |
    | $O(1)$  | 常數時間   | 無論輸入大小為何，操作次數始終保持不變。                                                        | 陣列依索引存取、哈希表的插入/查詢 (平均)        |
    | $O(\log n)$ | 對數時間   | 每次操作將問題規模減半。即使 $n$ 很大，所需時間也增加得很慢。                                   | 二元搜尋、平衡二元搜尋樹的操作                    |
    | $O(n)$  | 線性時間   | 操作次數與輸入大小成正比。                                                                        | 線性搜尋、遍歷陣列/鏈結串列                       |
    | $O(n \log n)$ | 線性對數時間 | 結合了線性操作和對數操作，常見於高效能的排序演算法。                                            | 合併排序 (Merge Sort)、快速排序 (Quick Sort)     |
    | $O(n^2)$ | 平方時間   | 操作次數與輸入大小的平方成正比。通常涉及兩層嵌套迴圈。                                            | 氣泡排序 (Bubble Sort)、選擇排序 (Selection Sort) |
    | $O(2^n)$ | 指數時間   | 操作次數以指數級增長。對於稍大的 $n$ 就無法接受。                                                  | 暴力法解決旅行推銷員問題 (部分情況)             |
    | $O(n!)$ | 階乘時間   | 操作次數以階乘級增長。通常只適用於極小規模的問題。                                                  | 暴力法解決旅行推銷員問題 (全部情況)             |

*   **與相鄰概念的關聯：**
    *   除了大O符號 (Big-O, 上界)，還有其他漸近符號：
        *   **$\Omega$ 符號 (Big-Omega Notation)：** 表示演算法執行時間的「下界」。$f(n) = \Omega(g(n))$ 意味著 $f(n)$ 的增長速度至少和 $g(n)$ 一樣快。
        *   **$\Theta$ 符號 (Big-Theta Notation)：** 表示演算法執行時間的「緊密界限」。$f(n) = \Theta(g(n))$ 意味著 $f(n)$ 的增長速度與 $g(n)$ 相同，即 $f(n)$ 既是 $O(g(n))$ 也是 $\Omega(g(n))$。
    *   在大O符號中，我們通常關注最差情況 (Worst-Case) 的時間複雜度，因為它提供了性能的保證，確保演算法在任何情況下都不會超出這個界限。

-----

### 3. 典型例子與時間複雜度推導

本節將透過具體演算法例子，演示如何推導其時間複雜度。

#### 3.1 演算法例子：搜尋問題

*   **問題：** 給定一個整數陣列 `arr` 和一個目標值 `target`，判斷 `target` 是否存在於 `arr` 中，如果存在，返回其索引；否則，返回 -1。

#### 3.2 線性搜尋 (Linear Search)

*   **演算法步驟：**
    1.  從陣列的第一個元素開始，設定一個索引 `i`。
    2.  逐一檢查 `arr[i]` 是否等於 `target`。
    3.  如果 `arr[i]` 等於 `target`，則找到目標，返回 `i`。
    4.  如果遍歷完所有元素仍未找到 `target`，則返回 -1。

*   **程式碼範例 (Python-like Pseudocode):**
    ```python
    function linear_search(arr, target):
        n = len(arr) # 獲取陣列長度
        for i from 0 to n - 1: # 遍歷陣列中的每個元素
            if arr[i] == target: # 執行一次比較操作
                return i # 找到目標，立即返回
        return -1 # 遍歷結束仍未找到，返回 -1
    ```

*   **時間複雜度推導：**
    *   **基本操作：** 每次比較 `arr[i] == target`。
    *   **最佳情況 (Best Case)：** 如果目標值 `target` 恰好是陣列的第一個元素。只需執行 1 次比較。時間複雜度為 $O(1)$。
    *   **最差情況 (Worst Case)：** 如果目標值 `target` 是陣列的最後一個元素，或者目標值不存在於陣列中。需要遍歷整個陣列，執行 $n$ 次比較。時間複雜度為 $O(n)$。
    *   **平均情況 (Average Case)：** 假設目標值在陣列中每個位置出現的機率相等，且目標值存在於陣列中的機率為 0.5。平均來說，大約需要 $n/2$ 次比較。時間複雜度為 $O(n)$。
    *   **大O符號表示：** 由於大O符號通常描述最差情況下的性能，線性搜尋的時間複雜度為 $\mathbf{O(n)}$。

#### 3.3 氣泡排序 (Bubble Sort)

*   **演算法步驟 (簡述):**
    1.  重複走訪待排序的列表。
    2.  每次走訪時，比較相鄰的兩個元素。
    3.  如果它們的順序錯誤（例如，前一個比後一個大），則交換它們。
    4.  重複此過程，直到沒有任何交換發生，表示列表已經排序完成。在每次走訪後，最大的（或最小的）元素會「浮」到其最終位置。

*   **程式碼範例 (Python-like Pseudocode):**
    ```python
    function bubble_sort(arr):
        n = len(arr)
        # 外層迴圈控制排序的趟數
        # 每趟會將一個最大的元素移動到正確位置
        for i from 0 to n - 2:
            swapped = False # 標記是否發生交換，用於提前終止
            # 內層迴圈進行比較和交換
            # (n-1-i) 是因為最後 i 個元素已經在正確位置
            for j from 0 to n - 2 - i:
                if arr[j] > arr[j+1]: # 比較操作
                    swap(arr[j], arr[j+1]) # 交換操作
                    swapped = True
            if not swapped: # 如果一趟下來沒有交換，表示已經排序完成
                break
    ```

*   **時間複雜度推導：**
    *   **基本操作：** 元素之間的比較和交換。
    *   **外層迴圈：** 最多執行 $n-1$ 次。
    *   **內層迴圈：**
        *   當 $i=0$ 時，內層迴圈執行 $n-1$ 次。
        *   當 $i=1$ 時，內層迴圈執行 $n-2$ 次。
        *   ...
        *   當 $i=n-2$ 時，內層迴圈執行 $1$ 次。
    *   **總比較次數 (最差情況)：** 約為 $(n-1) + (n-2) + \dots + 1 = \frac{n(n-1)}{2}$ 次。
    *   這是一個二次多項式，其最高階項為 $n^2$。
    *   **大O符號表示：** 氣泡排序的時間複雜度為 $\mathbf{O(n^2)}$。

#### 3.4 二元搜尋 (Binary Search)

*   **前提：** 執行二元搜尋的陣列**必須是已排序的**。

*   **演算法步驟：**
    1.  確定搜尋範圍的起始 (low) 和結束 (high) 索引。
    2.  計算中間元素 `mid = (low + high) // 2`。
    3.  將 `arr[mid]` 與 `target` 進行比較：
        *   如果 `arr[mid] == target`，則找到目標，返回 `mid`。
        *   如果 `target < arr[mid]`，目標在左半部分，將 `high` 更新為 `mid - 1`。
        *   如果 `target > arr[mid]`，目標在右半部分，將 `low` 更新為 `mid + 1`。
    4.  重複步驟 2 和 3，直到找到目標或 `low > high`（表示搜尋範圍為空）。

*   **程式碼範例 (Python-like Pseudocode):**
    ```python
    function binary_search(arr, target):
        low = 0
        high = len(arr) - 1

        while low <= high: # 只要搜尋範圍有效
            mid = low + (high - low) // 2 # 計算中間索引
            if arr[mid] == target: # 找到目標
                return mid
            elif target < arr[mid]: # 目標在左半部分
                high = mid - 1
            else: # 目標在右半部分
                low = mid + 1
        return -1 # 未找到目標
    ```

*   **時間複雜度推導：**
    *   **基本操作：** 每次比較 `arr[mid]` 與 `target`。
    *   **每次迭代：** 搜尋範圍都會縮小一半。
        *   起始範圍：$n$
        *   第一次迭代後：$n/2$
        *   第二次迭代後：$n/4$
        *   ...
        *   第 $k$ 次迭代後：$n/2^k$
    *   當搜尋範圍縮小到只剩一個元素時，演算法終止。設經過 $k$ 次迭代後，範圍大小為 1：
        $n / 2^k \approx 1 \implies n \approx 2^k \implies k \approx \log_2 n$。
    *   **大O符號表示：** 二元搜尋的時間複雜度為 $\mathbf{O(\log n)}$。

-----

### 4. 與相鄰概念的關聯

#### 4.1 資料結構與演算法的關係

*   **核心觀念：** 資料結構 (Data Structure) 是組織和儲存資料的方式，而演算法是處理這些資料的步驟。兩者緊密相連，互為表裡。

*   **關聯：**
    *   **相輔相成：** 沒有資料結構，演算法無法有效地處理資料；沒有演算法，資料結構只是一堆靜態的數據。
    *   **效率影響：** 選擇合適的資料結構可以極大地提升演算法的效率。例如：
        *   在未排序的陣列中搜尋元素只能用 $O(n)$ 的線性搜尋。
        *   但如果資料儲存在已排序的陣列中，就可以使用 $O(\log n)$ 的二元搜尋。
        *   要快速查找，哈希表 (Hash Table) 通常能提供平均 $O(1)$ 的查詢時間，但它犧牲了一定的空間。
    *   **設計考量：** 在設計解決方案時，通常需要同時考慮資料的儲存方式 (資料結構) 和處理方式 (演算法)，兩者協同工作以達到最佳性能。

#### 4.2 演算法設計策略 (簡述)

*   **核心觀念：** 演算法設計策略是解決問題時採用的通用方法論，它們提供了一種思考問題、分解問題和構建解決方案的框架。

*   **常見策略：**
    1.  **分治法 (Divide and Conquer)：**
        *   **思想：** 將一個大問題分解成數個相同或相似的、獨立的子問題，遞歸地解決這些子問題，然後將子問題的解合併以得到原問題的解。
        *   **範例：** 合併排序 (Merge Sort)、快速排序 (Quick Sort)、二元搜尋。
    2.  **貪婪法 (Greedy Approach)：**
        *   **思想：** 在每一步都做出局部最優的選擇，希望這些局部最優選擇最終能導向全局最優解。這種方法不總能找到全局最優解，但對於某些特定問題有效。
        *   **範例：** 霍夫曼編碼 (Huffman Coding)、找零錢問題（在某些貨幣系統下）。
    3.  **動態規劃 (Dynamic Programming)：**
        *   **思想：** 透過將複雜問題分解為重疊子問題，並儲存這些子問題的解，避免重複計算。適用於具有「最優子結構」和「重疊子問題」特性的問題。
        *   **範例：** 費波那契數列 (遞迴優化)、最長公共子序列、背包問題。
    4.  **回溯法 (Backtracking)：**
        *   **思想：** 深度優先搜尋 (DFS) 的一種應用。從根節點開始，嘗試所有可能的路徑，當發現當前路徑無法導向解時，就回溯到上一個決策點，嘗試另一條路徑。
        *   **範例：** 八皇后問題、數獨求解、組合問題。

-----

### 5. 常見錯誤與澄清

#### 5.1 時間複雜度 vs 實際執行時間

*   **常見錯誤：** 認為 $O(n^2)$ 的程式一定比 $O(n)$ 的程式慢。
*   **澄清：** 大O符號描述的是演算法在輸入規模趨近於無窮大時的「漸近增長趨勢」，它忽略了常數因子和低階項。
    *   對於**非常小的輸入 $n$**，一個常數因子很大的 $O(n)$ 演算法可能比一個常數因子很小的 $O(n^2)$ 演算法慢。例如，當 $n=5$ 時，$T_A(n) = 100n$ ($O(n)$) 會是 $500$ 單位時間，而 $T_B(n) = 5n^2$ ($O(n^2)$) 會是 $125$ 單位時間。此時 $O(n^2)$ 反而更快。
    *   **只有當 $n$ 夠大時**，大O符號所描述的漸近趨勢才變得顯著和有意義，並準確反映演算法的相對效率。因此，在分析時應區分「理論性能」和「實際微觀性能」。

#### 5.2 最佳、平均、最差情況

*   **常見錯誤：** 只考慮最佳情況，或將平均情況誤認為最差情況。
*   **澄清：** 演算法的性能通常在三種情況下進行分析：
    *   **最佳情況 (Best Case)：** 演算法在最順利、最理想的輸入下所達到的性能。例如，線性搜尋第一個元素即找到。通常用 $\Omega$ 符號表示。
    *   **最差情況 (Worst Case)：** 演算法在最不利、最糟糕的輸入下所達到的性能。例如，線性搜尋遍歷到最後才找到或未找到。這是最常用的分析，通常用 $O$ 符號表示，提供了性能的「保證」。
    *   **平均情況 (Average Case)：** 考慮所有可能的輸入情況下，演算法預期的表現。這通常是最複雜的分析，因為它需要考慮輸入數據的機率分佈。
*   **重要性：** 在大多數實際應用中，最差情況分析是最重要的，因為它能保證演算法在任何輸入下都不會超出特定的性能上限，這對於設計魯棒 (robust) 的系統至關重要。

#### 5.3 常數項的影響

*   **常見錯誤：** 認為 $O(1)$ 總是比 $O(\log n)$ 快，或 $O(n)$ 總是比 $O(n \log n)$ 慢。
*   **澄清：** 大O符號再次提醒我們，它忽略了常數項。
    *   一個「複雜」的 $O(1)$ 操作（例如，涉及多次內存訪問和複雜計算的哈希函數）可能比一個「簡單」的 $O(\log n)$ 操作（例如，在一個小集合上進行幾次比較）花費更長的時間。
    *   同樣，如果一個 $O(n)$ 演算法的常數因子非常小，而另一個 $O(n \log n)$ 演算法的常數因子非常大，則在一定的 $n$ 值範圍內，$O(n)$ 演算法可能表現更好，甚至在某些情況下，常數因子巨大的 $O(n \log n)$ 可能比常數因子小的 $O(n^2)$ 演算法在特定 $n$ 值下更慢。
*   **結論：** 大O符號是關於「增長趨勢」和「相對排序」，而不是「精確執行時間」。在選擇演算法時，除了大O符號，還要根據實際的輸入規模、程式語言、硬體環境等因素綜合考量。

-----

### 6. 小練習 (附詳解)

#### 練習 1：分析時間複雜度

分析以下 Python 程式碼片段的時間複雜度：

```python
def analyze_me(n, m):
    result = 0
    # 第一部分
    for i in range(n):
        result += i

    # 第二部分
    for j in range(m):
        for k in range(m):
            result += j * k

    # 第三部分
    while n > 1:
        n //= 2
        result += n
    
    return result
```

**詳解：**

1.  **分析第一部分：**
    ```python
    for i in range(n):
        result += i
    ```
    *   這個迴圈會執行 $n$ 次。
    *   迴圈內部的 `result += i` 操作是常數時間 $O(1)$。
    *   因此，第一部分的總時間複雜度為 $n \times O(1) = O(n)$。

2.  **分析第二部分：**
    ```python
    for j in range(m):
        for k in range(m):
            result += j * k
    ```
    *   外層迴圈 `for j in range(m):` 會執行 $m$ 次。
    *   內層迴圈 `for k in range(m):` 會執行 $m$ 次。
    *   內層迴圈內部的 `result += j * k` 操作是常數時間 $O(1)$。
    *   因此，內層迴圈總共執行 $m \times O(1) = O(m)$。
    *   外層迴圈包裹內層迴圈，所以第二部分的總時間複雜度為 $m \times O(m) = O(m^2)$。

3.  **分析第三部分：**
    ```python
    while n > 1:
        n //= 2
        result += n
    ```
    *   這個 `while` 迴圈在每次迭代中都將 `n` 除以 2 (`n //= 2`)。
    *   這種每次將問題規模減半的操作模式是典型的對數時間複雜度。
    *   它將執行大約 $\log_2 n$ 次。
    *   迴圈內的操作是常數時間 $O(1)$。
    *   因此，第三部分的總時間複雜度為 $\log n \times O(1) = O(\log n)$。

4.  **合併總時間複雜度：**
    *   整個函數的總時間複雜度是各部分時間複雜度的和：$O(n) + O(m^2) + O(\log n)$。
    *   在漸近分析中，我們只保留最高階項。因此，如果 $n$ 和 $m$ 都是輸入，我們不能簡單地丟棄其中一個。
    *   最終的總時間複雜度為 $\mathbf{O(n + m^2)}$。 （因為通常 $m^2$ 會比 $\log n$ 增長快，所以 $\log n$ 會被捨棄，留下 $O(n + m^2)$。）

#### 練習 2：比較演算法效率

假設有兩個演算法，A 和 B，其時間複雜度函數分別為 $T_A(n) = 50n$ 和 $T_B(n) = 0.5n^2$。

1.  當 $n=50$ 時，哪個演算法更快？
2.  當 $n=200$ 時，哪個演算法更快？
3.  找出一個 $n_0$，使得當 $n > n_0$ 時，演算法 A 總是比演算法 B 快。

**詳解：**

1.  **當 $n=50$ 時：**
    *   演算法 A 的執行時間：$T_A(50) = 50 \times 50 = 2500$。
    *   演算法 B 的執行時間：$T_B(50) = 0.5 \times 50^2 = 0.5 \times 2500 = 1250$。
    *   **結論：** 演算法 B 更快（執行時間更短）。

2.  **當 $n=200$ 時：**
    *   演算法 A 的執行時間：$T_A(200) = 50 \times 200 = 10000$。
    *   演算法 B 的執行時間：$T_B(200) = 0.5 \times 200^2 = 0.5 \times 40000 = 20000$。
    *   **結論：** 演算法 A 更快。

3.  **找出 $n_0$：**
    *   我們需要找到 $n$ 值，使得 $T_A(n) < T_B(n)$。
    *   $50n < 0.5n^2$
    *   由於 $n$ 通常為正數（輸入大小），我們可以兩邊同時除以 $n$：
    *   $50 < 0.5n$
    *   將兩邊同時乘以 2：
    *   $100 < n$
    *   **結論：** 當 $n > 100$ 時，演算法 A ($O(n)$) 總是比演算法 B ($O(n^2)$) 快。所以 $n_0 = 100$。
    *   這個練習再次強調了在小規模問題下，常數因子可能決定性能優劣；但隨著輸入規模 $n$ 的增長，漸近時間複雜度（大O符號）將是決定哪個演算法更優的關鍵因素。

-----

### 7. 延伸閱讀/參考

*   **經典書籍：**
    *   **《演算法導論 (Introduction to Algorithms)》** - Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein (俗稱 CLRS)：電腦科學領域最權威的演算法教材之一，內容全面深入。
    *   **《資料結構與演算法 (Data Structures and Algorithms in Python/Java/C++)》** - 各種版本，例如 Robert Lafore, Michael T. Goodrich 等：針對特定程式語言，結合資料結構與演算法的實作細節。
    *   **《學習演算法的樂趣 (Groking Algorithms)》** - Aditya Bhargava：以圖文並茂的方式介紹基礎演算法，非常適合初學者。

*   **線上課程與學習平台：**
    *   **Coursera:**
        *   "Algorithms, Part I" & "Algorithms, Part II" by Princeton University
        *   "Data Structures and Algorithms Specialization" by University of California San Diego
    *   **MIT OpenCourseware:** "6.006 Introduction to Algorithms"：麻省理工學院的免費公開課程，提供教材、講義和練習。
    *   **LeetCode:** 是一個廣受歡迎的線上平台，提供大量演算法題目供練習，並有社群分享解法與討論。
    *   **GeeksforGeeks:** 豐富的演算法和資料結構教學文章、程式碼範例和問題解析。

*   **維基百科：**
    *   [演算法](https://zh.wikipedia.org/zh-tw/%E7%AE%97%E6%B3%95)
    *   [演算法分析](https://zh.wikipedia.org/zh-tw/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90)
    *   [大O符號](https://zh.wikipedia.org/zh-tw/%E5%A4%A7O%E7%AC%A6%E5%8F%B7)
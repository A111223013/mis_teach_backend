# enhanced_ai_agent.py
"""
å¢å¼·ç‰ˆAI Agent - æ•´åˆRAGå‘é‡è³‡æ–™åº«
çµåˆåŸæœ‰çš„LangGraphæ¶æ§‹èˆ‡æ–°çš„RAGç³»çµ±
"""

import os
import sys
from pathlib import Path
from langgraph.graph import StateGraph
from langchain.schema.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from flask import Blueprint, request, jsonify
from typing import TypedDict, List, Dict, Any, Optional

# æ·»åŠ ocrç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥RAGè™•ç†å™¨
current_dir = Path(__file__).parent
ocr_dir = current_dir.parent.parent / "ocr"
sys.path.append(str(ocr_dir))

try:
    from enhanced_rag_processor import EnhancedRAGProcessor
    RAG_AVAILABLE = True
except ImportError:
    print("âš ï¸ RAGè™•ç†å™¨ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨åŸå§‹çš„FAISSæª¢ç´¢")
    RAG_AVAILABLE = False

enhanced_ai_agent_bp = Blueprint('enhanced_ai_agent', __name__)

class EnhancedGraphState(TypedDict):
    question: str
    docs: List
    chat_history: List[BaseMessage]
    answer: str
    rag_sources: List[Dict]  # æ–°å¢ï¼šRAGä¾†æºä¿¡æ¯

class EnhancedAIAgent:
    """å¢å¼·ç‰ˆAI Agenté¡"""
    
    def __init__(self):
        self.retriever = None
        self.rag_processor = None
        self.chat_memory = []
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)
        
        # åˆå§‹åŒ–RAGç³»çµ±
        if RAG_AVAILABLE:
            self._init_rag_system()
        
    def _init_rag_system(self):
        """åˆå§‹åŒ–RAGç³»çµ±"""
        try:
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–RAGç³»çµ±...")
            
            # è¨­ç½®RAGè™•ç†å™¨
            self.rag_processor = EnhancedRAGProcessor(
                use_chromadb=True,
                chromadb_path=str(ocr_dir / "knowledge_db")
            )
            
            # å˜—è©¦è¼‰å…¥ç¾æœ‰å‘é‡è³‡æ–™åº«
            if not self.rag_processor.load_vector_database("textbook_knowledge"):
                print("ğŸ“š æœªæ‰¾åˆ°ç¾æœ‰RAGè³‡æ–™åº«ï¼Œå°‡ä½¿ç”¨å‚³çµ±FAISSæª¢ç´¢")
                self.rag_processor = None
            else:
                print("âœ… RAGç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            print(f"âŒ RAGç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            self.rag_processor = None

    def init_embeddings_from_txt(self, txt_path: str):
        """åˆå§‹åŒ–å‚³çµ±FAISSå‘é‡è³‡æ–™åº«ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            docs = [Document(page_content=content)]
            chunks = CharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            self.retriever = FAISS.from_documents(chunks, embedding=embeddings).as_retriever()
            print("âœ… å‚³çµ±FAISSæª¢ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ FAISSæª¢ç´¢å™¨åˆå§‹åŒ–å¤±æ•—: {e}")

    def retrieve_context(self, question: str) -> tuple[str, List[Dict]]:
        """
        æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡
        
        Returns:
            tuple: (context_text, rag_sources)
        """
        rag_sources = []
        
        # å„ªå…ˆä½¿ç”¨RAGç³»çµ±
        if self.rag_processor:
            try:
                rag_results = self.rag_processor.search(question, top_k=3)
                if rag_results:
                    context_parts = []
                    for result in rag_results:
                        metadata = result["metadata"]
                        content = result["content"]
                        
                        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
                        context_part = f"""
ä¾†æºï¼š{metadata.get('chapter', 'N/A')} - {metadata.get('section', 'N/A')}
é ç¢¼ï¼š{metadata.get('page_number', 'N/A')}
å…§å®¹ï¼š{content[:400]}...
"""
                        context_parts.append(context_part)
                        
                        # ä¿å­˜ä¾†æºä¿¡æ¯
                        rag_sources.append({
                            "chapter": metadata.get('chapter', 'N/A'),
                            "section": metadata.get('section', 'N/A'),
                            "page": metadata.get('page_number', 'N/A'),
                            "content_preview": content[:200] + "...",
                            "similarity": 1 - result.get('distance', 0)
                        })
                    
                    return "\n".join(context_parts), rag_sources
            except Exception as e:
                print(f"âŒ RAGæª¢ç´¢å¤±æ•—: {e}")
        
        # å‚™ç”¨ï¼šä½¿ç”¨å‚³çµ±FAISSæª¢ç´¢
        if self.retriever:
            try:
                docs = self.retriever.invoke(question)
                context = "\n".join([doc.page_content for doc in docs])
                return context, []
            except Exception as e:
                print(f"âŒ å‚³çµ±æª¢ç´¢å¤±æ•—: {e}")
        
        return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚", []

    def generate_answer(self, state: dict) -> dict:
        """ç”Ÿæˆç­”æ¡ˆç¯€é»ï¼ˆLangGraphï¼‰"""
        question = state["question"]
        history = state.get("chat_history", [])
        
        # æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡
        context, rag_sources = self.retrieve_context(question)
        
        # å°è©±æ­·å²æ ¼å¼åŒ–
        history_str = "\n".join(
            [f"ä½¿ç”¨è€…ï¼š{msg.content}" if isinstance(msg, HumanMessage) else f"AIï¼š{msg.content}"
             for msg in history[-6:]]  # åªä¿ç•™æœ€è¿‘3è¼ªå°è©±
        )
        
        # å»ºç«‹å¢å¼·çš„prompt
        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ•™å­¸åŠ©ç†ï¼Œè«‹æ ¹æ“šæä¾›çš„æ•™æå…§å®¹å›ç­”å•é¡Œã€‚

å…ˆå‰çš„å°è©±ç´€éŒ„ï¼š
{history_str}

ç›¸é—œæ•™æå…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šæ•™æå…§å®¹æä¾›æº–ç¢ºã€è©³ç´°çš„å›ç­”ã€‚å¦‚æœæ•™æä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹èª å¯¦èªªæ˜ã€‚
å›ç­”æ™‚è«‹ï¼š
1. ç›´æ¥å›ç­”å•é¡Œ
2. å¼•ç”¨ç›¸é—œçš„æ•™æå…§å®¹
3. å¦‚æœæœ‰å¤šå€‹ç›¸é—œæ¦‚å¿µï¼Œè«‹åˆ†é»èªªæ˜
4. ä¿æŒå›ç­”çš„å­¸è¡“æ€§å’Œæº–ç¢ºæ€§
"""
        
        # å‘¼å«Gemini LLM
        result = self.llm.invoke(prompt)
        
        return {
            "answer": result.content,
            "rag_sources": rag_sources
        }

    def build_graph(self):
        """å»ºç«‹LangGraph"""
        builder = StateGraph(dict)
        builder.add_node("generate_answer", self.generate_answer)
        builder.set_entry_point("generate_answer")
        builder.set_finish_point("generate_answer")
        return builder.compile()

    def answer_with_langgraph(self, question: str, history: List[BaseMessage]) -> Dict[str, Any]:
        """ä½¿ç”¨LangGraphå›ç­”å•é¡Œ"""
        graph = self.build_graph()
        result = graph.invoke({
            "question": question,
            "chat_history": history
        })
        return result

    def ask_question(self, question: str) -> Dict[str, Any]:
        """è™•ç†å•é¡Œä¸¦è¿”å›ç­”æ¡ˆ"""
        try:
            # åŠ å…¥å°è©±æ­·å²ï¼ˆä½¿ç”¨è€…å•é¡Œï¼‰
            self.chat_memory.append(HumanMessage(content=question))
            
            # å›ç­”å•é¡Œ
            result = self.answer_with_langgraph(question, self.chat_memory)
            answer = result["answer"]
            rag_sources = result.get("rag_sources", [])
            
            # åŠ å…¥å°è©±æ­·å²ï¼ˆAIå›ç­”ï¼‰
            self.chat_memory.append(AIMessage(content=answer))
            
            # é™åˆ¶å°è©±æ­·å²é•·åº¦
            if len(self.chat_memory) > 20:
                self.chat_memory = self.chat_memory[-20:]
            
            return {
                "answer": answer,
                "sources": rag_sources,
                "has_rag": self.rag_processor is not None
            }
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "answer": "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚",
                "sources": [],
                "has_rag": False
            }

    def clear_memory(self):
        """æ¸…é™¤å°è©±è¨˜æ†¶"""
        self.chat_memory = []

    def get_memory_summary(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶æ‘˜è¦"""
        return {
            "total_messages": len(self.chat_memory),
            "recent_questions": [
                msg.content for msg in self.chat_memory[-6:] 
                if isinstance(msg, HumanMessage)
            ][-3:]
        }

# å…¨å±€AI Agentå¯¦ä¾‹
ai_agent = EnhancedAIAgent()

# APIç«¯é»
@enhanced_ai_agent_bp.route("/ask", methods=["POST"])
def ask():
    """è™•ç†å•ç­”è«‹æ±‚"""
    data = request.get_json()
    question = data.get("question", "")
    
    if not question:
        return jsonify({"error": "è«‹è¼¸å…¥å•é¡Œ"}), 400
    
    result = ai_agent.ask_question(question)
    
    if "error" in result:
        return jsonify(result), 500
    
    return jsonify(result)

@enhanced_ai_agent_bp.route("/memory/clear", methods=["POST"])
def clear_memory():
    """æ¸…é™¤å°è©±è¨˜æ†¶"""
    ai_agent.clear_memory()
    return jsonify({"message": "å°è©±è¨˜æ†¶å·²æ¸…é™¤"})

@enhanced_ai_agent_bp.route("/memory/summary", methods=["GET"])
def memory_summary():
    """ç²å–è¨˜æ†¶æ‘˜è¦"""
    summary = ai_agent.get_memory_summary()
    return jsonify(summary)

@enhanced_ai_agent_bp.route("/status", methods=["GET"])
def status():
    """ç²å–ç³»çµ±ç‹€æ…‹"""
    return jsonify({
        "rag_available": ai_agent.rag_processor is not None,
        "fallback_retriever": ai_agent.retriever is not None,
        "memory_size": len(ai_agent.chat_memory)
    })

# åˆå§‹åŒ–å‡½æ•¸ï¼ˆåœ¨app.pyä¸­èª¿ç”¨ï¼‰
def init_enhanced_ai_agent(txt_path: str = None):
    """åˆå§‹åŒ–å¢å¼·ç‰ˆAI Agent"""
    global ai_agent
    
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¢å¼·ç‰ˆAI Agent...")
    
    # å¦‚æœæä¾›äº†txtæ–‡ä»¶è·¯å¾‘ï¼Œåˆå§‹åŒ–å‚™ç”¨æª¢ç´¢å™¨
    if txt_path and os.path.exists(txt_path):
        ai_agent.init_embeddings_from_txt(txt_path)
    
    print("âœ… å¢å¼·ç‰ˆAI Agentåˆå§‹åŒ–å®Œæˆ")
    return ai_agent

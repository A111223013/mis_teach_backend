import os
import base64
import json
import requests
import google.generativeai as genai



# ========== Gemini API Key ==========
GEMINI_API_KEY = os.environ.get("AIzaSyCYsh5zsAH-DE4ChAD8PMT1xIvNw1YSWzQ", "AIzaSyBad7mpaX-fPPtpjbcgZ1JpKOBPJZJkmf4")

# ========== æç¤ºè©å®šç¾© ==========
main_agent_prompt_template = """
ä½ æ˜¯ã€Œè¨ˆç®—æ©Ÿæ¦‚è«–åˆ¤é¡Œç³»çµ±ã€ä¸­çš„ä¸»ä»£ç†äºº Gemini 2.0ã€‚

è«‹æ ¹æ“šé¡Œç›®å…§å®¹ï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
1. åˆ¤æ–·æ­£ç¢ºç­”æ¡ˆç‚ºä½•ï¼Œä¸¦èªªæ˜ç†ç”±ã€‚
2. è«‹èªªæ˜æœ¬é¡Œå±¬æ–¼ä»¥ä¸‹å“ªä¸€ç¨®å›ºå®šé¡Œå‹ï¼Œä¸¦æŒ‡å‡ºæ˜¯å¦å­˜åœ¨é¡Œæ„éŒ¯èª¤æˆ–æ•˜è¿°æ¨¡ç³Šæƒ…å½¢ã€‚å¯ç”¨çš„é¡Œå‹å¦‚ä¸‹ï¼š
   - "single-choice"ï¼šå–®é¸é¡Œ  
   - "multiple-choice"ï¼šå¤šé¸é¡Œ  
   - "fill-in-the-blank"ï¼šå¡«ç©ºé¡Œ  
   - "true-false"ï¼šæ˜¯éé¡Œ  
   - "short-answer"ï¼šç°¡ç­”é¡Œï¼å•ç­”é¡Œ  
   - "long-answer"ï¼šç”³è«–é¡Œï¼é•·ç­”é¡Œ  
   - "choice-answer"ï¼šé¸å¡«é¡Œ  
   - "draw-answer"ï¼šç•«åœ–é¡Œ  
   - "coding-answer"ï¼šç¨‹å¼æ’°å¯«é¡Œ
3. åˆ¤æ–·é¡Œç›®æ•˜è¿°æ˜¯å¦æœ‰èª¤ï¼ˆè‹¥æœ‰éŒ¯èª¤è«‹æŒ‡å‡ºä¸¦èªªæ˜ï¼‰ã€‚
4. å¾ä»¥ä¸‹ 12 å€‹çŸ¥è­˜é»ä¸­ï¼Œé¸å‡ºæœ¬é¡Œæœ€ç›¸é—œçš„ä¸€é …ï¼š
   - åŸºæœ¬è¨ˆæ¦‚ã€æ•¸ä½é‚è¼¯ã€ä½œæ¥­ç³»çµ±ã€ç¨‹å¼èªè¨€ã€è³‡æ–™çµæ§‹ã€ç¶²è·¯ã€è³‡æ–™åº«ã€AIèˆ‡æ©Ÿå™¨å­¸ç¿’ã€è³‡è¨Šå®‰å…¨ã€é›²ç«¯èˆ‡è™›æ“¬åŒ–ã€MISã€è»Ÿé«”å·¥ç¨‹èˆ‡ç³»çµ±é–‹ç™¼
5. è©•ä¼°é›£æ˜“åº¦ï¼ˆç°¡å–®ã€ä¸­ç­‰ã€å›°é›£ï¼‰ã€‚
6. è©³ç´°èªªæ˜è§£æèˆ‡æ€è·¯ã€‚

ã€é¡Œç›®å…§å®¹ã€‘
{question_text}
"""

secondary_agent_prompt_template = """
ä½ æ˜¯ã€Œè¨ˆç®—æ©Ÿæ¦‚è«–åˆ¤é¡Œç³»çµ±ã€ä¸­çš„æ¬¡è¦ä»£ç†äºº LLaMA 3.1ã€‚

è«‹é–±è®€ä»¥ä¸‹ä¸»ä»£ç†äºº Gemini 2.0 çš„åˆ†æçµæœï¼Œä¸¦ä¾ä¸‹åˆ—è¦å‰‡åšå‡ºå›æ‡‰ï¼š

ã€ä¸»ä»£ç†äººåˆ†æã€‘
{main_response}

ã€ä»»å‹™ã€‘
è«‹åˆ¤æ–·ä½ æ˜¯å¦åŒæ„ä¸»ä»£ç†äººçš„åˆ¤æ–·èˆ‡çµè«–ï¼Œå›è¦†æ ¼å¼å¦‚ä¸‹ï¼š

1. ä½ æ˜¯å¦åŒæ„ä¸»ä»£ç†äººçš„ã€Œæ­£ç¢ºç­”æ¡ˆã€åˆ¤æ–·ï¼Ÿï¼ˆåŒæ„ / ä¸åŒæ„ï¼‰è‹¥ä¸åŒæ„ï¼Œè«‹èªªæ˜ä½ èªç‚ºæ­£ç¢ºçš„é¸é …èˆ‡ç†ç”±ã€‚
2. ä½ æ˜¯å¦åŒæ„å…¶ã€Œé¡Œå‹åˆ¤å®šã€èˆ‡ã€Œé¡Œæ„æ˜¯å¦éŒ¯èª¤ã€çš„åˆ†æï¼Ÿ
3. ä½ æ˜¯å¦åŒæ„å…¶é¸å®šçš„ã€ŒçŸ¥è­˜åˆ†é¡ã€èˆ‡ã€Œé›£æ˜“åº¦ã€ï¼Ÿ
4. æœ‰ç„¡å…¶ä»–è£œå……æˆ–ç•°è­°ï¼Ÿ
"""

arbiter_agent_prompt_template = """
ä½ æ˜¯ã€Œè¨ˆç®—æ©Ÿæ¦‚è«–åˆ¤é¡Œç³»çµ±ã€ä¸­çš„ä»²è£ä»£ç†äºº Gemini 2.5ï¼Œç•¶ä¸»ä»£ç†äººèˆ‡æ¬¡ä»£ç†äººæ„è¦‹ä¸ä¸€è‡´æ™‚ï¼Œç”±ä½ åšå‡ºæœ€çµ‚åˆ¤æ–·ã€‚

è«‹é–±è®€ä»¥ä¸‹å…©ä½ä»£ç†äººçš„æ„è¦‹å¾Œï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼Œä¸¦å°‡è¼¸å‡ºçµæœ**åš´æ ¼æŒ‰ç…§æŒ‡å®šçš„ JSON é™£åˆ—æ ¼å¼ï¼ˆlist of dictï¼‰**ç”¢å‡ºï¼š

ã€ä¸»ä»£ç†äººåˆ†æã€‘
{main_response}

ã€æ¬¡ä»£ç†äººå›æ‡‰ã€‘
{secondary_response}

ã€ä»»å‹™èªªæ˜ã€‘

è«‹æ•´åˆä¸Šè¿°å…©ä½ä»£ç†äººçš„å›ç­”ï¼Œå®Œæˆé¡Œç›®æ¬„ä½çš„æ•´åˆèˆ‡è£œå…¨ï¼Œä¸¦éµå®ˆä»¥ä¸‹æ ¼å¼è¦ç¯„ï¼š

---

ã€è¼¸å‡ºè¦å‰‡ã€‘

1. è«‹è¼¸å‡ºä¸€å€‹ã€ŒåŒ…å« 1 ç­†é¡Œç›®è³‡æ–™çš„ JSON é™£åˆ—ã€ï¼ˆå³ `[{{...}}]` å½¢å¼ï¼‰ï¼Œå³ä½¿åªæœ‰ä¸€é¡Œï¼Œä¹Ÿä¸è¦çœç•¥ list åŒ…è£ã€‚
2. å¿…é ˆä¿ç•™é¡Œç›®åŸæœ¬çš„æ‰€æœ‰æ¬„ä½ï¼ˆä¾‹å¦‚ï¼štypeã€question_textã€question_idã€schoolã€departmentã€year... ç­‰ï¼‰ã€‚
3. å°‡ `"answer"` æ¬„ä½ç§»å‹•åˆ° `"image_file"` æ¬„ä½ä¹‹å¾Œã€‚
4. æ–°å¢ä»¥ä¸‹ 4 å€‹æ¬„ä½ï¼ˆè«‹å‹™å¿…è£œé½Šï¼‰ï¼š
   - `"detail-answer"`ï¼šæä¾›æ¸…æ¥šä¸”å®Œæ•´çš„è§£é¡Œè©³è§£ã€‚
   - `"key-points"`ï¼šè«‹å¾ä»¥ä¸‹ 12 å€‹é¸é …ä¸­æ“‡ä¸€å¡«å…¥ï¼ˆå¿…å¡«ï¼‰ï¼š
     `"åŸºæœ¬è¨ˆæ¦‚"ã€"æ•¸ä½é‚è¼¯"ã€"ä½œæ¥­ç³»çµ±"ã€"ç¨‹å¼èªè¨€"ã€"è³‡æ–™çµæ§‹"ã€"ç¶²è·¯"ã€"è³‡æ–™åº«"ã€"AIèˆ‡æ©Ÿå™¨å­¸ç¿’"ã€"è³‡è¨Šå®‰å…¨"ã€"é›²ç«¯èˆ‡è™›æ“¬åŒ–"ã€"MIS"ã€"è»Ÿé«”å·¥ç¨‹èˆ‡ç³»çµ±é–‹ç™¼"`
   - `"difficulty level"`ï¼šåªèƒ½å¡«å…¥ `"ç°¡å–®"`ã€`"ä¸­ç­‰"` æˆ– `"å›°é›£"`ï¼ˆå¿…å¡«ï¼‰ã€‚
   - `"error reason"`ï¼šè‹¥ä¸»ä»£ç†äººèˆ‡æ¬¡ä»£ç†äººæœ‰ã€Œè¡çªæˆ–ä¸åŒæ„è¦‹ã€ï¼Œè«‹å¡«å…¥ç°¡çŸ­èªªæ˜ï¼Œå¦å‰‡ç•™ç©ºå­—ä¸²ã€‚
5. è‹¥é¡Œå‹èˆ‡åŸå§‹ `"answer_type"` ä¸ä¸€è‡´ï¼Œè«‹æ¡ç”¨ä¸»ä»£ç†äººæä¾›çš„ç­”æ¡ˆæ›´æ–° `"answer_type"` æ¬„ä½ã€‚
6. é™¤ä¸Šè¿°èª¿æ•´ï¼Œå…¶é¤˜åŸå§‹æ¬„ä½è«‹å‹¿éºæ¼ï¼Œä¸¦ä¿ç•™åŸå§‹é †åºèˆ‡å…§å®¹ã€‚

---

ã€è¼¸å‡ºæ ¼å¼ã€‘

è«‹ä»¥å¦‚ä¸‹æ ¼å¼ï¼ˆlist of one dictï¼‰å›å‚³æ•´åˆå¾Œçš„ä»²è£åˆ¤æ–·çµæœï¼Œä¸è¦å›å‚³ä»»ä½•è§£é‡‹èªªæ˜æˆ–é¡å¤–æ–‡å­—ï¼Œä¸¦ä»¥åš´æ ¼çš„ JSON æ ¼å¼è¼¸å‡ºï¼š

[{{  
  "answer": "è«‹æŒ‰ç…§ä»²è£çµæœå¡«å…¥ç°¡è¦ç­”æ¡ˆ",  
  "detail-answer": "è«‹æŒ‰ç…§ä»²è£çµæœå¡«å…¥è©³ç´°ç­”æ¡ˆ",  
  "key-points": "æŒ‰ç…§ä¸»ä»£ç†äººæä¾›çš„é—œéµé»å¡«å¯«ï¼Œå¾ä»¥ä¸‹é¸é …ä¸­æ“‡ä¸€ï¼šåŸºæœ¬è¨ˆæ¦‚ã€æ•¸ä½é‚è¼¯ã€ä½œæ¥­ç³»çµ±ã€ç¨‹å¼èªè¨€ã€è³‡æ–™çµæ§‹ã€ç¶²è·¯ã€è³‡æ–™åº«ã€AIèˆ‡æ©Ÿå™¨å­¸ç¿’ã€è³‡è¨Šå®‰å…¨ã€é›²ç«¯èˆ‡è™›æ“¬åŒ–ã€MISã€è»Ÿé«”å·¥ç¨‹èˆ‡ç³»çµ±é–‹ç™¼",  
  "difficulty level": "æŒ‰ç…§ä»²è£çµæœå¡«å¯«é›£åº¦ç­‰ç´šï¼Œåªèƒ½å¡«å…¥ï¼šç°¡å–®ã€ä¸­ç­‰ã€å›°é›£",  
  "error reason": "è‹¥ä¸»ä»£ç†äººèˆ‡æ¬¡ä»£ç†äººæœ‰ã€Œè¡çªæˆ–ä¸åŒæ„è¦‹ã€ï¼Œè«‹å¡«å…¥ç°¡çŸ­èªªæ˜ï¼Œå¦å‰‡ç•™ç©ºå­—ä¸²"  
}}]

"""




# ========== æ¨¡å‹åˆå§‹åŒ– ==========
# è¨­å®šé‡‘é‘°
genai.configure(api_key=GEMINI_API_KEY)

def read_image_base64(image_path):
    if not os.path.exists(image_path):
        return None
    with open(image_path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
        return encoded

def call_gemini_model(prompt, image_base64=None):
    model = genai.GenerativeModel("gemini-2.0-flash")

    if image_base64:
        import base64
        import io
        from PIL import Image

        image_data = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_data))

        response = model.generate_content([
            prompt,
            image  # æ³¨æ„é€™è£¡æ˜¯ç›´æ¥å‚³åœ–ç‰‡ç‰©ä»¶
        ])
    else:
        response = model.generate_content(prompt)

    return response.text.strip()

def call_llama_model(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "llama3:8b",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "max_tokens": 1024
            }
        }
    )
    if response.status_code == 200:
        return response.json()["response"].strip()
    else:
        raise Exception(f"Ollama å›æ‡‰å¤±æ•—ï¼š{response.status_code} - {response.text}")

# ========== è™•ç† single ==========
def process_question(question):
    import copy
    question_text = question["question_text"]
    image_path = question.get("image_file")
    image_base64 = None

    if isinstance(image_path, list):
        image_file = image_path[0] if image_path else None
    else:
        image_file = image_path

    if image_file:
        image_base64 = read_image_base64(os.path.join("backend", "src", "picture", image_file))

    main_prompt = main_agent_prompt_template.format(question_text=question_text)
    main_response = call_gemini_model(main_prompt, image_base64=image_base64)

    secondary_prompt = secondary_agent_prompt_template.format(main_response=main_response)
    secondary_response = call_llama_model(secondary_prompt)

    arbiter_prompt = arbiter_agent_prompt_template.format(
        main_response=main_response,
        secondary_response=secondary_response
    )
    arbiter_response = call_gemini_model(arbiter_prompt)

    try:
        result = json.loads(arbiter_response)
        if isinstance(result, list) and result:
            return result[0]  # å›å‚³é¡Œç›®çµæ§‹ï¼ˆä»²è£å¾Œæ ¼å¼ï¼‰
        else:
            raise ValueError("ä»²è£è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢º")
    except Exception as e:
        # è‹¥è§£æå¤±æ•—ï¼Œå›å‚³åŸå§‹é¡Œç›®ä½†è£œé½Šæ–°æ¬„ä½
        fallback = copy.deepcopy(question)
        fallback["answer"] = ""
        fallback["detail-answer"] = ""
        fallback["key-points"] = ""
        fallback["difficulty level"] = ""
        fallback["error reason"] = f"ä»²è£è§£æéŒ¯èª¤: {e}"
        return fallback
    
# ========== è™•ç† group ==========
def process_group_question(group):
    import copy
    group_copy = copy.deepcopy(group)
    processed_sub_questions = []

    for sub_question in group_copy["sub_questions"]:
        question_text = sub_question["question_text"]
        image_path = sub_question.get("image_file")
        image_base64 = None

        if isinstance(image_path, list):
            image_file = image_path[0] if image_path else None
        else:
            image_file = image_path

        if image_file:
            image_base64 = read_image_base64(os.path.join("backend", "src", "picture", image_file))

        main_prompt = main_agent_prompt_template.format(question_text=question_text)
        main_response = call_gemini_model(main_prompt, image_base64=image_base64)

        secondary_prompt = secondary_agent_prompt_template.format(main_response=main_response)
        secondary_response = call_llama_model(secondary_prompt)

        arbiter_prompt = arbiter_agent_prompt_template.format(
            main_response=main_response,
            secondary_response=secondary_response
        )
        arbiter_response = call_gemini_model(arbiter_prompt)

        try:
            result = json.loads(arbiter_response)
            if isinstance(result, list) and result:
                processed_sub_questions.append(result[0])
            else:
                raise ValueError("ä»²è£è¼¸å‡ºæ ¼å¼éŒ¯èª¤")
        except Exception as e:
            fallback = copy.deepcopy(sub_question)
            fallback["answer"] = ""
            fallback["detail-answer"] = ""
            fallback["key-points"] = ""
            fallback["difficulty level"] = ""
            fallback["error reason"] = f"ä»²è£è§£æéŒ¯èª¤: {e}"
            processed_sub_questions.append(fallback)

    group_copy["sub_questions"] = processed_sub_questions
    return group_copy

# ========== åˆä½µ ==========
def process_all_questions(questions):
    results = []
    count = 0

    for q in questions:
        if q["type"] == "group":
            print(f"ğŸ”„ è™•ç†ç¾¤çµ„é¡Œç›®ï¼š{q.get('group_question_text', '')}ï¼Œå…± {len(q['sub_questions'])} é¡Œ")
            group_result = process_group_question(q)
            results.append(group_result)
            count += len(q['sub_questions'])
            print(f"å·²è™•ç† {count} é¡Œ")
        else:
            count += 1
            print(f"ğŸ”„ è™•ç†ç¬¬ {count} é¡Œ...")
            result = process_question(q)
            results.append(result)

    return results




if __name__ == "__main__":
    with open("../data/grouped_exam_processed_test.json", "r", encoding="utf-8") as f:
        questions = json.load(f)

    results = process_all_questions(questions)
    with open("../data/results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("\nâœ… æ‰€æœ‰é¡Œç›®è™•ç†å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ results.json")
    print(f"ğŸ” å…±è™•ç† {len(results)} é¡Œç›®ã€‚")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGè™•ç†å™¨ - æ ¸å¿ƒPDFè™•ç†å’Œå‘é‡è³‡æ–™åº«å»ºç«‹æ¨¡çµ„
æ”¯æ´æ‰¹é‡è™•ç†PDFæ•™æï¼Œè‡ªå‹•å»ºç«‹çŸ¥è­˜é»å‘é‡è³‡æ–™åº«
"""

import os
import re
import json
import uuid
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

# PDFè™•ç†ç›¸é—œ
from unstructured.partition.pdf import partition_pdf
from unstructured.documents.elements import Title, NarrativeText, ListItem, Table, Text

# å‘é‡åŒ–å’ŒåµŒå…¥
from sentence_transformers import SentenceTransformer
import numpy as np
import torch

# å‘é‡è³‡æ–™åº«
import chromadb
from chromadb.config import Settings

# é€²åº¦æ¢å’Œå·¥å…·
from tqdm import tqdm
# æ™ºèƒ½å°å…¥configæ¨¡çµ„
def _import_config():
    """æ™ºèƒ½å°å…¥configæ¨¡çµ„ï¼Œè™•ç†ä¸åŒçš„åŸ·è¡Œç’°å¢ƒ"""
    import sys
    import os

    # ç²å–ç•¶å‰æ–‡ä»¶æ‰€åœ¨ç›®éŒ„
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # æ–¹æ³•1: å˜—è©¦å¾ç•¶å‰ç›®éŒ„ç›´æ¥å°å…¥
    try:
        sys.path.insert(0, current_dir)
        import config as config
        # é©—è­‰æ˜¯å¦æ˜¯æ­£ç¢ºçš„configï¼ˆæª¢æŸ¥é—œéµå±¬æ€§ï¼‰
        if hasattr(config, 'EMBEDDING_MODEL') and hasattr(config, 'LOGGING_CONFIG'):
            return config
    except ImportError:
        pass

    # æ–¹æ³•2: å˜—è©¦ç›¸å°å°å…¥
    try:
        from . import config
        if hasattr(config, 'EMBEDDING_MODEL') and hasattr(config, 'LOGGING_CONFIG'):
            return config
    except ImportError:
        pass

    # æ–¹æ³•3: å˜—è©¦çµ•å°å°å…¥
    try:
        import rag_system.config as config
        if hasattr(config, 'EMBEDDING_MODEL') and hasattr(config, 'LOGGING_CONFIG'):
            return config
    except ImportError:
        pass

    # å¦‚æœéƒ½å¤±æ•—ï¼Œæ‹‹å‡ºè©³ç´°éŒ¯èª¤
    raise ImportError(
        "ç„¡æ³•å°å…¥æ­£ç¢ºçš„configæ¨¡çµ„ã€‚è«‹ç¢ºèª:\n"
        "1. rag_system/config.py æ–‡ä»¶å­˜åœ¨\n"
        "2. config.py åŒ…å« EMBEDDING_MODEL å’Œ LOGGING_CONFIG\n"
        "3. ç•¶å‰å·¥ä½œç›®éŒ„æ­£ç¢º"
    )

# å°å…¥config
config = _import_config()

# è¨­å®šæ—¥èªŒ
try:
    logging.basicConfig(
        level=getattr(logging, config.LOGGING_CONFIG['level']),
        format=config.LOGGING_CONFIG['format'],
        handlers=[
            logging.FileHandler(config.LOGGING_CONFIG['file'], encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
except (AttributeError, KeyError) as e:
    # å¦‚æœé…ç½®æœ‰å•é¡Œï¼Œä½¿ç”¨é»˜èªé…ç½®
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
logger = logging.getLogger(__name__)

class RAGProcessor:
    """
    RAGè™•ç†å™¨ - è² è²¬PDFè™•ç†å’Œå‘é‡è³‡æ–™åº«å»ºç«‹

    ä¸»è¦åŠŸèƒ½:
    1. æ‰¹é‡è™•ç†PDFæ•™ææª”æ¡ˆ
    2. æå–å’Œçµæ§‹åŒ–çŸ¥è­˜é»
    3. å»ºç«‹å‘é‡è³‡æ–™åº«
    4. æ”¯æ´ä¸­è‹±æ–‡æ··åˆå…§å®¹
    """

    def __init__(self,
                 embedding_model: str = None,
                 use_chromadb: bool = True,
                 verbose: bool = None,
                 use_gpu: bool = None):
        """
        åˆå§‹åŒ–RAGè™•ç†å™¨

        Args:
            embedding_model: å‘é‡åŒ–æ¨¡å‹åç¨±ï¼Œé è¨­ä½¿ç”¨configä¸­çš„è¨­å®š
            use_chromadb: æ˜¯å¦ä½¿ç”¨ChromaDBï¼Œé è¨­True
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Šï¼Œé è¨­ä½¿ç”¨configä¸­çš„è¨­å®š
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUï¼Œé è¨­ä½¿ç”¨configä¸­çš„è¨­å®š
        """
        # è¼‰å…¥é…ç½®
        self.embedding_model_name = embedding_model or config.EMBEDDING_MODEL
        self.use_chromadb = use_chromadb
        self.verbose = verbose if verbose is not None else config.VERBOSE_MODE
        self.use_gpu = use_gpu if use_gpu is not None else config.GPU_CONFIG['enable_gpu']

        # æª¢æŸ¥GPUå¯ç”¨æ€§
        self.gpu_info = config.check_gpu_availability()
        self.device = self._setup_device()

        # åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹ (GPUå„ªåŒ–)
        logger.info(f"ğŸ”„ æ­£åœ¨è¼‰å…¥å‘é‡åŒ–æ¨¡å‹: {self.embedding_model_name}")
        logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
        try:
            # ä½¿ç”¨GPUå„ªåŒ–é…ç½®è¼‰å…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(
                self.embedding_model_name,
                device=self.device
            )

            # è¨­å®šGPUå„ªåŒ–åƒæ•¸
            if self.device == 'cuda':
                # è¨­å®šæ¨¡å‹ç‚ºåŠç²¾åº¦æ¨¡å¼ (FP16) ä»¥åŠ é€Ÿå’Œç¯€çœè¨˜æ†¶é«”
                if config.EMBEDDING_CONFIG.get('precision') == 'float16':
                    self.embedding_model.half()
                    logger.info("ğŸš€ å•Ÿç”¨FP16åŠç²¾åº¦æ¨¡å¼ï¼Œæå‡GPUè™•ç†é€Ÿåº¦")

                # è¨­å®šæœ€å¤§åºåˆ—é•·åº¦
                max_seq_length = config.EMBEDDING_CONFIG.get('max_seq_length', 512)
                self.embedding_model.max_seq_length = max_seq_length
                logger.info(f"ğŸ“ è¨­å®šæœ€å¤§åºåˆ—é•·åº¦: {max_seq_length}")

            logger.info("âœ… å‘é‡åŒ–æ¨¡å‹è¼‰å…¥æˆåŠŸ (GPUå„ªåŒ–)")
        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

        # åˆå§‹åŒ–è³‡æ–™å­˜å„²
        self.structured_data = []      # çµæ§‹åŒ–å…§å®¹æ•¸æ“š
        self.knowledge_points = []     # çŸ¥è­˜é»æ•¸æ“š

        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        if self.use_chromadb:
            self._init_chromadb()
        else:
            # å¦‚æœæ²’æœ‰FAISS-GPUï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨CPU FAISS
            self.faiss_index = None
            self.faiss_metadata = []
            if self.use_gpu:
                logger.info("ğŸ’¡ å»ºè­°å®‰è£ faiss-gpu ä»¥ç²å¾—æ›´å¥½çš„GPUåŠ é€Ÿæ•ˆæœ")

        logger.info("ğŸš€ RAGè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_device(self):
        """
        è¨­å®šè¨ˆç®—è¨­å‚™ (GPU/CPU)

        Returns:
            str: è¨­å‚™åç¨±
        """
        if self.use_gpu and self.gpu_info['available']:
            device = config.GPU_CONFIG['device']
            logger.info(f"ğŸš€ ä½¿ç”¨GPU: {self.gpu_info['device_name']}")
            logger.info(f"ğŸ’¾ GPUè¨˜æ†¶é«”: {self.gpu_info['memory_total']}GB ç¸½è¨ˆ, {self.gpu_info['memory_free']}GB å¯ç”¨")

            # è¨­å®šGPUè¨˜æ†¶é«”ä½¿ç”¨æ¯”ä¾‹
            if device == 'cuda':
                try:
                    import torch
                    torch.cuda.set_per_process_memory_fraction(config.GPU_CONFIG['gpu_memory_fraction'])
                    logger.info(f"ğŸ”§ GPUè¨˜æ†¶é«”ä½¿ç”¨æ¯”ä¾‹è¨­å®šç‚º: {config.GPU_CONFIG['gpu_memory_fraction']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¨­å®šGPUè¨˜æ†¶é«”æ¯”ä¾‹å¤±æ•—: {e}")

            return device
        else:
            if self.use_gpu:
                logger.warning("âš ï¸ GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
            else:
                logger.info("ğŸ–¥ï¸ ä½¿ç”¨CPUæ¨¡å¼")
            return 'cpu'

    def _init_chromadb(self):
        """åˆå§‹åŒ–ChromaDB"""
        try:
            self.chroma_client = chromadb.PersistentClient(path=config.CHROMA_DB_PATH)
            self.collection = None
            logger.info(f"âœ… ChromaDBåˆå§‹åŒ–æˆåŠŸï¼Œè·¯å¾‘: {config.CHROMA_DB_PATH}")

            # å˜—è©¦è¼‰å…¥ç¾æœ‰çš„collection
            try:
                self.collection = self.chroma_client.get_collection(config.COLLECTION_NAME)
                count = self.collection.count()
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ç¾æœ‰å‘é‡è³‡æ–™åº«ï¼ŒåŒ…å« {count} å€‹çŸ¥è­˜é»")
            except Exception:
                logger.info("ğŸ’¡ æœªæ‰¾åˆ°ç¾æœ‰å‘é‡è³‡æ–™åº«ï¼Œéœ€è¦å…ˆå»ºç«‹çŸ¥è­˜åº«")

        except Exception as e:
            logger.error(f"âŒ ChromaDBåˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def process_multiple_pdfs(self,
                            pdf_paths: List[str],
                            output_json: str = None) -> bool:
        """
        æ‰¹é‡è™•ç†å¤šå€‹PDFæª”æ¡ˆ

        Args:
            pdf_paths: PDFæª”æ¡ˆè·¯å¾‘åˆ—è¡¨
            output_json: è¼¸å‡ºJSONæª”æ¡ˆè·¯å¾‘ï¼Œé è¨­ä½¿ç”¨configä¸­çš„è¨­å®š

        Returns:
            bool: è™•ç†æ˜¯å¦æˆåŠŸ
        """
        if not pdf_paths:
            logger.warning("âš ï¸ æ²’æœ‰æä¾›PDFæª”æ¡ˆè·¯å¾‘")
            return False

        # è¨­å®šè¼¸å‡ºè·¯å¾‘
        if output_json is None:
            output_json = config.OUTPUT_DIR / config.OUTPUT_FILES['structured_content']

        logger.info(f"ğŸ“š é–‹å§‹æ‰¹é‡è™•ç† {len(pdf_paths)} å€‹PDFæª”æ¡ˆ")

        # æ¸…ç©ºä¹‹å‰çš„æ•¸æ“š
        self.structured_data = []

        # è™•ç†æ¯å€‹PDFæª”æ¡ˆ
        successful_files = 0
        failed_files = []

        for pdf_path in tqdm(pdf_paths, desc="è™•ç†PDFæª”æ¡ˆ", disable=not self.verbose):
            try:
                if self._process_single_pdf(pdf_path):
                    successful_files += 1
                    logger.info(f"âœ… æˆåŠŸè™•ç†: {os.path.basename(pdf_path)}")
                else:
                    failed_files.append(pdf_path)
                    logger.warning(f"âš ï¸ è™•ç†å¤±æ•—: {os.path.basename(pdf_path)}")
            except Exception as e:
                failed_files.append(pdf_path)
                logger.error(f"âŒ è™•ç† {os.path.basename(pdf_path)} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # ä¿å­˜çµæ§‹åŒ–æ•¸æ“š
        if self.structured_data:
            self._save_structured_data(output_json)
            logger.info(f"ğŸ“Š æ‰¹é‡è™•ç†å®Œæˆ: æˆåŠŸ {successful_files} å€‹ï¼Œå¤±æ•— {len(failed_files)} å€‹")

            if failed_files:
                logger.warning(f"âŒ å¤±æ•—çš„æª”æ¡ˆ: {[os.path.basename(f) for f in failed_files]}")

            return successful_files > 0
        else:
            logger.error("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•PDFæª”æ¡ˆ")
            return False

    def _process_single_pdf(self, pdf_path: str) -> bool:
        """
        è™•ç†å–®å€‹PDFæª”æ¡ˆ

        Args:
            pdf_path: PDFæª”æ¡ˆè·¯å¾‘

        Returns:
            bool: è™•ç†æ˜¯å¦æˆåŠŸ
        """
        if not os.path.exists(pdf_path):
            logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {pdf_path}")
            return False

        logger.info(f"ğŸ“– æ­£åœ¨è™•ç†: {os.path.basename(pdf_path)}")

        try:
            # ä½¿ç”¨unstructuredè§£æPDF
            elements = partition_pdf(
                filename=pdf_path,
                strategy=config.PDF_PROCESSING['strategy'],
                infer_table_structure=config.PDF_PROCESSING['infer_table_structure'],
                chunking_strategy=config.PDF_PROCESSING['chunking_strategy'],
                max_characters=config.PDF_PROCESSING['max_characters'],
                new_after_n_chars=config.PDF_PROCESSING['new_after_n_chars'],
                languages=config.PDF_PROCESSING['languages']
            )

            # åˆå§‹åŒ–ç« ç¯€è¿½è¹¤è®Šæ•¸
            current_chapter = "å‰è¨€"
            current_section = "ç„¡å°ç¯€"
            current_subsection = "ç„¡å‰¯æ¨™é¡Œ"

            # è™•ç†æ¯å€‹å…ƒç´ 
            processed_count = 0
            for element in tqdm(elements, desc="è§£æå…§å®¹å…ƒç´ ", leave=False, disable=not self.verbose):
                content = str(element).strip()

                # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                if not content or len(content) < config.PDF_PROCESSING['min_content_length']:
                    continue

                # ç²å–å…ƒæ•¸æ“š
                page_num = getattr(element.metadata, 'page_number', 0) if hasattr(element, 'metadata') else 0
                source_file = os.path.basename(pdf_path)
                element_type = type(element).__name__

                # ç« ç¯€è­˜åˆ¥å’Œæ›´æ–°
                if isinstance(element, Title):
                    chapter_info = self._identify_chapter_structure(content)
                    if chapter_info['type'] == 'chapter':
                        current_chapter = content
                        current_section = "ç„¡å°ç¯€"
                        current_subsection = "ç„¡å‰¯æ¨™é¡Œ"
                        if self.verbose:
                            logger.info(f"  ğŸ“š ç™¼ç¾ç« ç¯€: {current_chapter}")
                    elif chapter_info['type'] == 'section':
                        current_section = content
                        current_subsection = "ç„¡å‰¯æ¨™é¡Œ"
                        if self.verbose:
                            logger.info(f"    ğŸ“ ç™¼ç¾å°ç¯€: {current_section}")
                    elif chapter_info['type'] == 'subsection':
                        current_subsection = content
                        if self.verbose:
                            logger.info(f"      ğŸ”¸ ç™¼ç¾å‰¯æ¨™é¡Œ: {current_subsection}")

                # å»ºç«‹çµæ§‹åŒ–æ•¸æ“šé …ç›®
                structured_item = {
                    "id": str(uuid.uuid4()),
                    "content": content,
                    "metadata": {
                        "source_file": source_file,
                        "page_number": page_num,
                        "chapter": current_chapter,
                        "section": current_section,
                        "subsection": current_subsection,
                        "element_type": element_type,
                        "content_length": len(content),
                        "language": self._detect_language(content)
                    }
                }

                self.structured_data.append(structured_item)
                processed_count += 1

            logger.info(f"âœ… æˆåŠŸè™•ç† {processed_count} å€‹å…§å®¹å…ƒç´ ")
            return processed_count > 0

        except Exception as e:
            logger.error(f"âŒ è™•ç†PDFæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def _identify_chapter_structure(self, content: str) -> Dict[str, str]:
        """
        è­˜åˆ¥ç« ç¯€çµæ§‹

        Args:
            content: å…§å®¹æ–‡æœ¬

        Returns:
            Dict: åŒ…å«ç« ç¯€é¡å‹å’Œè³‡è¨Šçš„å­—å…¸
        """
        # ç« ç¯€æ¨¡å¼åŒ¹é…
        patterns = {
            'chapter': [
                r'(?:Chapter|ç¬¬|ç« )\s*(\d+)[:\s]*(.*)',
                r'Part\s+(\w+)[:\s]*(.*)',
                r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« [:\s]*(.*)'
            ],
            'section': [
                r'^(\d+\.\d+)\s+(.*)',
                r'^(\d+\.\d+\.\d+)\s+(.*)',
                r'Â§\s*(\d+\.\d+)\s+(.*)'
            ],
            'subsection': [
                r'^(\d+\.\d+\.\d+)\s+(.*)',
                r'^(\d+\.\d+\.\d+\.\d+)\s+(.*)'
            ]
        }

        for structure_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                if re.match(pattern, content, re.IGNORECASE):
                    return {'type': structure_type, 'content': content}

        return {'type': 'content', 'content': content}

    def _detect_language(self, content: str) -> str:
        """
        æª¢æ¸¬å…§å®¹èªè¨€

        Args:
            content: å…§å®¹æ–‡æœ¬

        Returns:
            str: èªè¨€ä»£ç¢¼ ('zh', 'en', 'mixed')
        """
        # ç°¡å–®çš„èªè¨€æª¢æ¸¬
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        english_chars = len(re.findall(r'[a-zA-Z]', content))
        total_chars = chinese_chars + english_chars

        if total_chars == 0:
            return 'unknown'

        chinese_ratio = chinese_chars / total_chars

        if chinese_ratio > 0.7:
            return 'zh'
        elif chinese_ratio < 0.3:
            return 'en'
        else:
            return 'mixed'

    def _save_structured_data(self, output_path: str):
        """
        ä¿å­˜çµæ§‹åŒ–æ•¸æ“šåˆ°JSONæª”æ¡ˆ

        Args:
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.structured_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… çµæ§‹åŒ–æ•¸æ“šå·²ä¿å­˜åˆ°: {output_path}")
            logger.info(f"ğŸ“Š ç¸½å…±æå–äº† {len(self.structured_data)} å€‹å…§å®¹å¡Š")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜çµæ§‹åŒ–æ•¸æ“šå¤±æ•—: {e}")

    def create_knowledge_points(self, min_content_length: int = None) -> bool:
        """
        å¾çµæ§‹åŒ–æ•¸æ“šå‰µå»ºçŸ¥è­˜é»

        Args:
            min_content_length: æœ€å°å…§å®¹é•·åº¦é–¾å€¼

        Returns:
            bool: å‰µå»ºæ˜¯å¦æˆåŠŸ
        """
        if not self.structured_data:
            logger.error("âŒ æ²’æœ‰çµæ§‹åŒ–æ•¸æ“šï¼Œè«‹å…ˆè™•ç†PDFæª”æ¡ˆ")
            return False

        min_length = min_content_length or config.KNOWLEDGE_POINT_CONFIG['min_content_length']
        logger.info(f"ğŸ§  æ­£åœ¨å‰µå»ºçŸ¥è­˜é» (æœ€å°é•·åº¦: {min_length})")

        self.knowledge_points = []

        for item in tqdm(self.structured_data, desc="è™•ç†çŸ¥è­˜é»", disable=not self.verbose):
            content = item["content"]
            metadata = item["metadata"]

            # éæ¿¾å¤ªçŸ­çš„å…§å®¹
            if len(content) < min_length:
                continue

            # å‰µå»ºçŸ¥è­˜é»
            knowledge_point = {
                "id": item["id"],
                "title": self._generate_title(content, metadata),
                "content": content,
                "summary": self._generate_summary(content),
                "keywords": self._extract_keywords(content),
                "metadata": metadata
            }

            self.knowledge_points.append(knowledge_point)

        logger.info(f"âœ… å‰µå»ºäº† {len(self.knowledge_points)} å€‹çŸ¥è­˜é»")
        return len(self.knowledge_points) > 0

    def _generate_title(self, content: str, metadata: Dict) -> str:
        """
        ç”ŸæˆçŸ¥è­˜é»æ¨™é¡Œ

        Args:
            content: å…§å®¹æ–‡æœ¬
            metadata: å…ƒæ•¸æ“š

        Returns:
            str: ç”Ÿæˆçš„æ¨™é¡Œ
        """
        max_length = config.KNOWLEDGE_POINT_CONFIG['max_title_length']

        # å¦‚æœæ˜¯æ¨™é¡Œå…ƒç´ ï¼Œç›´æ¥ä½¿ç”¨
        if metadata.get("element_type") == "Title":
            return content[:max_length]

        # å¦å‰‡å¾å…§å®¹ç”Ÿæˆæ¨™é¡Œ
        sentences = content.split('.')
        if sentences and len(sentences[0].strip()) > 10:
            return sentences[0].strip()[:max_length] + ("..." if len(sentences[0]) > max_length else "")

        # å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨ç« ç¯€è³‡è¨Š
        chapter = metadata.get('chapter', '')
        section = metadata.get('section', '')
        if chapter != "å‰è¨€" and section != "ç„¡å°ç¯€":
            return f"{chapter} - {section}"[:max_length]

        return content[:max_length] + "..."

    def _generate_summary(self, content: str) -> str:
        """
        ç”Ÿæˆå…§å®¹æ‘˜è¦

        Args:
            content: å…§å®¹æ–‡æœ¬

        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦
        """
        max_length = config.KNOWLEDGE_POINT_CONFIG['max_summary_length']

        if len(content) <= max_length:
            return content

        # ç°¡å–®çš„æ‘˜è¦ç”Ÿæˆï¼šå–å‰å¹¾å¥è©±
        sentences = content.split('.')
        summary = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if len(summary + sentence) <= max_length - 3:  # ç•™ç©ºé–“çµ¦çœç•¥è™Ÿ
                summary += sentence + ". "
            else:
                break

        return summary.strip() + "..." if summary else content[:max_length] + "..."

    def _extract_keywords(self, content: str) -> List[str]:
        """
        æå–é—œéµè©

        Args:
            content: å…§å®¹æ–‡æœ¬

        Returns:
            List[str]: é—œéµè©åˆ—è¡¨
        """
        max_keywords = config.KNOWLEDGE_POINT_CONFIG['max_keywords']

        # æå–ä¸­è‹±æ–‡è©å½™
        words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]{3,}\b', content)
        word_freq = {}

        # çµ±è¨ˆè©é »
        for word in words:
            word_lower = word.lower()
            # éæ¿¾å¸¸è¦‹åœç”¨è©
            if word_lower not in ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']:
                word_freq[word_lower] = word_freq.get(word_lower, 0) + 1

        # æ’åºä¸¦è¿”å›å‰Nå€‹
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:max_keywords]]

    def build_knowledge_database(self, collection_name: str = None) -> bool:
        """
        å»ºç«‹çŸ¥è­˜é»å‘é‡è³‡æ–™åº«

        Args:
            collection_name: é›†åˆåç¨±ï¼ˆChromaDBç”¨ï¼‰

        Returns:
            bool: å»ºç«‹æ˜¯å¦æˆåŠŸ
        """
        if not self.knowledge_points:
            logger.error("âŒ æ²’æœ‰çŸ¥è­˜é»å¯ä»¥å»ºç«‹å‘é‡è³‡æ–™åº«ï¼Œè«‹å…ˆåŸ·è¡Œ create_knowledge_points()")
            return False

        collection_name = collection_name or config.COLLECTION_NAME
        logger.info(f"ğŸ”§ æ­£åœ¨å»ºç«‹å‘é‡è³‡æ–™åº« ({'ChromaDB' if self.use_chromadb else 'FAISS'})")

        # æº–å‚™æ–‡æœ¬æ•¸æ“š
        texts = [kp["content"] for kp in self.knowledge_points]

        try:
            if self.use_chromadb:
                return self._build_chromadb(texts, collection_name)
            else:
                return self._build_faiss(texts)
        except Exception as e:
            logger.error(f"âŒ å»ºç«‹å‘é‡è³‡æ–™åº«å¤±æ•—: {e}")
            return False

    def _build_chromadb(self, texts: List[str], collection_name: str) -> bool:
        """
        å»ºç«‹ChromaDBå‘é‡è³‡æ–™åº« (æ”¯æ´GPUåŠ é€Ÿ)

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            collection_name: é›†åˆåç¨±

        Returns:
            bool: å»ºç«‹æ˜¯å¦æˆåŠŸ
        """
        try:
            # å‰µå»ºæˆ–ç²å–é›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name=collection_name,
                metadata={"description": "æ•™æçŸ¥è­˜é»å‘é‡è³‡æ–™åº«"}
            )

            # æº–å‚™æ•¸æ“š
            ids = [kp["id"] for kp in self.knowledge_points]
            metadatas = []

            # è™•ç†å…ƒæ•¸æ“šï¼Œç¢ºä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•¸å­—
            for kp in self.knowledge_points:
                metadata = {}
                for key, value in kp["metadata"].items():
                    if isinstance(value, (str, int, float)):
                        metadata[key] = value
                    else:
                        metadata[key] = str(value)

                # æ·»åŠ é¡å¤–çš„çŸ¥è­˜é»è³‡è¨Š
                metadata.update({
                    "title": kp["title"],
                    "summary": kp["summary"],
                    "keywords": ", ".join(kp["keywords"])
                })
                metadatas.append(metadata)

            # æ ¹æ“šè¨­å‚™é¸æ“‡æ‰¹é‡å¤§å°
            if self.device == 'cuda':
                batch_size = config.PERFORMANCE_CONFIG['gpu_batch_size']
                logger.info(f"ğŸš€ ä½¿ç”¨GPUæ‰¹é‡è™•ç†ï¼Œæ‰¹é‡å¤§å°: {batch_size}")
            else:
                batch_size = config.PERFORMANCE_CONFIG['cpu_batch_size']
                logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨CPUæ‰¹é‡è™•ç†ï¼Œæ‰¹é‡å¤§å°: {batch_size}")

            total_batches = (len(texts) + batch_size - 1) // batch_size

            # é å…ˆç”Ÿæˆæ‰€æœ‰åµŒå…¥å‘é‡ (GPUåŠ é€Ÿ)
            logger.info("ğŸ”„ æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
            all_embeddings = []

            for i in tqdm(range(0, len(texts), batch_size),
                         desc="ç”Ÿæˆå‘é‡",
                         total=total_batches,
                         disable=not self.verbose):
                batch_texts = texts[i:i+batch_size]

                # ä½¿ç”¨GPU/CPUç”ŸæˆåµŒå…¥å‘é‡ (å„ªåŒ–ç‰ˆ)
                try:
                    if self.device == 'cuda':
                        # GPUæ¨¡å¼ï¼šä½¿ç”¨æ··åˆç²¾åº¦å’Œå„ªåŒ–åƒæ•¸
                        with torch.cuda.amp.autocast(enabled=config.GPU_CONFIG['mixed_precision']):
                            batch_embeddings = self.embedding_model.encode(
                                batch_texts,
                                device=self.device,
                                batch_size=config.EMBEDDING_CONFIG['batch_size'],
                                show_progress_bar=config.EMBEDDING_CONFIG['show_progress_bar'],
                                convert_to_numpy=True,
                                normalize_embeddings=config.EMBEDDING_CONFIG['normalize_embeddings'],
                                convert_to_tensor=False  # è½‰ç‚ºnumpyä»¥ç¯€çœGPUè¨˜æ†¶é«”
                            )
                    else:
                        # CPUæ¨¡å¼
                        batch_embeddings = self.embedding_model.encode(
                            batch_texts,
                            batch_size=config.PERFORMANCE_CONFIG['cpu_batch_size'],
                            show_progress_bar=False,
                            convert_to_numpy=True,
                            normalize_embeddings=config.EMBEDDING_CONFIG['normalize_embeddings']
                        )

                    all_embeddings.extend(batch_embeddings.tolist())

                    # GPUè¨˜æ†¶é«”æ¸…ç†
                    if self.device == 'cuda':
                        torch.cuda.empty_cache()

                except Exception as e:
                    logger.error(f"âŒ æ‰¹é‡ {i//batch_size + 1} å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
                    return False

            # æ‰¹é‡æ·»åŠ åˆ°ChromaDB
            logger.info("ğŸ”„ æ­£åœ¨æ·»åŠ åˆ°ChromaDB...")
            for i in tqdm(range(0, len(texts), batch_size),
                         desc="æ·»åŠ åˆ°ChromaDB",
                         total=total_batches,
                         disable=not self.verbose):
                batch_texts = texts[i:i+batch_size]
                batch_ids = ids[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]

                self.collection.add(
                    documents=batch_texts,
                    ids=batch_ids,
                    metadatas=batch_metadatas
                )

            logger.info(f"âœ… ChromaDBå»ºç«‹å®Œæˆï¼ŒåŒ…å« {len(texts)} å€‹å‘é‡")
            logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
            return True

        except Exception as e:
            logger.error(f"âŒ å»ºç«‹ChromaDBæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def _build_faiss(self, texts: List[str]) -> bool:
        """
        å»ºç«‹FAISSå‘é‡è³‡æ–™åº« (æ”¯æ´GPUåŠ é€Ÿ)

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            bool: å»ºç«‹æ˜¯å¦æˆåŠŸ
        """
        try:
            import faiss

            # æ ¹æ“šè¨­å‚™é¸æ“‡æ‰¹é‡å¤§å°
            if self.device == 'cuda':
                batch_size = config.PERFORMANCE_CONFIG['gpu_batch_size']
                logger.info(f"ğŸš€ ä½¿ç”¨GPUç”ŸæˆFAISSå‘é‡ï¼Œæ‰¹é‡å¤§å°: {batch_size}")
            else:
                batch_size = config.PERFORMANCE_CONFIG['cpu_batch_size']
                logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨CPUç”ŸæˆFAISSå‘é‡ï¼Œæ‰¹é‡å¤§å°: {batch_size}")

            # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
            logger.info("ğŸ”„ æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
            all_embeddings = []
            total_batches = (len(texts) + batch_size - 1) // batch_size

            for i in tqdm(range(0, len(texts), batch_size),
                         desc="ç”ŸæˆFAISSå‘é‡",
                         total=total_batches,
                         disable=not self.verbose):
                batch_texts = texts[i:i+batch_size]

                try:
                    if self.device == 'cuda':
                        # GPUæ¨¡å¼ï¼šä½¿ç”¨æ··åˆç²¾åº¦å’Œå„ªåŒ–åƒæ•¸
                        with torch.cuda.amp.autocast(enabled=config.GPU_CONFIG['mixed_precision']):
                            batch_embeddings = self.embedding_model.encode(
                                batch_texts,
                                device=self.device,
                                batch_size=config.EMBEDDING_CONFIG['batch_size'],
                                show_progress_bar=False,
                                convert_to_numpy=True,
                                normalize_embeddings=config.EMBEDDING_CONFIG['normalize_embeddings']
                            )
                    else:
                        # CPUæ¨¡å¼
                        batch_embeddings = self.embedding_model.encode(
                            batch_texts,
                            batch_size=config.PERFORMANCE_CONFIG['cpu_batch_size'],
                            show_progress_bar=False,
                            convert_to_numpy=True,
                            normalize_embeddings=config.EMBEDDING_CONFIG['normalize_embeddings']
                        )

                    all_embeddings.append(batch_embeddings)

                    # GPUè¨˜æ†¶é«”æ¸…ç†
                    if self.device == 'cuda':
                        torch.cuda.empty_cache()

                except Exception as e:
                    logger.error(f"âŒ æ‰¹é‡ {i//batch_size + 1} FAISSå‘é‡ç”Ÿæˆå¤±æ•—: {e}")
                    return False

            # åˆä½µæ‰€æœ‰åµŒå…¥å‘é‡
            embeddings = np.vstack(all_embeddings)

            # å»ºç«‹FAISSç´¢å¼•
            dimension = embeddings.shape[1]

            if self.device == 'cuda' and self.gpu_info['available']:
                # å˜—è©¦ä½¿ç”¨GPU FAISS
                try:
                    # å‰µå»ºGPUè³‡æº
                    res = faiss.StandardGpuResources()

                    # å‰µå»ºCPUç´¢å¼•ç„¶å¾Œè½‰ç§»åˆ°GPU
                    cpu_index = faiss.IndexFlatL2(dimension)
                    self.faiss_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)

                    logger.info("ğŸš€ ä½¿ç”¨GPU FAISSç´¢å¼•")
                except Exception as e:
                    logger.warning(f"âš ï¸ GPU FAISSåˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨CPU FAISS: {e}")
                    self.faiss_index = faiss.IndexFlatL2(dimension)
            else:
                # ä½¿ç”¨CPU FAISS
                self.faiss_index = faiss.IndexFlatL2(dimension)
                logger.info("ğŸ–¥ï¸ ä½¿ç”¨CPU FAISSç´¢å¼•")

            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            self.faiss_index.add(embeddings.astype('float32'))

            # ä¿å­˜å…ƒæ•¸æ“š
            self.faiss_metadata = self.knowledge_points.copy()

            logger.info(f"âœ… FAISSç´¢å¼•å»ºç«‹å®Œæˆï¼ŒåŒ…å« {self.faiss_index.ntotal} å€‹å‘é‡")
            logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
            return True

        except Exception as e:
            logger.error(f"âŒ å»ºç«‹FAISSæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def save_knowledge_points(self, output_path: str = None) -> bool:
        """
        ä¿å­˜çŸ¥è­˜é»åˆ°JSONæª”æ¡ˆ

        Args:
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not self.knowledge_points:
            logger.warning("âš ï¸ æ²’æœ‰çŸ¥è­˜é»å¯ä»¥ä¿å­˜")
            return False

        if output_path is None:
            output_path = config.OUTPUT_DIR / config.OUTPUT_FILES['knowledge_points']

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_points, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… çŸ¥è­˜é»å·²ä¿å­˜åˆ°: {output_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜çŸ¥è­˜é»å¤±æ•—: {e}")
            return False

    def get_processing_summary(self) -> Dict[str, Any]:
        """
        ç²å–è™•ç†æ‘˜è¦è³‡è¨Š

        Returns:
            Dict: è™•ç†æ‘˜è¦
        """
        return {
            "structured_data_count": len(self.structured_data),
            "knowledge_points_count": len(self.knowledge_points),
            "embedding_model": self.embedding_model_name,
            "database_type": "ChromaDB" if self.use_chromadb else "FAISS",
            "database_path": config.CHROMA_DB_PATH if self.use_chromadb else config.FAISS_INDEX_PATH,
            "device": self.device,
            "gpu_available": self.gpu_info['available'],
            "gpu_name": self.gpu_info.get('device_name', 'N/A'),
            "gpu_memory": f"{self.gpu_info.get('memory_total', 0)}GB",
            "collection_available": self.collection is not None
        }

    def search_knowledge(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸é—œçŸ¥è­˜é»

        Args:
            query: æœç´¢æŸ¥è©¢
            top_k: è¿”å›çµæœæ•¸é‡

        Returns:
            List[Dict]: æœç´¢çµæœåˆ—è¡¨
        """
        if not self.collection:
            logger.warning("âš ï¸ å‘é‡è³‡æ–™åº«æœªåˆå§‹åŒ–")
            return []

        try:
            # ä½¿ç”¨ChromaDBæœç´¢
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k
            )

            # æ ¼å¼åŒ–çµæœ
            formatted_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {}
                    distance = results['distances'][0][i] if results['distances'] and results['distances'][0] else 0

                    formatted_results.append({
                        'content': doc,
                        'metadata': metadata,
                        'similarity': 1 - distance,  # è½‰æ›ç‚ºç›¸ä¼¼åº¦
                        'title': metadata.get('title', 'ç›¸é—œçŸ¥è­˜'),
                        'source': metadata.get('source_file', 'æ•™å­¸è³‡æ–™'),
                        'chapter': metadata.get('chapter', 'ç›¸é—œç« ç¯€'),
                        'keywords': metadata.get('keywords', [])
                    })
            logger.info(f"ğŸ” åŸå§‹æœç´¢çµæœ: {results['documents'][0]}")
            logger.info(f"ğŸ” æœç´¢åˆ° {len(formatted_results)} å€‹ç›¸é—œçµæœ")
            return formatted_results

        except Exception as e:
            logger.error(f"âŒ æœç´¢çŸ¥è­˜é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

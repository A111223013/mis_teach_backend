import os
import json
import time
from datetime import datetime
from collections import Counter
import concurrent.futures
import shutil
from tqdm import tqdm
import threading

import google.generativeai as genai
from PIL import Image

# --- çµ„æ…‹è¨­å®š ---
API_KEYS = [
    "AIzaSyCnllszzkt3TMCpYq8vgbaaUWXCZzxJyk0", "AIzaSyDpI2FOAjUztmOK2x-K-AQtMhAtkUrNppY",
    "AIzaSyDlhMMaj4Owmw972C5uIMrq-mV71xvqf7I", "AIzaSyAz0QM_rV82V8ZopytpOPpQhiLTBROTxwU",
    "AIzaSyBKhBuTiOLtqh1_ZXDieFgyj6y_QiU28-s", "AIzaSyAxKs4FNJRnmZz6Fukz5qJDcX_Af46NkT4",
    "AIzaSyB6v3FEBazlViULPpfl8j6yw7sxMAGxnvg", "AIzaSyBa5Pm3ABDpqmZTTJHVI6GhF8b1h4T4Kj0",
    "AIzaSyB7inhc2xDJ3ZZmLujOR6hEeXaFtZZeRh4", "AIzaSyDxAbCMPA_aYZlvcpDlmVBpHinLxrEfDOg",
    "AIzaSyDdktjMeyQqmiM0Mj-rt6Tfr_yK80DomsQ", "AIzaSyBVBBtfqEbr-jV3h8JkCVES-GqhH1ebFlg",
    "AIzaSyDR6qAiFCRNqKMlXSm7x8InMLIGVDlI5-s",
]

# å‰µå»ºä¸€å€‹å…¨åŸŸçš„ lock ç‰©ä»¶å­—å…¸ï¼Œæ¯å€‹æš«å­˜æª”ä¸€å€‹
file_locks = {i: threading.Lock() for i in range(len(API_KEYS))}

def chunk_list(data, num_chunks):
    """å°‡ä¸€å€‹åˆ—è¡¨ç›¡å¯èƒ½å‡å‹»åœ°åˆ†å‰²æˆ N å€‹å€å¡Š"""
    k, m = divmod(len(data), num_chunks)
    return [data[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(num_chunks)]

def process_chunk(task_tuple):
    """
    å·¥äººåŸ·è¡Œç·’çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼šè™•ç†ä¸€æ•´å€‹é¡Œåº«å€å¡Šï¼Œä¸¦å­˜å…¥å–®ä¸€æš«å­˜æª”ã€‚
    """
    worker_id, question_chunk, api_key, picture_dir, temp_dir = task_tuple
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
    except Exception as e:
        print(f"âŒ å·¥äºº {worker_id} API Key åˆå§‹åŒ–å¤±æ•—: {e}")
        # å°‡éŒ¯èª¤å¯«å…¥ä¸€å€‹å°ˆé–€çš„éŒ¯èª¤æ—¥èªŒï¼Œè€Œä¸æ˜¯å›å‚³
        error_log_path = os.path.join(temp_dir, f"error_log_{worker_id}.txt")
        with open(error_log_path, 'w', encoding='utf-8') as f:
            f.write(f"API Key init failed: {e}\n")
        return f"å·¥äºº #{worker_id} å¤±æ•—"

    temp_file_path = os.path.join(temp_dir, f"temp_chunk_{worker_id}.json")
    
    # åˆå§‹åŒ–æš«å­˜æª”ç‚ºä¸€å€‹ç©ºçš„ JSON é™£åˆ— (ä½¿ç”¨äºŒé€²ä½å¯«å…¥)
    with open(temp_file_path, 'wb') as f:
        f.write(b'[]')

    progress_bar = tqdm(question_chunk, desc=f"å·¥äºº #{worker_id}", leave=False, position=worker_id)
    for i, item in enumerate(progress_bar):
        processed_item = item.copy()
        try:
            q_data = processed_item.get("question_data", processed_item)

            # --- ç”¢ç”Ÿç­”æ¡ˆ ---
            answer_templates = { "single-choice": "è«‹ä»¥å–®é¸é¡Œæ ¼å¼å›ç­”ï¼š", "multiple-choice": "è«‹ä»¥å¤šé¸é¡Œæ ¼å¼å›ç­”ï¼š", "true-false": "è«‹ä»¥æ˜¯éé¡Œæ ¼å¼å›ç­”ï¼š", "short-answer": "è«‹ä»¥ç°¡ç­”é¡Œæ ¼å¼å›ç­”ï¼Œè©³è¿°é‡é»ï¼š", "long-answer": "è«‹ä»¥è©³é¡Œæ ¼å¼å›ç­”ï¼Œæ¢ç†åˆ†æ˜ï¼š", "coding-answer": "è«‹ä»¥ç¨‹å¼ç¢¼ç¯„ä¾‹å½¢å¼å›ç­”ï¼š", "draw-answer": "è«‹ä»¥ç¹ªåœ–æè¿°çš„æ–¹å¼å›ç­”ï¼Œä¸¦æä¾›æ­¥é©Ÿï¼š", "fill-in-the-blank": "è«‹ä»¥å¡«ç©ºé¡Œæ ¼å¼å›ç­”ï¼Œä¸¦æä¾›å„ç©ºæ ¼ç­”æ¡ˆï¼š", "other": "è«‹ä¾é¡Œæ„å›ç­”ï¼š" }
            qtype = q_data.get("type", "other")
            template = answer_templates.get(qtype, answer_templates["other"])
            question_text = q_data.get("question_text", "")
            prompt_text = f"{template}\n{question_text}"
            options = q_data.get('options')
            if options and isinstance(options, list) and len(options) > 0:
                prompt_text += "\né¸é …ï¼š"
                for opt in options: prompt_text += f"\n{opt}"
            answer_contents = [prompt_text]
            image_files = q_data.get('image_file')
            if image_files:
                for image_filename in image_files:
                    image_path = os.path.join(picture_dir, image_filename)
                    if os.path.exists(image_path): answer_contents.append(Image.open(image_path))
            answer_response = model.generate_content(answer_contents)
            q_data['answer'] = answer_response.text

            # --- å­¸ç§‘åˆ†é¡ ---
            # ç‚ºäº†è®“åˆ†é¡æ›´æº–ç¢ºï¼Œå°‡é¡Œç›®æ–‡å­—å’Œé¸é …çµ„åˆåœ¨ä¸€èµ·
            full_question_for_classification = q_data.get('question_text', '')
            classification_options = q_data.get('options')
            if classification_options and isinstance(classification_options, list) and len(classification_options) > 0:
                full_question_for_classification += "\né¸é …ï¼š"
                for opt in classification_options:
                    full_question_for_classification += f"\n- {opt}"

            classification_prompt = f"""ä½ æ˜¯ä¸€ä½è³‡è¨Šå·¥ç¨‹é ˜åŸŸçš„å°ˆå®¶æ•™æˆã€‚è«‹æ ¹æ“šä»¥ä¸‹åœ‹éš›çŸ¥åæ•™ç§‘æ›¸çš„æ¨™æº–ç« ç¯€çµæ§‹ï¼Œç²¾ç¢ºåˆ†æè€ƒè©¦é¡Œç›®çš„æ­¸å±¬ã€‚

=== é¡Œç›®è³‡è¨Š ===
å­¸æ ¡ï¼š{q_data.get('school', 'N/A')}
ç³»æ‰€ï¼š{q_data.get('department', 'N/A')}
é¡Œç›®é¡å‹ï¼š{q_data.get('type', 'N/A')}
é¡Œç›®å…§å®¹ï¼š{full_question_for_classification}

=== æ¨™æº–æ•™ç§‘æ›¸ç« ç¯€åˆ†é¡æ¶æ§‹ ===
**1. è³‡æ–™çµæ§‹èˆ‡æ¼”ç®—æ³•** (ä¾†æº: "Introduction to Algorithms", Cormen et al.)
- Chapters: Elementary Data Structures, Trees, Hashing, Sorting, Graph Algorithms, Dynamic Programming, Greedy Algorithms.
**2. ä½œæ¥­ç³»çµ±** (ä¾†æº: "Operating System Concepts", Silberschatz et al.)
- Chapters: Processes, Threads, CPU Scheduling, Synchronization, Deadlocks, Memory Management, File Systems.
**3. è³‡æ–™åº«ç³»çµ±** (ä¾†æº: "Fundamentals of Database Systems", Elmasri & Navathe)
- Chapters: ER Model, Relational Model, SQL, Normalization, Transaction Processing, Concurrency Control.
**4. é›»è…¦ç¶²è·¯** (ä¾†æº: "Computer Networks", Tanenbaum & Wetherall)
- Chapters: OSI Model, Physical/Data Link/Network/Transport/Application Layers, Network Security.
**5. ç¨‹å¼è¨­è¨ˆ** (ä¾†æº: "The C++ Programming Language", Stroustrup; "Effective Java", Bloch)
- Topics: Variables, Control Structures, Functions, OOP, Memory Management.
**6. è»Ÿé«”å·¥ç¨‹** (ä¾†æº: "Software Engineering", Sommerville)
- Chapters: Processes, Requirements, Design, Testing, Project Management.
**7. è³‡è¨Šå®‰å…¨** (ä¾†æº: "Cryptography and Network Security", Stallings)
- Chapters: Encryption, AES, RSA, Digital Signatures, Network Security.

è«‹æ ¹æ“šä¸Šè¿°æ¶æ§‹ï¼Œå°‡é¡Œç›®æ­¸é¡åˆ°æœ€é©åˆçš„é …ç›®ï¼Œä¸¦ä»¥åš´æ ¼çš„ JSON æ ¼å¼è¼¸å‡ºï¼š
{{
    "ä¸»è¦å­¸ç§‘": "å°æ‡‰ä¸Šè¿°7å€‹é ˜åŸŸä¹‹ä¸€",
    "æ•™ç§‘æ›¸ä¾†æº": "å…·é«”çš„æ•™ç§‘æ›¸åç¨±èˆ‡ä½œè€…",
    "æ•™ç§‘æ›¸ç« ç¯€": "å°æ‡‰æ¨™æº–æ•™ç§‘æ›¸çš„å…·é«”ç« ç¯€åç¨±ï¼ˆè‹±æ–‡ï¼‰",
    "è€ƒé»å–®å…ƒ": "è©²ç« ç¯€ä¸‹çš„å…·é«”è€ƒé»æˆ–æŠ€è¡“è¦é»",
    "ç›¸é—œæ¦‚å¿µ": ["ç›¸é—œçš„é‡è¦æ¦‚å¿µæˆ–æŠ€è¡“", "æœ€å¤š3å€‹"],
    "åˆ†æèªªæ˜": "åŸºæ–¼æ•™ç§‘æ›¸æ¶æ§‹çš„å°ˆæ¥­åˆ†æèªªæ˜"
}}
"""
            classification_response = model.generate_content(classification_prompt)
            response_text = classification_response.text
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1:
                classification = json.loads(response_text[json_start:json_end])
                processed_item.update(classification)
            else:
                processed_item['åˆ†æèªªæ˜'] = "API å›æ‡‰æ ¼å¼ç•°å¸¸"
            processed_item['gemini_process_timestamp'] = datetime.now().isoformat()
        except Exception as e:
            processed_item['error'] = str(e)
            
        # ä½¿ç”¨ç·šç¨‹é–å’ŒäºŒé€²ä½æ¨¡å¼ä¾†å®‰å…¨åœ°å³æ™‚å¯«å…¥
        with file_locks[worker_id]:
            with open(temp_file_path, 'rb+') as temp_f:
                # æª¢æŸ¥æª”æ¡ˆå¤§å°ä¾†åˆ¤æ–·æ˜¯å¦ç‚ºåˆå§‹ç‹€æ…‹ '[]'
                temp_f.seek(0, os.SEEK_END)
                is_initial_file = temp_f.tell() <= 2
                
                # ç§»å‹•åˆ°æª”æ¡ˆçš„å€’æ•¸ç¬¬äºŒå€‹ä½ç½®ï¼ˆåœ¨ ']' å‰é¢ï¼‰
                temp_f.seek(-1, os.SEEK_END)
                
                # å¦‚æœæª”æ¡ˆä¸æ˜¯å‰›åˆå§‹åŒ–çš„ï¼Œå°±å…ˆåŠ ä¸Šé€—è™Ÿå’Œæ›è¡Œ
                if not is_initial_file:
                    temp_f.write(b',\n')
                
                # å°‡ Python ç‰©ä»¶è½‰ç‚ºæ ¼å¼åŒ–çš„ JSON å­—ä¸²ï¼Œå†ç·¨ç¢¼ç‚º bytes
                new_data_bytes = json.dumps(processed_item, ensure_ascii=False, indent=4).encode('utf-8')
                temp_f.write(new_data_bytes)
                
                # å¯«å›çµå°¾çš„ ']'
                temp_f.write(b'\n]')

    return f"å·¥äºº #{worker_id} å·²å®Œæˆã€‚"

if __name__ == "__main__":
    if not API_KEYS or not all(API_KEYS):
        print("âŒ éŒ¯èª¤ï¼šè«‹åœ¨ API_KEYS åˆ—è¡¨ä¸­å¡«å…¥è‡³å°‘ä¸€å€‹æœ‰æ•ˆçš„ API Keyã€‚")
        exit()
        
    print(f"ğŸš€ Gemini å€å¡ŠåŒ–ä¸¦è¡Œè™•ç†ç³»çµ±å·²å•Ÿå‹• (å…± {len(API_KEYS)} å€‹å·¥äºº) ğŸš€")
    print("="*70)
    
    input_file_path = 'correct_exam.json'
    picture_dir = 'picture'
    output_dir = 'output'
    temp_dir = os.path.join(output_dir, 'temp_chunks')
    
    if os.path.exists(temp_dir): shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)
    os.makedirs(picture_dir, exist_ok=True)

    if not os.path.exists(input_file_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆï¼ è·¯å¾‘: {os.path.abspath(input_file_path)}")
        exit()

    with open(input_file_path, 'r', encoding='utf-8') as f:
        all_questions = json.load(f)

    num_workers = len(API_KEYS)
    question_chunks = chunk_list(all_questions, num_workers)
    
    tasks = []
    for i, chunk in enumerate(question_chunks):
        if not chunk: continue
        tasks.append((i, chunk, API_KEYS[i % len(API_KEYS)], picture_dir, temp_dir))

    print(f"â³ å°‡ {len(all_questions)} é“é¡Œç›®åˆ†å‰²æˆ {len(tasks)} å€‹å€å¡Šï¼Œé–‹å§‹ä¸¦è¡Œè™•ç†...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        list(tqdm(executor.map(process_chunk, tasks), total=len(tasks), desc="ç¸½é€²åº¦"))
    
    print("\nğŸ”„ æ‰€æœ‰å€å¡Šè™•ç†å®Œæˆï¼Œé–‹å§‹å¾æš«å­˜å€åˆä½µæª”æ¡ˆ...")
    final_results = []
    all_files_found = True
    for i in range(len(tasks)):
        temp_path = os.path.join(temp_dir, f"temp_chunk_{i}.json")
        if os.path.exists(temp_path):
            with open(temp_path, 'r', encoding='utf-8') as f:
                try:
                    # ç¾åœ¨å¯ä»¥ç›´æ¥è®€å–æ•´å€‹åˆæ³•çš„ JSON æª”æ¡ˆ
                    final_results.extend(json.load(f))
                except json.JSONDecodeError:
                    print(f"âš ï¸ è­¦å‘Šï¼šæš«å­˜æª” {temp_path} ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚")
                    all_files_found = False
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°æš«å­˜æª” {temp_path}ï¼Œæœ€çµ‚çµæœå¯èƒ½ä¸å®Œæ•´ã€‚")
            all_files_found = False
            
    output_file_path = os.path.join(output_dir, os.path.basename(input_file_path))
    try:
        with open(output_file_path, 'w', encoding='utf-8') as wf:
            json.dump(final_results, wf, ensure_ascii=False, indent=4)
        print(f"âœ… æœ€çµ‚æª”æ¡ˆå·²æˆåŠŸåˆä½µä¸¦å„²å­˜è‡³: {os.path.abspath(output_file_path)}")
        if all_files_found:
            shutil.rmtree(temp_dir)
            print("ğŸ—‘ï¸ å·²æˆåŠŸæ¸…ç†æš«å­˜è³‡æ–™å¤¾ã€‚")
    except Exception as e:
        print(f"âŒ åˆä½µæˆ–å„²å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    successful_results = [r for r in final_results if 'error' not in r]
    failed_count = len(final_results) - len(successful_results)
    
    if successful_results:
        print("\n" + "="*60)
        print("ğŸ“Š Gemini åˆ†é¡çµæœçµ±è¨ˆ")
        print("="*60)
        subjects = [item.get('ä¸»è¦å­¸ç§‘', 'æœªåˆ†é¡') for item in successful_results]
        subject_counts = Counter(subjects)
        print(f"\nğŸ“ ä¸»è¦å­¸ç§‘åˆ†å¸ƒ (å…± {len(subject_counts)} å€‹):")
        for subject, count in subject_counts.most_common():
            print(f"  â€¢ {subject}: {count} é¡Œ")
        if failed_count > 0: print(f"\nâ—ï¸ {failed_count} é¡Œè™•ç†å¤±æ•—ã€‚")
        print("\n" + "="*60)
    elif failed_count > 0:
         print(f"\nâŒ æ‰€æœ‰ {failed_count} é¡Œå‡è™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯ã€‚") 
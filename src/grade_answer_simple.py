#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ‰¹æ”¹æ¨¡çµ„ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾è³´aiohttp
"""

import json
from typing import Dict, Any, Tuple, List
from accessories import mongo
from bson import ObjectId
from tool.api_keys import get_api_key, get_api_keys_count

class AnswerGrader:
    """ç­”æ¡ˆæ‰¹æ”¹å™¨"""
    
    def __init__(self):
        self.supported_types = {
            'single-choice': self._grade_single_choice,
            'multiple-choice': self._grade_multiple_choice,
            'true-false': self._grade_true_false,
            'fill-in-the-blank': self._grade_fill_in_blank,
            'short-answer': self._grade_short_answer,
            'long-answer': self._grade_long_answer,
            'group': self._grade_group_question
        }
    
    def grade_answer(self, question_id: str, user_answer: Any, question_type: str = None) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹å–®å€‹ç­”æ¡ˆ"""
        try:
            # å¾MongoDBç²å–é¡Œç›®è©³æƒ…
            question = self._get_question_by_id(question_id)
            if not question:
                return False, 0, {'error': 'é¡Œç›®ä¸å­˜åœ¨'}
            
            # ç²å–é¡Œç›®é¡å‹
            if not question_type:
                question_type = self._get_question_type(question)
            
            # æ ¹æ“šé¡Œç›®é¡å‹é€²è¡Œæ‰¹æ”¹
            if question_type in self.supported_types:
                return self.supported_types[question_type](question, user_answer)
            else:
                return False, 0, {'error': f'ä¸æ”¯æŒçš„é¡Œç›®é¡å‹: {question_type}'}
                
        except Exception as e:
            return False, 0, {'error': f'æ‰¹æ”¹å¤±æ•—: {str(e)}'}
    
    def batch_grade_ai_questions(self, questions_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ‰¹æ”¹éœ€è¦AIè©•åˆ†çš„é¡Œç›®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        if not questions_data:
            return []
        
        print(f"ğŸ”‘ å¯ç”¨APIå¯†é‘°æ•¸é‡: {get_api_keys_count()}")
        print(f"ğŸ”‘ éš¨æ©ŸAPIå¯†é‘°: {get_api_key()[:20]}...")
        
        results = []
        for question_data in questions_data:
            try:
                question_id = question_data['question_id']
                user_answer = question_data['user_answer']
                question_type = question_data['question_type']
                
                # ç²å–é¡Œç›®è©³æƒ…
                question = self._get_question_by_id(question_id)
                if not question:
                    results.append({
                        'question_id': question_id,
                        'is_correct': False,
                        'score': 0,
                        'feedback': {'error': 'é¡Œç›®ä¸å­˜åœ¨'}
                    })
                    continue
                
                # ä½¿ç”¨ç°¡å–®AIè©•åˆ†
                is_correct, score, feedback = self._simple_ai_grading(
                    user_answer, 
                    question.get('answer', ''),
                    question.get('detail-answer', ''),
                    question.get('key-points', []),
                    question_type
                )
                
                results.append({
                    'question_id': question_id,
                    'is_correct': is_correct,
                    'score': score,
                    'feedback': feedback
                })
                
            except Exception as e:
                results.append({
                    'question_id': question_data.get('question_id', ''),
                    'is_correct': False,
                    'score': 0,
                    'feedback': {'error': f'æ‰¹æ”¹å¤±æ•—: {str(e)}'}
                })
        
        return results
    
    def _get_question_by_id(self, question_id: str) -> Dict[str, Any]:
        """å¾MongoDBç²å–é¡Œç›®"""
        try:
            # å˜—è©¦ä½¿ç”¨ObjectIdæŸ¥è©¢
            question = mongo.db.exam.find_one({"_id": ObjectId(question_id)})
            if not question:
                # å¦‚æœObjectIdæŸ¥è©¢å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥æŸ¥è©¢
                question = mongo.db.exam.find_one({"_id": question_id})
            return question
        except Exception as e:
            print(f"âš ï¸ ç²å–é¡Œç›®å¤±æ•—: {e}")
            return None
    
    def _get_question_type(self, question: Dict[str, Any]) -> str:
        """ç²å–é¡Œç›®é¡å‹"""
        exam_type = question.get('type', 'single')
        if exam_type == 'group':
            # å¦‚æœæ˜¯é¡Œçµ„ï¼Œè®€å–å­é¡Œç›®çš„answer_type
            sub_questions = question.get('sub_questions', [])
            if sub_questions:
                return sub_questions[0].get('answer_type', 'single-choice')
            else:
                return 'single-choice'
        else:
            # å¦‚æœæ˜¯å–®é¡Œï¼Œç›´æ¥è®€å–answer_type
            return question.get('answer_type', 'single-choice')
    
    def _grade_single_choice(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹å–®é¸é¡Œ"""
        correct_answer = question.get('answer', '')
        is_correct = str(user_answer).strip() == str(correct_answer).strip()
        score = 100 if is_correct else 0
        
        feedback = {
            'user_answer': user_answer,
            'reference_answer': correct_answer,
            'is_correct': is_correct,
            'score': score,
            'explanation': 'å–®é¸é¡Œå®Œå…¨åŒ¹é…è©•åˆ†'
        }
        
        return is_correct, score, feedback
    
    def _grade_multiple_choice(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹å¤šé¸é¡Œ"""
        correct_answer = question.get('answer', '')
        
        if isinstance(user_answer, list) and isinstance(correct_answer, list):
            is_correct = sorted(user_answer) == sorted(correct_answer)
        else:
            is_correct = str(user_answer) == str(correct_answer)
        
        score = 100 if is_correct else 0
        
        feedback = {
            'user_answer': user_answer,
            'reference_answer': correct_answer,
            'is_correct': is_correct,
            'score': score,
            'explanation': 'å¤šé¸é¡Œå®Œå…¨åŒ¹é…è©•åˆ†'
        }
        
        return is_correct, score, feedback
    
    def _grade_true_false(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹æ˜¯éé¡Œ"""
        correct_answer = question.get('answer', '')
        
        user_bool = str(user_answer).lower() in ['true', 'æ˜¯', '1', 'yes']
        correct_bool = str(correct_answer).lower() in ['true', 'æ˜¯', '1', 'yes']
        is_correct = user_bool == correct_bool
        score = 100 if is_correct else 0
        
        feedback = {
            'user_answer': user_answer,
            'reference_answer': correct_answer,
            'is_correct': is_correct,
            'score': score,
            'explanation': 'æ˜¯éé¡Œå®Œå…¨åŒ¹é…è©•åˆ†'
        }
        
        return is_correct, score, feedback
    
    def _grade_fill_in_blank(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹å¡«ç©ºé¡Œ"""
        correct_answer = question.get('answer', '')
        
        user_text = str(user_answer).strip().lower()
        correct_text = str(correct_answer).strip().lower()
        
        if user_text == correct_text:
            is_correct = True
            score = 100
        elif len(user_text) > 3 and len(correct_text) > 3:
            # é—œéµè©åŒ¹é…ï¼ˆ70%ä»¥ä¸Šï¼‰
            user_words = set(user_text.split())
            correct_words = set(correct_text.split())
            if len(user_words.intersection(correct_words)) >= min(len(user_words), len(correct_words)) * 0.7:
                is_correct = True
                score = 80
            else:
                is_correct = False
                score = 0
        else:
            is_correct = False
            score = 0
        
        feedback = {
            'user_answer': user_answer,
            'reference_answer': correct_answer,
            'is_correct': is_correct,
            'score': score,
            'explanation': 'å¡«ç©ºé¡Œé—œéµè©åŒ¹é…è©•åˆ†'
        }
        
        return is_correct, score, feedback
    
    def _grade_short_answer(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹ç°¡ç­”é¡Œ"""
        return self._simple_ai_grading(
            user_answer,
            question.get('answer', ''),
            question.get('detail-answer', ''),
            question.get('key-points', []),
            'short-answer'
        )
    
    def _grade_long_answer(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹ç”³è«–é¡Œ"""
        return self._simple_ai_grading(
            user_answer,
            question.get('answer', ''),
            question.get('detail-answer', ''),
            question.get('key-points', []),
            'long-answer'
        )
    
    def _grade_group_question(self, question: Dict[str, Any], user_answer: Any) -> Tuple[bool, float, Dict[str, Any]]:
        """æ‰¹æ”¹é¡Œçµ„é¡Œ"""
        sub_questions = question.get('sub_questions', [])
        if not sub_questions:
            return False, 0, {'error': 'é¡Œçµ„æ²’æœ‰å­é¡Œç›®'}
        
        total_score = 0
        total_correct = 0
        
        for sub_q in sub_questions:
            sub_answer = user_answer.get(str(sub_q.get('index', 0)), '')
            sub_type = sub_q.get('answer_type', 'single-choice')
            
            if sub_type in self.supported_types:
                is_correct, score, _ = self.supported_types[sub_type](sub_q, sub_answer)
                if is_correct:
                    total_correct += 1
                total_score += score
        
        avg_score = total_score / len(sub_questions) if sub_questions else 0
        is_correct = avg_score >= 60
        
        feedback = {
            'user_answer': user_answer,
            'reference_answer': 'é¡Œçµ„é¡Œç›®',
            'is_correct': is_correct,
            'score': avg_score,
            'explanation': f'é¡Œçµ„é¡Œç›®ï¼Œ{total_correct}/{len(sub_questions)} é¡Œæ­£ç¢º'
        }
        
        return is_correct, avg_score, feedback
    
    def _simple_ai_grading(self, user_answer: str, reference_answer: str, detail_answer: str, key_points: List[str], answer_type: str) -> Tuple[bool, float, Dict[str, Any]]:
        """ä½¿ç”¨ Gemini API é€²è¡Œæ™ºèƒ½è©•åˆ†"""
        try:
            # å˜—è©¦ä½¿ç”¨ Gemini API é€²è¡Œè©•åˆ†
            api_key = get_api_key()
            if not api_key:
                print("âš ï¸ æ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨é—œéµè©åŒ¹é…è©•åˆ†")
                return self._fallback_keyword_grading(user_answer, reference_answer, detail_answer, key_points, answer_type)
            
            # æ§‹å»ºè©•åˆ†æç¤º
            prompt = self._build_grading_prompt(user_answer, reference_answer, detail_answer, key_points, answer_type)
            
            # èª¿ç”¨ Gemini API
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel('gemini-pro')
                
                response = model.generate_content(prompt)
                result = self._parse_ai_response(response.text)
                
                if result:
                    return result['is_correct'], result['score'], result['feedback']
                else:
                    print("âš ï¸ AI è©•åˆ†å¤±æ•—ï¼Œä½¿ç”¨é—œéµè©åŒ¹é…è©•åˆ†")
                    return self._fallback_keyword_grading(user_answer, reference_answer, detail_answer, key_points, answer_type)
                    
            except Exception as ai_error:
                print(f"âš ï¸ Gemini API èª¿ç”¨å¤±æ•—: {ai_error}")
                return self._fallback_keyword_grading(user_answer, reference_answer, detail_answer, key_points, answer_type)
                
        except Exception as e:
            print(f"âŒ AI è©•åˆ†éŒ¯èª¤: {e}")
            return False, 0, {'error': f'AI è©•åˆ†å¤±æ•—: {str(e)}'}
    
    def _fallback_keyword_grading(self, user_answer: str, reference_answer: str, detail_answer: str, key_points: List[str], answer_type: str) -> Tuple[bool, float, Dict[str, Any]]:
        """é—œéµè©åŒ¹é…è©•åˆ†ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            # è¨ˆç®—é—œéµè©åŒ¹é…ç‡
            user_words = set(str(user_answer).lower().split())
            reference_words = set(str(reference_answer).lower().split())
            
            if not reference_words:
                return False, 0, {'error': 'æ²’æœ‰åƒè€ƒç­”æ¡ˆ'}
            
            # è¨ˆç®—åŒ¹é…ç‡
            intersection = user_words.intersection(reference_words)
            match_ratio = len(intersection) / len(reference_words)
            
            # æ ¹æ“šé¡Œå‹è¨­ç½®ä¸åŒçš„é–¾å€¼
            if answer_type == 'short-answer':
                threshold = 0.4  # ç°¡ç­”é¡Œè¦æ±‚40%ä»¥ä¸ŠåŒ¹é…
            elif answer_type == 'long-answer':
                threshold = 0.3  # ç”³è«–é¡Œè¦æ±‚30%ä»¥ä¸ŠåŒ¹é…
            else:
                threshold = 0.5  # å…¶ä»–é¡Œå‹è¦æ±‚50%ä»¥ä¸ŠåŒ¹é…
            
            # æ ¹æ“šåŒ¹é…ç‡è©•åˆ†
            if match_ratio >= 0.9:
                score = 95  # å®Œå…¨æ­£ç¢º
                is_correct = True
            elif match_ratio >= 0.8:
                score = 85  # æ¥è¿‘æ­£ç¢º
                is_correct = True
            elif match_ratio >= 0.6:
                score = 70  # å‹‰å¼·åŠæ ¼
                is_correct = True
            elif match_ratio >= threshold:
                score = 60  # å‹‰å¼·åŠæ ¼
                is_correct = True
            else:
                score = 0   # å®Œå…¨éŒ¯èª¤
                is_correct = False
            
            feedback = {
                'user_answer': user_answer,
                'reference_answer': reference_answer,
                'detail_answer': detail_answer,
                'key_points': key_points,
                'match_ratio': round(match_ratio, 2),
                'threshold': threshold,
                'is_correct': is_correct,
                'score': score,
                'explanation': f'é—œéµè©åŒ¹é…ç‡: {match_ratio:.1%}ï¼Œé–¾å€¼: {threshold:.1%}'
            }
            
            return is_correct, score, feedback
            
        except Exception as e:
            return False, 0, {'error': f'AIè©•åˆ†å¤±æ•—: {str(e)}'}
    
    def _build_grading_prompt(self, user_answer: str, reference_answer: str, detail_answer: str, key_points: List[str], answer_type: str) -> str:
        """æ§‹å»º AI è©•åˆ†æç¤º"""
        prompt = f"""
è«‹ä½œç‚ºä¸€ä½å°ˆæ¥­çš„ MIS èª²ç¨‹æ•™å¸«ï¼Œå°ä»¥ä¸‹å­¸ç”Ÿç­”æ¡ˆé€²è¡Œè©•åˆ†ã€‚

é¡Œç›®é¡å‹ï¼š{answer_type}
å­¸ç”Ÿç­”æ¡ˆï¼š{user_answer}
åƒè€ƒç­”æ¡ˆï¼š{reference_answer}
è©³ç´°ç­”æ¡ˆï¼š{detail_answer}
é—œéµè¦é»ï¼š{', '.join(key_points) if key_points else 'ç„¡'}

è©•åˆ†æ¨™æº–ï¼š
- 90-100åˆ†ï¼šå®Œå…¨æ­£ç¢ºï¼Œç­”æ¡ˆå®Œæ•´ä¸”æº–ç¢º
- 80-89åˆ†ï¼šæ¥è¿‘æ­£ç¢ºï¼Œä¸»è¦æ¦‚å¿µæ­£ç¢ºä½†æœ‰å°éŒ¯èª¤
- 60-79åˆ†ï¼šå‹‰å¼·åŠæ ¼ï¼Œéƒ¨åˆ†æ­£ç¢ºä½†ç†è§£ä¸å¤ æ·±å…¥
- 0-59åˆ†ï¼šéŒ¯èª¤ï¼Œä¸»è¦æ¦‚å¿µéŒ¯èª¤æˆ–ç­”æ¡ˆä¸å®Œæ•´

è«‹ä»¥JSONæ ¼å¼è¿”å›è©•åˆ†çµæœï¼š
{{
    "is_correct": true/false,
    "score": åˆ†æ•¸(0-100),
    "feedback": {{
        "explanation": "è©•åˆ†èªªæ˜",
        "strengths": "å„ªé»",
        "weaknesses": "éœ€è¦æ”¹é€²çš„åœ°æ–¹",
        "suggestions": "å­¸ç¿’å»ºè­°"
    }}
}}
"""
        return prompt
    
    def _parse_ai_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æ AI å›æ‡‰"""
        try:
            # å˜—è©¦æå– JSON éƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # é©—è­‰å¿…è¦å­—æ®µ
                if all(key in result for key in ['is_correct', 'score', 'feedback']):
                    return result
                else:
                    print("âš ï¸ AI å›æ‡‰ç¼ºå°‘å¿…è¦å­—æ®µ")
                    return None
            else:
                print("âš ï¸ ç„¡æ³•å¾ AI å›æ‡‰ä¸­æå– JSON")
                return None
                
        except Exception as e:
            print(f"âš ï¸ è§£æ AI å›æ‡‰å¤±æ•—: {e}")
            return None

# å‰µå»ºå…¨å±€å¯¦ä¾‹
grader = AnswerGrader()

def grade_single_answer(question_id: str, user_answer: Any, question_type: str = None) -> Tuple[bool, float, Dict[str, Any]]:
    """æ‰¹æ”¹å–®å€‹ç­”æ¡ˆçš„ä¾¿æ·å‡½æ•¸"""
    return grader.grade_answer(question_id, user_answer, question_type)

def batch_grade_ai_questions(questions_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æ‰¹é‡æ‰¹æ”¹AIé¡Œç›®çš„ä¾¿æ·å‡½æ•¸ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    return grader.batch_grade_ai_questions(questions_data)

# æ¸¬è©¦å‡½æ•¸
def test_grading():
    """æ¸¬è©¦è©•åˆ†åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦è©•åˆ†åŠŸèƒ½...")
    
    # æ¸¬è©¦å–®é¸é¡Œ
    result = grade_single_answer("test_id", "A", "single-choice")
    print(f"å–®é¸é¡Œæ¸¬è©¦: {result}")
    
    # æ¸¬è©¦APIå¯†é‘°
    try:
        from tool.api_keys import get_api_key, get_api_keys_count
        print(f"APIå¯†é‘°æ•¸é‡: {get_api_keys_count()}")
        print(f"éš¨æ©Ÿå¯†é‘°: {get_api_key()[:20]}...")
    except Exception as e:
        print(f"APIå¯†é‘°æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    test_grading()
